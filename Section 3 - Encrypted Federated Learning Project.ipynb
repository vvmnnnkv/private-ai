{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Federated Learning Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvmnnnkv/private-ai/blob/master/Section%203%20-%20Encrypted%20Federated%20Learning%20Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWjkPM2zZDy5",
        "colab_type": "text"
      },
      "source": [
        "# Federated Learning with Encrypted Gradients Aggregation Project\n",
        "\n",
        "This project is improvement of naive Federated Learning protocol implemented previuosly. Now the gradients are securely aggregated using additive secret sharing method.\n",
        "\n",
        "Aggregation is done among workers so we don't need separate dedicated aggregator worker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "746q9LdrQV9s",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "4f2912f6-0cef-427d-c4b5-6e3133d6dd0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "# install dependency\n",
        "!pip install syft"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syft in /usr/local/lib/python3.6/dist-packages (0.1.20a1)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.6/dist-packages (from syft) (7.0)\n",
            "Requirement already satisfied: tf-encrypted>=0.5.4 in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.6)\n",
            "Requirement already satisfied: zstd>=1.4.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.0.3)\n",
            "Requirement already satisfied: flask-socketio>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from syft) (4.1.0)\n",
            "Requirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.2)\n",
            "Requirement already satisfied: msgpack>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from syft) (0.6.1)\n",
            "Requirement already satisfied: websocket-client>=0.56.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.56.0)\n",
            "Requirement already satisfied: lz4>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from syft) (2.1.10)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (5.1.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.4)\n",
            "Requirement already satisfied: python-socketio>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from flask-socketio>=3.3.2->syft) (4.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.56.0->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: python-engineio>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from python-socketio>=2.1.0->flask-socketio>=3.3.2->syft) (3.8.2.post1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMfNO4USQOkW",
        "colab_type": "code",
        "outputId": "97f5245c-f338-44ab-8839-febe8eac4924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "hook = sy.TorchHook(torch)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0706 07:15:57.749050 139834759829376 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0706 07:15:57.772230 139834759829376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO0qJA23QRCn",
        "colab_type": "code",
        "outputId": "07bd3507-8ea5-423e-b04d-f26788d2e4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load MNIST included with Colab\n",
        "def mnist_to_torch(df, train=True):\n",
        "  y = pd.get_dummies(df[0])\n",
        "  X = df.drop(0, axis=1)\n",
        "  X, y = torch.tensor(X.values).type(torch.float), torch.tensor(y.values).type(torch.float)\n",
        "  return X, y\n",
        "\n",
        "# Train & test datasets\n",
        "X_train, y_train = mnist_to_torch(pd.read_csv(\"sample_data/mnist_train_small.csv\", header=None))\n",
        "X_test, y_test = mnist_to_torch(pd.read_csv(\"sample_data/mnist_test.csv\", header=None))\n",
        "\n",
        "num_train = X_train.size(0)\n",
        "num_features = X_train.size(1)\n",
        "\n",
        "print(\"Train size %d, test size: %d\" % (num_train, y_test.size(0)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size 20000, test size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7mxYqKeUFJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of workers\n",
        "num_workers = 3\n",
        "# Create workers\n",
        "workers = []\n",
        "for i in range(num_workers):\n",
        "  worker = sy.VirtualWorker(hook, id=\"worker_%d\" % i)\n",
        "  workers.append(worker)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fPvEoC4Zvfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data and send chunk to each worker\n",
        "fed_dataset = []\n",
        "chunk_size = num_train // num_workers\n",
        "for i in range(num_workers):\n",
        "  start = i * chunk_size\n",
        "  if i + 1 < num_workers:\n",
        "    end = (i + 1) * chunk_size\n",
        "  else:\n",
        "    end = num_train\n",
        "  fed_dataset.append((\n",
        "      X_train[start:end].send(workers[i]), \n",
        "      y_train[start:end].send(workers[i])\n",
        "  ))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfClHa20aGZe",
        "colab_type": "code",
        "outputId": "1ea21675-4196-4ae7-ee1c-89474b7da888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(workers)\n",
        "print(fed_dataset)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<VirtualWorker id:worker_0 #objects:2>, <VirtualWorker id:worker_1 #objects:2>, <VirtualWorker id:worker_2 #objects:2>]\n",
            "[((Wrapper)>[PointerTensor | me:84902066393 -> worker_0:96416324762], (Wrapper)>[PointerTensor | me:66644451691 -> worker_0:73625216394]), ((Wrapper)>[PointerTensor | me:16287878638 -> worker_1:71673147673], (Wrapper)>[PointerTensor | me:8234497527 -> worker_1:36528660801]), ((Wrapper)>[PointerTensor | me:18413793037 -> worker_2:3112532217], (Wrapper)>[PointerTensor | me:50245312515 -> worker_2:23713311158])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_o5xum8Vw97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate mean for all parameters of list of models and set to target_model\n",
        "def avg_params(source_models, target_model):\n",
        "  # get params and emptify\n",
        "  avg_dict = target_model.state_dict()\n",
        "  for param, _ in avg_dict.items():\n",
        "    avg_dict[param].zero_()\n",
        "  \n",
        "  # sum up params\n",
        "  for _, m in source_models.items():\n",
        "    m_dict = m[0].state_dict()\n",
        "    for param, _ in m_dict.items():\n",
        "      avg_dict[param] += m_dict[param]\n",
        "  \n",
        "  # calc avg\n",
        "  for param, _ in avg_dict.items():\n",
        "    avg_dict[param] /= len(source_models)\n",
        "  \n",
        "  # set\n",
        "  target_model.load_state_dict(avg_dict)\n",
        "\n",
        "def share_model_grads(model, workers):\n",
        "  out = {}\n",
        "  for name, param in model.named_parameters():\n",
        "    if not param.requires_grad: continue\n",
        "    out[name] = param.grad.fix_prec().share(*workers)\n",
        "  return out\n",
        "\n",
        "  \n",
        "# Federated training procedure\n",
        "def fed_train(model, criteria, fed_dataset, test_dataset, opt, avg_epochs = 10, worker_epochs = 30, lr=0.001):\n",
        "  for global_epoch in range(avg_epochs):\n",
        "    # copy latest model to workers\n",
        "    fed_models = {}\n",
        "    for X, y in fed_dataset:\n",
        "      fed_model = model.copy().send(X.location)\n",
        "      optimizer = opt(params=fed_model.parameters(), lr=lr)\n",
        "      fed_models[fed_model.location.id] = (fed_model, optimizer)\n",
        "    \n",
        "    # train in parallel on workers\n",
        "    for local_epoch in range(worker_epochs):\n",
        "      losses = []\n",
        "      for X, y in fed_dataset:\n",
        "        fed_model, optimizer = fed_models[X.location.id]\n",
        "        pred = fed_model(X)\n",
        "        loss = criteria(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss = loss.get()\n",
        "        losses.append(loss)\n",
        "      print('Avg loss (%d/%d): %f' % (global_epoch, local_epoch, sum(losses) / len(losses)))\n",
        "      \n",
        "    # aggregate worker's models\n",
        "    # share each model to each worker\n",
        "    all_grads = {}\n",
        "    for w, fm in fed_models.items():\n",
        "      print('sharing %s model' % w)\n",
        "      all_grads[w] = share_model_grads(fm[0], workers)\n",
        "      # move shared pointers to my machine\n",
        "      all_grads[w] = { k:v.get() for k,v in all_grads[w].items() }\n",
        "    print(all_grads)\n",
        "      \n",
        "    # prepare avg model placeholder\n",
        "    with torch.no_grad():\n",
        "      # calc grads sum\n",
        "      grads_sum = None\n",
        "      for w, grads in all_grads.items():\n",
        "        if not grads_sum:\n",
        "          print('grad sum = ', grads)\n",
        "          grads_sum = grads\n",
        "          continue\n",
        "        for n, data in grads_sum.items():\n",
        "          # print(grads_sum[n], grads[n])\n",
        "          grads_sum[n] += grads[n]\n",
        "      \n",
        "      state = model.state_dict()\n",
        "      # cal avg, retrieve and apply to local model\n",
        "      for n, data in grads_sum.items():\n",
        "        print('calculating %s' % n)\n",
        "        grads_sum[n] /= len(workers)\n",
        "        grads_sum[n] = grads_sum[n].get().float_prec()\n",
        "        print(grads_sum[n])\n",
        "        state[n] -= lr * grads_sum[n]\n",
        "      model.load_state_dict(state)\n",
        "\n",
        "      # calculate accuracy on test set\n",
        "      X_test, y_test = test_dataset\n",
        "      y_pred = torch.softmax(model(X_test), dim=1)\n",
        "      valid = (torch.argmax(y_pred, dim=1) == torch.argmax(y_test, dim=1)).sum()\n",
        "      print('Test Accuracy: %f' % (float(valid) / float(y_test.size(0))))\n",
        "\n",
        "  return model\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD2irlGvQhkU",
        "colab_type": "code",
        "outputId": "623d08b4-801d-479a-e2c0-2765e465ad6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define a simple MLP model (softmax is included in loss)\n",
        "model = torch.nn.Sequential(\n",
        "  torch.nn.Linear(num_features, 50),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(50, 10)\n",
        ")\n",
        "loss = torch.nn.modules.loss.BCEWithLogitsLoss()\n",
        "\n",
        "# Train!\n",
        "fed_train(model, loss, fed_dataset, (X_test, y_test), torch.optim.SGD, 10, 20)\n",
        "\n",
        "t = torch.tensor([1,2,3])\n",
        "t = t.send(workers[1])\n",
        "t = t.share(*workers)\n",
        "t = t.get()\n",
        "t"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg loss (0/0): 8.216489\n",
            "Avg loss (0/1): 3.180396\n",
            "Avg loss (0/2): 2.158376\n",
            "Avg loss (0/3): 2.191385\n",
            "Avg loss (0/4): 2.217928\n",
            "Avg loss (0/5): 2.153948\n",
            "Avg loss (0/6): 1.974271\n",
            "Avg loss (0/7): 1.708811\n",
            "Avg loss (0/8): 1.441572\n",
            "Avg loss (0/9): 1.182407\n",
            "Avg loss (0/10): 0.929071\n",
            "Avg loss (0/11): 0.765021\n",
            "Avg loss (0/12): 0.851567\n",
            "Avg loss (0/13): 1.048102\n",
            "Avg loss (0/14): 1.109802\n",
            "Avg loss (0/15): 1.018232\n",
            "Avg loss (0/16): 0.896424\n",
            "Avg loss (0/17): 0.805563\n",
            "Avg loss (0/18): 0.754212\n",
            "Avg loss (0/19): 0.734603\n",
            "sharing worker_0 model\n",
            "sharing worker_1 model\n",
            "sharing worker_2 model\n",
            "{'worker_0': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:88620217308 -> worker_0:36439259640]\n",
            "\t-> (Wrapper)>[PointerTensor | me:99039981213 -> worker_1:84745112870]\n",
            "\t-> (Wrapper)>[PointerTensor | me:4072167083 -> worker_2:33391834009]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:31554127920 -> worker_0:43992072078]\n",
            "\t-> (Wrapper)>[PointerTensor | me:52163912629 -> worker_1:16266864423]\n",
            "\t-> (Wrapper)>[PointerTensor | me:36342222071 -> worker_2:72651109239]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:57933176889 -> worker_0:27247699757]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87944016931 -> worker_1:96887671086]\n",
            "\t-> (Wrapper)>[PointerTensor | me:18911333203 -> worker_2:94535786542]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:46547327773 -> worker_0:81549882767]\n",
            "\t-> (Wrapper)>[PointerTensor | me:61375048248 -> worker_1:22591014002]\n",
            "\t-> (Wrapper)>[PointerTensor | me:2027298902 -> worker_2:5163938524]\n",
            "\t*crypto provider: me*}, 'worker_1': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:65104831340 -> worker_0:33567775640]\n",
            "\t-> (Wrapper)>[PointerTensor | me:9863892340 -> worker_1:65712908093]\n",
            "\t-> (Wrapper)>[PointerTensor | me:8451976381 -> worker_2:29584443758]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:88292994910 -> worker_0:31560832790]\n",
            "\t-> (Wrapper)>[PointerTensor | me:36188732247 -> worker_1:20463301263]\n",
            "\t-> (Wrapper)>[PointerTensor | me:81577793844 -> worker_2:46771468578]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:91381591486 -> worker_0:81011997766]\n",
            "\t-> (Wrapper)>[PointerTensor | me:10065393992 -> worker_1:7316801453]\n",
            "\t-> (Wrapper)>[PointerTensor | me:22291127156 -> worker_2:37255603193]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:11731388716 -> worker_0:57620516422]\n",
            "\t-> (Wrapper)>[PointerTensor | me:5253752196 -> worker_1:34451855301]\n",
            "\t-> (Wrapper)>[PointerTensor | me:24000340240 -> worker_2:95994934101]\n",
            "\t*crypto provider: me*}, 'worker_2': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:16875242417 -> worker_0:87805346916]\n",
            "\t-> (Wrapper)>[PointerTensor | me:30750361321 -> worker_1:73064047130]\n",
            "\t-> (Wrapper)>[PointerTensor | me:16487283403 -> worker_2:31879376003]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:11182141499 -> worker_0:84442221007]\n",
            "\t-> (Wrapper)>[PointerTensor | me:51159886802 -> worker_1:3576958981]\n",
            "\t-> (Wrapper)>[PointerTensor | me:14027898056 -> worker_2:48484164903]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:52156527313 -> worker_0:2598377166]\n",
            "\t-> (Wrapper)>[PointerTensor | me:21736051536 -> worker_1:31513039603]\n",
            "\t-> (Wrapper)>[PointerTensor | me:33309186034 -> worker_2:46802187582]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:2399891333 -> worker_0:4663628177]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87874121363 -> worker_1:40274075785]\n",
            "\t-> (Wrapper)>[PointerTensor | me:89869154875 -> worker_2:789276252]\n",
            "\t*crypto provider: me*}}\n",
            "grad sum =  {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:88620217308 -> worker_0:36439259640]\n",
            "\t-> (Wrapper)>[PointerTensor | me:99039981213 -> worker_1:84745112870]\n",
            "\t-> (Wrapper)>[PointerTensor | me:4072167083 -> worker_2:33391834009]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:31554127920 -> worker_0:43992072078]\n",
            "\t-> (Wrapper)>[PointerTensor | me:52163912629 -> worker_1:16266864423]\n",
            "\t-> (Wrapper)>[PointerTensor | me:36342222071 -> worker_2:72651109239]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:57933176889 -> worker_0:27247699757]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87944016931 -> worker_1:96887671086]\n",
            "\t-> (Wrapper)>[PointerTensor | me:18911333203 -> worker_2:94535786542]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:46547327773 -> worker_0:81549882767]\n",
            "\t-> (Wrapper)>[PointerTensor | me:61375048248 -> worker_1:22591014002]\n",
            "\t-> (Wrapper)>[PointerTensor | me:2027298902 -> worker_2:5163938524]\n",
            "\t*crypto provider: me*}\n",
            "calculating 0.weight\n",
            "tensor([[-1.5372e+15, -1.5372e+15,  1.5372e+15,  ...,  0.0000e+00,\n",
            "          1.0000e-03,  1.0000e-03],\n",
            "        [-1.0000e-03, -1.0000e-03,  0.0000e+00,  ..., -1.5372e+15,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.0000e-03,  0.0000e+00,  1.0000e-03,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 1.0000e-03, -1.0000e-03, -1.5372e+15,  ...,  0.0000e+00,\n",
            "          0.0000e+00, -1.0000e-03],\n",
            "        [ 0.0000e+00,  0.0000e+00,  1.0000e-03,  ...,  0.0000e+00,\n",
            "          1.5372e+15,  1.5372e+15],\n",
            "        [ 1.0000e-03,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          1.0000e-03, -1.5372e+15]])\n",
            "calculating 0.bias\n",
            "tensor([ 3.0000e-03,  7.0000e-03, -1.5372e+15,  1.7000e-02,  0.0000e+00,\n",
            "         3.0000e-03,  4.0000e-03,  7.0000e-03, -1.5372e+15,  1.5372e+15,\n",
            "        -1.5372e+15,  2.0000e-03, -1.5372e+15,  1.0000e-03,  2.0000e-03,\n",
            "        -1.5372e+15,  4.0000e-03, -7.0000e-03,  4.0000e-03,  6.0000e-03,\n",
            "         4.0000e-03,  8.0000e-03, -5.0000e-03,  7.0000e-03, -1.5372e+15,\n",
            "         0.0000e+00,  1.0000e-03,  0.0000e+00,  1.4000e-02,  2.1000e-02,\n",
            "         5.0000e-03,  1.5372e+15,  1.5372e+15,  1.5372e+15,  8.0000e-03,\n",
            "         1.0000e-03, -3.0000e-03,  1.5000e-02,  9.0000e-03,  2.0000e-03,\n",
            "         6.0000e-03,  9.0000e-03, -1.0000e-03,  1.5372e+15,  1.0000e-02,\n",
            "         6.0000e-03,  2.0000e-03, -1.5372e+15, -1.5372e+15,  9.0000e-03])\n",
            "calculating 2.weight\n",
            "tensor([[-1.0000e-03, -3.1000e-02,  9.6700e-01,  8.5800e-01, -3.7110e+00,\n",
            "         -7.2000e-02, -2.0500e-01,  2.2500e-01,  9.4900e-01, -1.5372e+15,\n",
            "         -7.0000e-03,  3.1900e-01,  1.5540e+00, -1.6190e+00,  2.7100e-01,\n",
            "          1.2440e+00,  1.5372e+15, -1.8570e+00,  2.2000e-02,  1.2100e-01,\n",
            "         -2.3950e+00, -1.5372e+15, -1.5372e+15,  2.5200e-01, -1.4200e-01,\n",
            "         -1.5372e+15,  2.4700e-01, -2.5980e+00, -1.6190e+00,  6.8000e-01,\n",
            "          3.3600e-01,  5.6900e-01,  1.4000e-02,  4.1300e-01,  2.4750e+00,\n",
            "          6.9400e-01,  1.5372e+15,  1.6220e+00,  6.7800e-01, -7.9000e-02,\n",
            "          1.5372e+15,  1.8900e-01,  2.6000e-02,  1.7700e-01, -1.5372e+15,\n",
            "          4.2000e-01, -3.0810e+00, -1.5372e+15,  1.5372e+15, -4.1580e+00],\n",
            "        [ 5.2100e-01,  2.8800e+00, -1.5372e+15,  1.5372e+15,  2.5740e+00,\n",
            "          5.3700e-01,  5.6000e-01,  5.9700e-01,  1.8680e+00, -1.5372e+15,\n",
            "          2.5100e-01,  5.5100e-01, -1.5372e+15,  1.2780e+00,  3.4100e-01,\n",
            "          2.0600e+00,  7.3400e-01,  2.5570e+00,  3.0000e-01,  1.5372e+15,\n",
            "          8.2100e-01, -1.5372e+15,  1.9320e+00,  3.8810e+00,  1.5372e+15,\n",
            "         -1.5372e+15,  1.0900e-01, -1.5372e+15, -2.4100e-01,  2.8430e+00,\n",
            "          1.0160e+00,  1.1590e+00,  1.5372e+15, -1.5460e+00, -1.5372e+15,\n",
            "         -5.4700e-01,  1.5372e+15,  3.9670e+00, -1.5372e+15,  2.9690e+00,\n",
            "          5.3790e+00,  4.2100e-01, -8.2200e-01,  1.9600e-01, -1.5372e+15,\n",
            "         -1.5372e+15,  8.1480e+00,  3.2690e+00, -1.5372e+15, -3.8690e+00],\n",
            "        [-4.8000e-02,  2.2580e+00,  8.8700e-01,  3.8300e-01,  3.7000e-02,\n",
            "         -9.8700e-01,  2.0000e-02,  2.0000e-01, -1.5372e+15,  7.1900e-01,\n",
            "          1.8400e-01,  2.4200e-01,  3.6350e+00,  2.7900e-01, -7.6000e-02,\n",
            "         -1.5000e-01,  1.8300e-01, -3.6280e+00, -1.5372e+15,  1.9300e-01,\n",
            "         -1.5372e+15, -4.3000e-02,  2.1170e+00,  1.2580e+00, -1.5372e+15,\n",
            "          1.5372e+15,  1.5372e+15,  1.9490e+00,  2.8260e+00,  8.2000e-02,\n",
            "          1.9300e-01,  6.9400e-01,  9.3000e-02,  7.7500e-01,  1.5372e+15,\n",
            "          3.0000e-01, -1.5372e+15,  6.2000e-01, -2.5800e-01, -3.0000e-01,\n",
            "          1.1950e+00, -1.1000e-02,  6.6000e-02, -1.5372e+15, -2.0000e-02,\n",
            "         -4.2000e-02, -2.7000e-01,  1.5372e+15,  6.0400e-01,  1.5372e+15],\n",
            "        [ 1.2760e+00,  2.4100e+00,  4.0640e+00,  7.8100e+00,  1.5372e+15,\n",
            "          9.7200e-01,  6.4600e-01, -1.5372e+15, -1.5372e+15,  1.8970e+00,\n",
            "          3.1000e-01,  8.0500e-01,  1.5372e+15, -1.5372e+15,  1.3290e+00,\n",
            "          3.9750e+00,  9.2600e-01,  3.5700e+00,  4.1100e-01,  9.2800e-01,\n",
            "          5.0510e+00, -1.5372e+15,  1.5372e+15,  3.9730e+00,  2.9030e+00,\n",
            "          1.3200e-01,  5.3200e-01,  1.5372e+15,  1.0710e+00,  5.5680e+00,\n",
            "          1.3490e+00,  2.2100e+00,  6.5400e-01,  6.4600e-01,  4.9650e+00,\n",
            "          1.9870e+00,  4.8800e-01,  4.6610e+00,  8.1530e+00,  4.8450e+00,\n",
            "          1.5372e+15,  1.5372e+15,  1.6600e-01,  4.0400e-01,  1.5720e+00,\n",
            "          1.0220e+00,  1.5372e+15,  1.5372e+15, -1.5372e+15, -4.0380e+00],\n",
            "        [ 1.7200e-01,  2.4390e+00,  1.4800e-01, -1.5372e+15, -4.4790e+00,\n",
            "          4.0500e-01,  7.1000e-02,  1.6500e-01, -1.5372e+15,  2.3600e-01,\n",
            "          1.0100e-01,  1.5100e-01, -6.8200e-01, -1.2590e+00,  1.8600e-01,\n",
            "         -2.2600e-01, -1.5372e+15,  1.3800e-01,  1.5372e+15,  5.3000e-02,\n",
            "         -4.4200e-01, -1.5372e+15,  3.0120e+00, -1.5372e+15,  1.5372e+15,\n",
            "         -1.5372e+15,  1.5000e-01, -3.3500e-01,  3.2000e+00, -1.0700e-01,\n",
            "         -1.5372e+15, -1.5372e+15,  7.0000e-03,  1.5372e+15,  1.5372e+15,\n",
            "          1.5372e+15,  6.2000e-02,  5.2600e-01,  4.2400e-01, -1.5372e+15,\n",
            "          1.5372e+15,  5.0000e-03,  9.2000e-02,  1.5372e+15, -1.0000e-01,\n",
            "         -6.5000e-02,  1.2930e+00, -3.9900e-01, -6.4000e-02,  1.5372e+15],\n",
            "        [ 1.0380e+00, -1.5372e+15,  1.5372e+15,  5.7120e+00,  1.0276e+01,\n",
            "          1.5372e+15,  3.2400e-01,  1.2310e+00,  3.0380e+00,  6.4000e-01,\n",
            "         -1.5372e+15,  5.0700e-01, -1.5372e+15,  5.6650e+00,  6.8700e-01,\n",
            "          3.6650e+00,  9.4000e-02, -2.3640e+00, -1.5372e+15,  4.9600e-01,\n",
            "          8.4500e+00,  1.3130e+00,  1.5372e+15,  1.5372e+15,  1.0930e+00,\n",
            "          2.3000e-02,  6.5700e-01,  3.1320e+00, -1.5372e+15, -1.5372e+15,\n",
            "          6.7500e-01,  1.7390e+00,  6.1300e-01,  3.1900e-01, -1.2980e+00,\n",
            "          5.4100e-01,  1.6400e+00,  4.1270e+00,  4.6210e+00,  1.7660e+00,\n",
            "         -1.5372e+15,  5.5200e-01,  6.9000e-02,  2.4600e-01, -1.5372e+15,\n",
            "          1.5372e+15,  1.5372e+15,  5.2060e+00, -1.5372e+15, -4.5090e+00],\n",
            "        [ 5.6000e-01,  9.9600e-01,  1.5372e+15,  1.5870e+00,  1.7700e-01,\n",
            "         -1.0100e-01,  1.3200e-01,  8.3000e-02,  1.5372e+15, -3.7300e-01,\n",
            "          1.8000e-02, -1.5372e+15,  5.8700e+00,  1.5372e+15,  1.5100e-01,\n",
            "          5.5800e-01, -1.5372e+15, -1.4380e+00,  2.2600e-01,  3.6600e-01,\n",
            "          1.1860e+00,  3.3200e-01,  2.4090e+00,  1.5630e+00, -1.5372e+15,\n",
            "          1.0800e-01,  8.5000e-02,  1.5670e+00,  1.5372e+15,  1.0470e+00,\n",
            "         -1.5372e+15,  1.8100e-01,  5.5400e-01,  1.3900e-01,  1.5372e+15,\n",
            "          1.0280e+00, -1.5372e+15, -2.7100e-01, -1.5372e+15, -1.5372e+15,\n",
            "          3.3590e+00,  2.3000e-02,  1.5372e+15, -1.5372e+15, -1.5372e+15,\n",
            "          1.5372e+15,  5.6610e+00, -1.3100e-01,  2.9600e-01, -5.3200e-01],\n",
            "        [ 7.8200e-01, -2.8820e+00,  9.1500e-01,  3.5670e+00, -1.5810e+00,\n",
            "          1.5372e+15,  5.0800e-01,  5.2000e-01,  2.1860e+00,  7.2000e-01,\n",
            "         -2.1000e-02,  2.9100e-01,  2.5770e+00,  2.0500e+00, -1.5372e+15,\n",
            "          3.4890e+00,  1.5372e+15, -1.5372e+15,  1.5800e-01,  2.5100e-01,\n",
            "          1.5372e+15,  1.8200e+00,  9.4500e-01, -1.5372e+15, -1.5372e+15,\n",
            "         -1.3400e-01, -1.5372e+15,  3.5960e+00, -2.1160e+00, -1.5372e+15,\n",
            "          1.5372e+15,  7.7700e-01,  1.5372e+15, -3.8200e-01,  4.2330e+00,\n",
            "          4.0500e-01,  2.1460e+00,  1.3280e+00,  4.9930e+00,  3.9280e+00,\n",
            "          1.8260e+00, -1.5372e+15,  9.6000e-02,  1.1600e-01, -1.5372e+15,\n",
            "          4.0400e-01, -7.2800e-01,  2.8970e+00,  2.0900e-01,  4.7410e+00],\n",
            "        [ 2.3300e-01,  1.5372e+15, -1.5372e+15,  1.5372e+15,  1.0640e+00,\n",
            "          1.0640e+00,  3.5000e-01,  1.5372e+15,  3.9500e-01,  1.5372e+15,\n",
            "          1.0500e-01,  1.3900e-01, -8.2900e-01,  1.1340e+00,  1.2600e-01,\n",
            "          5.5700e-01,  7.5000e-02,  1.5372e+15,  1.5372e+15,  1.5000e-01,\n",
            "          9.8000e-01,  2.1200e-01, -2.3200e+00, -8.0500e-01, -6.2000e-02,\n",
            "         -1.5372e+15,  9.5000e-02, -6.6400e-01, -1.0100e-01,  1.5372e+15,\n",
            "          1.5372e+15,  1.9600e-01,  4.7500e-01,  1.5372e+15,  5.6980e+00,\n",
            "          4.4500e-01,  1.4650e+00,  1.0250e+00,  2.3000e-01,  1.1500e-01,\n",
            "         -1.5372e+15, -1.5372e+15,  1.5372e+15,  4.0000e-02,  6.0000e-03,\n",
            "          6.4000e-02, -3.9870e+00,  3.6900e-01, -1.9100e-01,  3.8070e+00],\n",
            "        [-1.5372e+15, -4.9900e-01, -1.5372e+15,  6.1570e+00, -1.5372e+15,\n",
            "          6.1100e-01, -1.5372e+15,  1.2160e+00,  2.3550e+00,  2.3000e-01,\n",
            "          1.6600e-01, -1.5372e+15,  6.6280e+00,  5.9910e+00,  7.3900e-01,\n",
            "         -1.5372e+15,  2.8000e-01, -1.9290e+00,  2.3700e-01,  1.5372e+15,\n",
            "         -9.8800e-01,  1.8810e+00, -2.5800e-01,  1.5372e+15,  2.3570e+00,\n",
            "         -1.5372e+15,  4.6800e-01, -2.4900e-01, -2.3060e+00,  4.1300e+00,\n",
            "          3.5300e-01,  4.8800e-01, -1.5372e+15, -4.4200e-01,  1.5372e+15,\n",
            "          3.1000e-01,  3.9480e+00, -1.5372e+15,  5.5500e+00, -1.5372e+15,\n",
            "          1.2660e+00,  4.4800e-01,  8.8000e-02,  2.3700e-01, -1.5372e+15,\n",
            "          1.5372e+15,  2.4990e+00,  4.0420e+00, -2.4000e-02, -4.8760e+00]])\n",
            "calculating 2.bias\n",
            "tensor([ 2.6000e-02,  6.9000e-02,  4.3000e-02,  2.0200e-01, -3.0000e-03,\n",
            "         1.5372e+15,  7.4000e-02, -1.5372e+15,  2.0000e-02,  1.5372e+15])\n",
            "Test Accuracy: 0.090300\n",
            "Avg loss (1/0): 4624184941915923137469349888.000000\n",
            "Avg loss (1/1): 1850629901801664142920122368.000000\n",
            "Avg loss (1/2): 914580126441553829830852608.000000\n",
            "Avg loss (1/3): 1140137689602837372215492608.000000\n",
            "Avg loss (1/4): 1339778610358227122314543104.000000\n",
            "Avg loss (1/5): 1420949892092747557128634368.000000\n",
            "Avg loss (1/6): 1380412803055889343261442048.000000\n",
            "Avg loss (1/7): 1229334706961971063908466688.000000\n",
            "Avg loss (1/8): 1017931699463326334669815808.000000\n",
            "Avg loss (1/9): 830960961768452137517318144.000000\n",
            "Avg loss (1/10): 669464513556584177602658304.000000\n",
            "Avg loss (1/11): 505700259521782512831430656.000000\n",
            "Avg loss (1/12): 352338567192003433245704192.000000\n",
            "Avg loss (1/13): 269781067980575946756325376.000000\n",
            "Avg loss (1/14): 239630436641163969636073472.000000\n",
            "Avg loss (1/15): 213169338583934639934537728.000000\n",
            "Avg loss (1/16): 180664755126764734351671296.000000\n",
            "Avg loss (1/17): 144794351075789673416622080.000000\n",
            "Avg loss (1/18): 111124108637230024412364800.000000\n",
            "Avg loss (1/19): 82825963901304218448822272.000000\n",
            "sharing worker_0 model\n",
            "sharing worker_1 model\n",
            "sharing worker_2 model\n",
            "{'worker_0': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:48415412831 -> worker_0:4888677663]\n",
            "\t-> (Wrapper)>[PointerTensor | me:49085018696 -> worker_1:87980433104]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87122489683 -> worker_2:46323713439]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:73244915982 -> worker_0:29730100744]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87437958921 -> worker_1:58826360498]\n",
            "\t-> (Wrapper)>[PointerTensor | me:99155005034 -> worker_2:76859347877]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95146225090 -> worker_0:84169745162]\n",
            "\t-> (Wrapper)>[PointerTensor | me:48226152709 -> worker_1:71343538969]\n",
            "\t-> (Wrapper)>[PointerTensor | me:39323302780 -> worker_2:17302346505]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:92306959878 -> worker_0:97872517030]\n",
            "\t-> (Wrapper)>[PointerTensor | me:86802043037 -> worker_1:95712132552]\n",
            "\t-> (Wrapper)>[PointerTensor | me:43593990106 -> worker_2:6925477614]\n",
            "\t*crypto provider: me*}, 'worker_1': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:51443275459 -> worker_0:78105033455]\n",
            "\t-> (Wrapper)>[PointerTensor | me:98968581408 -> worker_1:70962633882]\n",
            "\t-> (Wrapper)>[PointerTensor | me:98827226796 -> worker_2:99490022478]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:60750755389 -> worker_0:83732578107]\n",
            "\t-> (Wrapper)>[PointerTensor | me:62644794745 -> worker_1:51253956005]\n",
            "\t-> (Wrapper)>[PointerTensor | me:6464897756 -> worker_2:43262659339]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87864700278 -> worker_0:92160619630]\n",
            "\t-> (Wrapper)>[PointerTensor | me:81772451808 -> worker_1:3882173043]\n",
            "\t-> (Wrapper)>[PointerTensor | me:42038488462 -> worker_2:13163358508]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:11415209229 -> worker_0:89850175877]\n",
            "\t-> (Wrapper)>[PointerTensor | me:46355690293 -> worker_1:49635365627]\n",
            "\t-> (Wrapper)>[PointerTensor | me:34464205293 -> worker_2:50442324194]\n",
            "\t*crypto provider: me*}, 'worker_2': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:57156487266 -> worker_0:37999264360]\n",
            "\t-> (Wrapper)>[PointerTensor | me:73198171357 -> worker_1:81648720948]\n",
            "\t-> (Wrapper)>[PointerTensor | me:15080603844 -> worker_2:28275969028]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79432923354 -> worker_0:69880549343]\n",
            "\t-> (Wrapper)>[PointerTensor | me:34869437144 -> worker_1:54272593614]\n",
            "\t-> (Wrapper)>[PointerTensor | me:20829406792 -> worker_2:82195755374]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:1589531771 -> worker_0:50307955864]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95980754067 -> worker_1:9536410261]\n",
            "\t-> (Wrapper)>[PointerTensor | me:26392315453 -> worker_2:15758911827]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:70970593936 -> worker_0:72068779134]\n",
            "\t-> (Wrapper)>[PointerTensor | me:72205913293 -> worker_1:34139197790]\n",
            "\t-> (Wrapper)>[PointerTensor | me:45366933727 -> worker_2:54422509937]\n",
            "\t*crypto provider: me*}}\n",
            "grad sum =  {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:48415412831 -> worker_0:4888677663]\n",
            "\t-> (Wrapper)>[PointerTensor | me:49085018696 -> worker_1:87980433104]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87122489683 -> worker_2:46323713439]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:73244915982 -> worker_0:29730100744]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87437958921 -> worker_1:58826360498]\n",
            "\t-> (Wrapper)>[PointerTensor | me:99155005034 -> worker_2:76859347877]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95146225090 -> worker_0:84169745162]\n",
            "\t-> (Wrapper)>[PointerTensor | me:48226152709 -> worker_1:71343538969]\n",
            "\t-> (Wrapper)>[PointerTensor | me:39323302780 -> worker_2:17302346505]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:92306959878 -> worker_0:97872517030]\n",
            "\t-> (Wrapper)>[PointerTensor | me:86802043037 -> worker_1:95712132552]\n",
            "\t-> (Wrapper)>[PointerTensor | me:43593990106 -> worker_2:6925477614]\n",
            "\t*crypto provider: me*}\n",
            "calculating 0.weight\n",
            "tensor([[ 0.0000e+00,  1.0000e-03,  1.5372e+15,  ...,  0.0000e+00,\n",
            "         -1.5372e+15, -1.0000e-03],\n",
            "        [-1.0000e-03,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-03,\n",
            "          1.5372e+15,  0.0000e+00],\n",
            "        [ 1.5372e+15, -1.0000e-03,  1.0000e-03,  ...,  0.0000e+00,\n",
            "         -1.5372e+15,  1.0000e-03],\n",
            "        ...,\n",
            "        [-1.5372e+15,  0.0000e+00,  0.0000e+00,  ...,  1.5372e+15,\n",
            "          1.5372e+15,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.5372e+15,\n",
            "          1.0000e-03,  1.0000e-03],\n",
            "        [ 1.0000e-03,  1.0000e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          1.5372e+15,  1.5372e+15]])\n",
            "calculating 0.bias\n",
            "tensor([ 3.4666e+11,  5.8330e+10,  1.5373e+15,  1.0146e+11,  4.4799e+10,\n",
            "         7.7409e+10,  1.5374e+15,  2.3962e+11,  8.9981e+10,  1.3458e+11,\n",
            "         4.4415e+10,  6.1366e+10,  4.3584e+10,  8.4403e+10, -1.5370e+15,\n",
            "         1.0215e+11,  3.5849e+11,  1.6911e+10,  3.3278e+11,  4.8663e+11,\n",
            "         9.2314e+10,  5.8543e+10,  3.2600e+11, -1.5371e+15,  1.5375e+15,\n",
            "         4.9098e+11,  1.7313e+11,  1.0138e+11,  1.6831e+11,  1.8786e+11,\n",
            "         1.9016e+11,  9.1526e+10,  3.4307e+11,  2.3316e+10,  1.5374e+15,\n",
            "         3.2521e+10,  2.2383e+11,  2.3500e+11,  2.8963e+11,  2.0795e+11,\n",
            "         8.7423e+10,  1.6907e+11,  1.9681e+11,  3.3223e+10,  7.5721e+11,\n",
            "         2.7691e+11,  1.5373e+15,  1.7812e+11,  1.3748e+11,  1.5374e+15])\n",
            "calculating 2.weight\n",
            "tensor([[ 1.4580e+14,  2.4651e+13,  1.5218e+15,  5.6234e+13,  3.0385e+13,\n",
            "          1.0220e+13, -1.4721e+15, -1.3729e+15,  1.3478e+13,  5.8395e+13,\n",
            "          3.2967e+13,  7.7678e+12,  1.1710e+13,  3.7067e+13,  1.1039e+14,\n",
            "          2.9867e+13,  1.4668e+15,  1.9751e+13,  1.6521e+15, -2.9161e+13,\n",
            "          4.6874e+13,  1.2352e+13, -1.4701e+15,  2.2254e+13,  1.5763e+15,\n",
            "          1.4047e+14,  3.0956e+13,  3.0270e+13,  7.7657e+13,  1.5766e+15,\n",
            "          1.2673e+14,  3.4652e+13,  1.9105e+14,  1.5577e+15,  1.5863e+13,\n",
            "         -1.4724e+15,  3.6259e+13,  9.8754e+13,  3.3769e+13,  1.8208e+13,\n",
            "         -1.5443e+13,  3.3066e+13, -1.7724e+13,  2.4474e+12,  1.8577e+14,\n",
            "         -6.8641e+12,  1.5154e+15,  1.5744e+15,  3.5340e+13,  1.7634e+13],\n",
            "        [ 2.1397e+14,  3.2236e+13,  3.4603e+13,  4.3232e+12, -1.5883e+15,\n",
            "          7.2935e+13,  3.6426e+12,  1.5783e+15, -6.9294e+12,  4.1993e+13,\n",
            "         -4.8782e+13, -8.5957e+12,  5.4438e+12,  3.3328e+12,  6.3904e+13,\n",
            "          5.6205e+13,  1.7803e+14,  1.7787e+13,  1.9297e+14, -1.1378e+14,\n",
            "         -1.5686e+12,  7.2118e+12,  9.9320e+13,  1.7630e+13, -7.4484e+11,\n",
            "          1.4127e+14,  4.9004e+13,  1.6330e+15,  7.3398e+13, -1.4498e+15,\n",
            "          1.5363e+14,  7.9776e+13, -1.0556e+13,  1.5035e+15,  4.0806e+13,\n",
            "         -1.4990e+15, -1.5280e+15,  1.7679e+14,  2.2038e+14, -1.5222e+15,\n",
            "         -1.5108e+15,  1.5940e+15, -1.5743e+15,  1.5423e+15,  1.7708e+14,\n",
            "          1.9587e+14, -1.5085e+15,  3.8711e+13,  9.5389e+13, -8.4800e+12],\n",
            "        [ 1.5917e+14, -1.5506e+15,  2.2204e+13,  1.5442e+15,  1.0160e+13,\n",
            "         -1.4768e+15,  3.4546e+13,  5.7467e+13,  1.5581e+15,  2.1840e+13,\n",
            "          2.2976e+13,  4.2862e+12,  2.7399e+12, -1.5245e+15,  1.7298e+13,\n",
            "          4.7714e+12,  9.4393e+13,  1.3195e+13,  1.2403e+14, -1.2662e+15,\n",
            "          6.8189e+13, -1.5309e+15, -1.5282e+15,  3.7358e+13,  7.4130e+13,\n",
            "          1.1776e+13, -3.2023e+13, -3.4281e+12,  5.9726e+13,  3.0993e+13,\n",
            "         -1.4697e+15, -1.4816e+15, -8.1689e+13,  1.5578e+15,  1.2404e+11,\n",
            "          4.1036e+13,  9.0394e+13,  4.8496e+13,  6.7221e+13,  1.9050e+13,\n",
            "          8.5971e+12,  6.4941e+13,  8.0735e+13,  1.4284e+13,  9.4765e+13,\n",
            "          1.2739e+14, -1.5090e+15, -2.8391e+13,  3.2622e+13, -7.2324e+12],\n",
            "        [ 1.5128e+15,  1.5623e+15,  1.3333e+13, -4.5858e+12, -3.2291e+12,\n",
            "         -1.4811e+15,  2.1171e+13,  1.1242e+14,  1.5594e+15, -1.3114e+12,\n",
            "          2.7238e+13,  3.5782e+12, -9.9148e+12,  9.5433e+13,  1.2828e+12,\n",
            "          1.5223e+15,  1.5272e+15,  1.2502e+13,  3.1423e+13,  1.6194e+15,\n",
            "          7.4437e+12,  1.1939e+13, -1.3195e+13,  2.6034e+13,  4.1899e+13,\n",
            "          1.5413e+15, -4.8191e+13, -1.5317e+15,  7.4947e+13, -4.5245e+12,\n",
            "         -6.7074e+12,  2.1575e+13,  1.5186e+15,  1.8740e+13, -1.5254e+15,\n",
            "          4.0236e+13,  2.1035e+13, -1.5124e+15,  7.6284e+12,  2.6590e+12,\n",
            "         -2.9105e+12,  1.5294e+15,  8.1122e+13,  2.6007e+11,  1.5451e+15,\n",
            "          1.5637e+15, -1.6189e+15, -5.6236e+13,  1.8353e+13, -1.4482e+15],\n",
            "        [ 2.2568e+14,  2.0458e+13, -5.9989e+12, -1.4811e+15,  2.4235e+13,\n",
            "         -2.5153e+13,  3.9474e+13, -2.8695e+13,  1.5690e+15,  3.4252e+13,\n",
            "          1.3104e+13,  6.6179e+12, -1.5299e+15, -4.0832e+12,  8.3335e+13,\n",
            "          3.7502e+13,  2.6676e+14,  2.6817e+12,  8.6730e+13, -1.6605e+15,\n",
            "         -1.5128e+15, -1.5378e+15,  4.7503e+13, -1.6282e+13, -8.9789e+12,\n",
            "          1.8051e+14,  1.1709e+14,  1.5740e+15, -1.3868e+12,  1.6626e+15,\n",
            "          1.2662e+14,  6.5317e+13,  1.2289e+14, -1.9444e+12, -1.5830e+15,\n",
            "          5.5135e+12,  1.6094e+15,  6.6361e+13,  9.3719e+13,  2.8531e+13,\n",
            "          1.5438e+15, -4.4617e+12, -1.0513e+14,  6.7625e+11,  1.2625e+14,\n",
            "          8.5416e+12,  1.5488e+15,  1.3503e+14, -1.4571e+15, -8.7139e+13],\n",
            "        [-1.3576e+15,  4.7093e+13,  1.7306e+13,  2.4566e+13,  2.0339e+13,\n",
            "         -4.1341e+13,  6.2386e+13,  1.6828e+15, -1.5227e+15,  3.3357e+13,\n",
            "          3.8278e+13,  6.9148e+12,  1.3369e+13,  1.4479e+13,  1.0994e+14,\n",
            "         -1.5188e+15,  1.5848e+14, -1.5251e+15,  2.2378e+14, -6.7987e+13,\n",
            "         -1.5820e+12,  8.8141e+11, -1.4701e+15,  1.5342e+15,  2.2743e+13,\n",
            "          1.3740e+14,  6.8329e+13, -1.4762e+15,  1.8579e+14,  9.5870e+13,\n",
            "          1.1442e+14,  2.6869e+13,  1.7211e+14,  1.4571e+13, -1.5409e+15,\n",
            "          5.3317e+13, -1.3488e+13,  1.3013e+14,  1.2903e+14,  2.1074e+13,\n",
            "          1.5539e+15,  7.2312e+13,  2.7592e+13, -7.1835e+12,  1.6990e+14,\n",
            "         -1.0757e+14,  8.0961e+12,  9.6258e+13,  8.1487e+13, -5.3879e+13],\n",
            "        [ 3.2906e+14, -1.5032e+15,  2.9626e+13, -1.5259e+15,  3.7533e+13,\n",
            "          3.2453e+13,  6.2225e+13,  6.8590e+13, -1.5196e+15,  4.3643e+13,\n",
            "          8.4734e+12,  1.1884e+13,  1.0053e+13,  1.5500e+15,  1.1484e+14,\n",
            "         -1.4910e+15,  2.6097e+14,  1.1598e+13,  2.4401e+14, -1.7232e+14,\n",
            "          1.5863e+15,  6.2213e+12,  1.6225e+15,  1.2649e+13, -4.9603e+13,\n",
            "          1.5960e+14,  9.8498e+13,  1.8745e+13,  1.5555e+15,  1.0402e+14,\n",
            "          1.3901e+14,  7.7214e+13,  1.5142e+14, -6.9817e+12,  1.0355e+13,\n",
            "          4.1990e+13,  1.6703e+15,  1.7015e+14,  2.1287e+14,  4.2273e+13,\n",
            "          2.4800e+13, -1.4204e+15,  1.4612e+15,  1.5453e+15, -1.3162e+15,\n",
            "         -7.7940e+13,  3.2712e+13, -1.3879e+15,  1.5959e+15,  1.4128e+15],\n",
            "        [-1.5499e+15, -4.3511e+12, -6.7175e+12, -1.5133e+15,  1.6099e+13,\n",
            "         -4.1021e+13, -1.5160e+15,  5.0741e+13,  1.5616e+15,  3.1486e+13,\n",
            "          1.5982e+13,  2.3835e+12,  8.1625e+12,  5.7501e+12, -1.4308e+15,\n",
            "          3.8861e+13,  2.2199e+13,  2.7415e+13,  8.0275e+13,  1.7143e+15,\n",
            "          3.7657e+11,  3.9391e+12, -1.0003e+13,  7.0431e+13,  1.8349e+14,\n",
            "          1.0607e+14,  1.3484e+14,  5.1418e+13,  4.2350e+13, -1.4301e+15,\n",
            "          8.5205e+13, -1.4920e+15,  4.8708e+13,  1.0106e+13, -5.7047e+13,\n",
            "          1.5514e+15,  5.5359e+13,  8.2662e+13,  1.1491e+14,  1.1746e+12,\n",
            "         -4.2140e+13,  4.9275e+13, -1.3893e+15,  1.5998e+11, -1.4439e+15,\n",
            "          8.3608e+13,  1.5804e+15,  4.6585e+13,  6.7279e+13,  6.7828e+13],\n",
            "        [ 7.5879e+12, -3.2932e+12, -1.4703e+15, -1.5488e+15,  1.5401e+15,\n",
            "          6.6348e+13,  1.5400e+15, -7.4074e+13, -5.1218e+12, -7.4916e+11,\n",
            "         -2.7360e+12,  9.3232e+10,  2.6466e+12,  1.6931e+13, -1.5184e+15,\n",
            "         -6.3528e+12,  3.7798e+13,  1.5394e+15, -3.7540e+13,  1.5627e+15,\n",
            "         -8.8021e+12,  2.1514e+11,  4.6235e+13, -1.5264e+15,  1.5552e+15,\n",
            "          1.4785e+13,  6.3164e+12,  1.2204e+13,  4.1480e+13,  1.5240e+15,\n",
            "         -2.7982e+13, -2.5885e+13, -1.4338e+13, -1.6120e+13,  4.4932e+13,\n",
            "          3.2330e+12,  3.1546e+12,  2.7818e+11,  2.4542e+13,  3.4401e+12,\n",
            "          6.7109e+13,  7.2314e+13,  1.4266e+15,  7.7530e+12, -1.5459e+15,\n",
            "         -3.3757e+13, -1.5111e+15,  4.6206e+13, -2.0502e+13,  9.6069e+13],\n",
            "        [ 3.3990e+14, -1.4856e+15, -1.4885e+15,  6.4835e+13, -1.4948e+15,\n",
            "         -4.2946e+13,  8.7675e+13, -1.4010e+15, -1.5021e+15,  1.5983e+15,\n",
            "          2.7481e+13,  1.2970e+13, -1.5251e+15, -1.5174e+13,  2.1376e+14,\n",
            "          1.6118e+15,  3.3020e+14, -5.1841e+12,  2.9650e+14,  1.3352e+15,\n",
            "          2.3826e+13,  5.5313e+12,  1.6871e+15, -4.6859e+13,  3.0924e+13,\n",
            "         -1.3009e+15,  1.6275e+14,  1.0152e+14, -1.5772e+15, -1.3735e+15,\n",
            "          1.9603e+14,  8.1834e+13,  2.6863e+14,  1.4010e+13, -2.8200e+13,\n",
            "          5.0634e+12, -1.4430e+15,  2.0705e+14,  1.7553e+15,  1.5816e+15,\n",
            "          1.0571e+13, -1.4466e+15,  6.2476e+13,  3.9639e+12,  2.1513e+14,\n",
            "          1.5682e+15,  3.3511e+12,  1.5605e+14,  1.1426e+14,  1.4748e+15]])\n",
            "calculating 2.bias\n",
            "tensor([ 2.1500e-01, -1.5372e+15,  1.8200e-01,  1.7100e-01,  1.5372e+15,\n",
            "         1.6000e-01, -1.5372e+15,  1.5100e-01,  8.3000e-02,  2.7000e-02])\n",
            "Test Accuracy: 0.049700\n",
            "Avg loss (2/0): 5469711083870094862751629312.000000\n",
            "Avg loss (2/1): 2960303679010485728700268544.000000\n",
            "Avg loss (2/2): 1711729165553588192132726784.000000\n",
            "Avg loss (2/3): 1819128913455824405492924416.000000\n",
            "Avg loss (2/4): 2079106256384242600237858816.000000\n",
            "Avg loss (2/5): 2192992174650700271299067904.000000\n",
            "Avg loss (2/6): 2110785218916905487419047936.000000\n",
            "Avg loss (2/7): 1832474025988508843514003456.000000\n",
            "Avg loss (2/8): 1422797517979170305818492928.000000\n",
            "Avg loss (2/9): 1010175064941260352729907200.000000\n",
            "Avg loss (2/10): 705753465007219671409098752.000000\n",
            "Avg loss (2/11): 527592929175437303527505920.000000\n",
            "Avg loss (2/12): 438663131350955368399241216.000000\n",
            "Avg loss (2/13): 394601275349980880828366848.000000\n",
            "Avg loss (2/14): 357272554722886674435538944.000000\n",
            "Avg loss (2/15): 317726461671156285100261376.000000\n",
            "Avg loss (2/16): 279108701014935328950059008.000000\n",
            "Avg loss (2/17): 244154260817568278945726464.000000\n",
            "Avg loss (2/18): 213593687484606254459912192.000000\n",
            "Avg loss (2/19): 185817576382594391952326656.000000\n",
            "sharing worker_0 model\n",
            "sharing worker_1 model\n",
            "sharing worker_2 model\n",
            "{'worker_0': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95317050995 -> worker_0:29705060850]\n",
            "\t-> (Wrapper)>[PointerTensor | me:26579368779 -> worker_1:29581755764]\n",
            "\t-> (Wrapper)>[PointerTensor | me:97130162173 -> worker_2:10687435744]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:12020045032 -> worker_0:35090499941]\n",
            "\t-> (Wrapper)>[PointerTensor | me:70148540002 -> worker_1:49861365896]\n",
            "\t-> (Wrapper)>[PointerTensor | me:74943861707 -> worker_2:1546496956]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:44733541476 -> worker_0:78390757431]\n",
            "\t-> (Wrapper)>[PointerTensor | me:58743039886 -> worker_1:31831445003]\n",
            "\t-> (Wrapper)>[PointerTensor | me:21532785611 -> worker_2:30782374002]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:26874132212 -> worker_0:76604235107]\n",
            "\t-> (Wrapper)>[PointerTensor | me:42429413889 -> worker_1:49554221773]\n",
            "\t-> (Wrapper)>[PointerTensor | me:69784260187 -> worker_2:65446521491]\n",
            "\t*crypto provider: me*}, 'worker_1': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:54341147640 -> worker_0:71874763604]\n",
            "\t-> (Wrapper)>[PointerTensor | me:43623842079 -> worker_1:27492281859]\n",
            "\t-> (Wrapper)>[PointerTensor | me:2468914073 -> worker_2:13578381869]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:31230894661 -> worker_0:3111940270]\n",
            "\t-> (Wrapper)>[PointerTensor | me:73598651336 -> worker_1:62177791397]\n",
            "\t-> (Wrapper)>[PointerTensor | me:16503006634 -> worker_2:90842490334]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:32248639648 -> worker_0:17679064290]\n",
            "\t-> (Wrapper)>[PointerTensor | me:14384323933 -> worker_1:38274298907]\n",
            "\t-> (Wrapper)>[PointerTensor | me:58587761819 -> worker_2:48559108449]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:48130618169 -> worker_0:2191513333]\n",
            "\t-> (Wrapper)>[PointerTensor | me:278046723 -> worker_1:47532773032]\n",
            "\t-> (Wrapper)>[PointerTensor | me:44612311811 -> worker_2:19555093205]\n",
            "\t*crypto provider: me*}, 'worker_2': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:62091248347 -> worker_0:60044084348]\n",
            "\t-> (Wrapper)>[PointerTensor | me:1551605185 -> worker_1:51412440788]\n",
            "\t-> (Wrapper)>[PointerTensor | me:48620523700 -> worker_2:12456952906]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:73788660753 -> worker_0:16369473269]\n",
            "\t-> (Wrapper)>[PointerTensor | me:68575509606 -> worker_1:48032246746]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87079414695 -> worker_2:28465200350]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:53300534388 -> worker_0:25530185356]\n",
            "\t-> (Wrapper)>[PointerTensor | me:72932718184 -> worker_1:51882044461]\n",
            "\t-> (Wrapper)>[PointerTensor | me:57036466846 -> worker_2:90238109339]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:6725008720 -> worker_0:71631587391]\n",
            "\t-> (Wrapper)>[PointerTensor | me:28343001488 -> worker_1:50572336232]\n",
            "\t-> (Wrapper)>[PointerTensor | me:85847957234 -> worker_2:22357354901]\n",
            "\t*crypto provider: me*}}\n",
            "grad sum =  {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95317050995 -> worker_0:29705060850]\n",
            "\t-> (Wrapper)>[PointerTensor | me:26579368779 -> worker_1:29581755764]\n",
            "\t-> (Wrapper)>[PointerTensor | me:97130162173 -> worker_2:10687435744]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:12020045032 -> worker_0:35090499941]\n",
            "\t-> (Wrapper)>[PointerTensor | me:70148540002 -> worker_1:49861365896]\n",
            "\t-> (Wrapper)>[PointerTensor | me:74943861707 -> worker_2:1546496956]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:44733541476 -> worker_0:78390757431]\n",
            "\t-> (Wrapper)>[PointerTensor | me:58743039886 -> worker_1:31831445003]\n",
            "\t-> (Wrapper)>[PointerTensor | me:21532785611 -> worker_2:30782374002]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:26874132212 -> worker_0:76604235107]\n",
            "\t-> (Wrapper)>[PointerTensor | me:42429413889 -> worker_1:49554221773]\n",
            "\t-> (Wrapper)>[PointerTensor | me:69784260187 -> worker_2:65446521491]\n",
            "\t*crypto provider: me*}\n",
            "calculating 0.weight\n",
            "tensor([[-1.5372e+15,  0.0000e+00,  0.0000e+00,  ..., -1.5372e+15,\n",
            "          0.0000e+00,  1.5372e+15],\n",
            "        [ 1.0000e-03, -1.5372e+15,  0.0000e+00,  ...,  0.0000e+00,\n",
            "         -1.0000e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.5372e+15,\n",
            "          1.0000e-03, -1.5372e+15],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00, -1.5372e+15,  ...,  0.0000e+00,\n",
            "         -1.5372e+15,  0.0000e+00],\n",
            "        [-1.5372e+15,  0.0000e+00,  0.0000e+00,  ..., -1.5372e+15,\n",
            "         -1.5372e+15,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  1.0000e-03,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "calculating 0.bias\n",
            "tensor([ 5.1416e+10,  3.5899e+11,  3.2891e+11, -1.5371e+15,  4.2824e+10,\n",
            "         2.1683e+11,  3.2123e+11,  4.3267e+11,  1.0144e+11,  6.1777e+10,\n",
            "         1.5373e+15, -1.5372e+15,  1.5377e+15, -1.5372e+15,  1.5374e+15,\n",
            "         1.5373e+15,  1.9602e+11,  3.2239e+10, -1.5370e+15,  2.1389e+11,\n",
            "         5.1549e+10,  3.8090e+10,  4.6973e+11,  1.6976e+11,  7.5104e+10,\n",
            "         1.5374e+15,  1.0554e+11,  1.5374e+15,  4.9532e+10,  3.7146e+11,\n",
            "         1.5373e+15,  2.6115e+11,  5.8277e+11,  3.3619e+10,  9.8707e+10,\n",
            "        -1.5371e+15,  8.5505e+10,  2.0065e+11,  1.6011e+11, -1.5371e+15,\n",
            "         7.6055e+10,  2.3245e+11, -1.5371e+15,  1.0445e+10,  3.5613e+11,\n",
            "         3.1253e+11,  1.3490e+11,  5.5877e+11,  8.7215e+10,  3.3603e+11])\n",
            "calculating 2.weight\n",
            "tensor([[ 1.6086e+15,  4.0183e+13, -3.0928e+13,  9.2739e+12, -1.5721e+12,\n",
            "         -5.3208e+13,  1.6849e+14,  2.6414e+14, -1.5552e+15, -1.5305e+15,\n",
            "          2.3804e+13,  1.5379e+15,  1.3659e+14,  2.8023e+13,  8.8914e+13,\n",
            "         -3.7377e+13, -2.3414e+14,  7.6989e+12,  6.8239e+13,  1.1112e+14,\n",
            "          2.9718e+12,  4.3454e+12,  2.2292e+14,  3.5659e+13,  2.8189e+12,\n",
            "          1.3261e+13, -2.3147e+13, -1.6768e+15,  3.6042e+13,  1.1314e+13,\n",
            "          1.5061e+14,  6.4084e+13,  1.2422e+14,  1.8717e+13,  1.5733e+15,\n",
            "          7.5860e+13,  3.4925e+13,  9.3904e+13,  1.4961e+15, -1.2587e+13,\n",
            "          1.5725e+15, -1.4352e+15,  8.9290e+13,  1.1359e+13,  7.2658e+13,\n",
            "         -1.5930e+13,  2.1693e+13, -2.7049e+14,  5.8509e+13,  2.9331e+13],\n",
            "        [ 8.0962e+13,  1.6329e+15,  1.6456e+14, -2.0964e+12,  8.4075e+12,\n",
            "          9.1888e+13,  8.7044e+13, -3.5384e+13,  1.5557e+15,  1.1451e+14,\n",
            "         -1.5804e+15,  3.7785e+12,  1.5755e+14,  3.2564e+13,  4.1963e+13,\n",
            "          5.3141e+13, -1.4745e+15,  1.5293e+13, -1.4387e+15, -9.1480e+13,\n",
            "          2.5708e+13,  1.4626e+12,  3.0825e+14,  2.2439e+13,  1.5603e+15,\n",
            "          3.1313e+13,  1.5910e+15,  1.5635e+14,  4.2357e+13,  1.8741e+14,\n",
            "          2.0349e+14,  1.2099e+14, -3.9310e+13, -1.5567e+15,  4.5086e+13,\n",
            "          5.7360e+13,  3.6899e+13,  1.7891e+14,  5.2969e+13,  6.2308e+13,\n",
            "          2.8863e+13,  6.2273e+13,  5.4895e+13,  5.5728e+12,  1.6157e+15,\n",
            "          1.7449e+14, -1.2236e+13,  3.7894e+14,  3.6243e+13, -1.5277e+15],\n",
            "        [ 1.2717e+14,  8.8333e+13,  1.1343e+14, -6.4724e+12,  7.6404e+12,\n",
            "          3.6106e+13,  2.0454e+14,  1.3864e+14, -3.2040e+12,  5.4898e+13,\n",
            "          4.0380e+13, -1.5362e+15, -1.3771e+15,  7.8841e+13,  3.4232e+13,\n",
            "          1.1194e+13, -1.5004e+15,  1.5387e+15,  1.7308e+15,  1.3830e+14,\n",
            "          1.4090e+13, -1.5318e+15,  4.7480e+14,  1.3776e+13,  2.9326e+13,\n",
            "         -1.5342e+15, -9.3743e+13,  4.3789e+13, -1.4849e+15,  1.7515e+15,\n",
            "          1.6443e+14,  1.7099e+15, -1.4493e+14,  1.8706e+13,  1.0785e+13,\n",
            "          7.0801e+13, -1.4574e+15,  1.4384e+14,  1.1211e+14,  4.0203e+13,\n",
            "          3.8619e+13,  1.6143e+15, -1.4307e+15,  1.9968e+13,  9.4047e+13,\n",
            "         -1.0350e+14,  1.5631e+15, -9.3430e+13,  1.6560e+15, -1.3529e+14],\n",
            "        [-1.3601e+13,  1.5137e+15, -1.4447e+13,  1.1483e+13, -1.9202e+12,\n",
            "          1.3945e+14, -1.2323e+13,  1.6508e+14, -7.1959e+11,  1.5580e+15,\n",
            "          1.3325e+13,  7.5729e+12, -1.5826e+15,  3.0213e+13,  5.9990e+13,\n",
            "         -2.8533e+13, -1.4859e+15,  1.2689e+13,  5.6499e+13, -3.9472e+13,\n",
            "          1.4936e+13,  7.9223e+12, -1.4876e+15,  5.0945e+13, -1.5195e+15,\n",
            "         -6.6288e+12,  2.0050e+13,  4.8226e+13,  1.2968e+13, -2.4674e+13,\n",
            "         -1.4624e+15, -1.5388e+15, -1.9709e+14,  6.7899e+13,  6.5365e+13,\n",
            "          2.4088e+13,  1.5789e+15,  2.4861e+14,  1.6289e+15,  1.5540e+15,\n",
            "         -8.1190e+12,  2.7095e+13,  7.7118e+12, -1.5367e+15, -1.1202e+12,\n",
            "         -5.8790e+13,  5.5406e+13, -1.5269e+15,  6.5999e+13,  3.3296e+14],\n",
            "        [ 5.2130e+13,  6.9268e+13,  1.5961e+15,  1.9074e+13,  8.4509e+12,\n",
            "         -2.0399e+13,  6.9764e+13, -3.2029e+13,  5.1459e+13,  8.8111e+12,\n",
            "          1.5864e+13, -1.8890e+12, -1.3867e+15,  9.4229e+12, -1.2154e+13,\n",
            "          2.7267e+13, -1.4165e+15,  1.1719e+13,  8.1132e+12,  1.1393e+14,\n",
            "          2.8538e+13,  2.3182e+12, -1.4578e+15, -1.5769e+15,  7.1549e+12,\n",
            "          3.8323e+13,  1.2902e+13, -1.5756e+15, -1.5412e+15,  5.4028e+13,\n",
            "         -5.7423e+13,  9.8598e+13,  1.2313e+14, -1.6246e+15, -8.0096e+12,\n",
            "          1.5253e+15,  1.5275e+15,  1.3926e+15, -1.1400e+13,  6.4508e+13,\n",
            "         -2.4969e+13, -1.6906e+13,  1.5553e+15, -1.5335e+15,  7.3723e+12,\n",
            "          3.5464e+12, -7.5052e+12, -9.4895e+13,  6.0895e+13, -2.0461e+14],\n",
            "        [ 7.3432e+13,  1.4240e+14,  7.0613e+13, -4.9461e+12, -1.5311e+15,\n",
            "         -9.4032e+12,  1.2897e+14,  8.2136e+13,  6.6097e+13, -1.5435e+15,\n",
            "          3.4823e+13,  1.3089e+12,  1.7194e+15, -5.0622e+12, -1.4598e+15,\n",
            "          1.5797e+15, -1.5315e+15,  1.4144e+13,  1.8107e+14,  1.6299e+15,\n",
            "          3.7266e+12,  1.5375e+15,  2.1775e+14, -1.7914e+13, -1.5097e+15,\n",
            "         -1.5159e+15,  1.5772e+15,  8.6778e+13,  6.7867e+13,  1.9104e+14,\n",
            "          1.2533e+14,  7.7489e+13,  1.7721e+14,  1.5712e+15,  4.4826e+13,\n",
            "         -1.5056e+15, -1.5319e+15, -1.4189e+15,  2.5165e+13,  3.0057e+13,\n",
            "         -1.4941e+15,  3.3960e+13,  8.8720e+13, -6.2513e+12,  7.2338e+13,\n",
            "         -1.2273e+14,  3.7474e+12,  9.8159e+13,  6.9567e+13, -7.7451e+13],\n",
            "        [ 1.5816e+15, -1.4412e+15, -1.4699e+15,  1.5325e+13,  8.9384e+12,\n",
            "          3.7226e+13,  1.6095e+15, -3.3016e+13,  1.1175e+13, -1.5041e+15,\n",
            "          9.8688e+11,  5.3472e+12,  9.0673e+13,  5.3565e+12,  2.7354e+12,\n",
            "          5.4438e+13, -6.0774e+13,  1.5340e+15, -1.4224e+15,  1.5733e+15,\n",
            "         -1.5401e+15,  2.0479e+12,  4.9022e+13,  2.1305e+12,  3.9582e+13,\n",
            "          2.5764e+13,  6.5805e+12, -7.2085e+12, -1.5425e+15, -1.4367e+15,\n",
            "          1.5875e+15, -1.7559e+13, -8.2140e+12, -1.8252e+13,  1.8609e+13,\n",
            "          3.8945e+13,  1.2961e+13, -1.4831e+15,  6.3914e+13,  5.4899e+13,\n",
            "          1.8994e+13, -1.3687e+15, -1.7574e+13,  4.3758e+12,  6.2448e+13,\n",
            "          1.4745e+15,  3.7918e+13,  5.4955e+13,  2.4129e+13, -2.4304e+14],\n",
            "        [-1.5529e+15,  6.6801e+13,  9.8687e+13,  2.3612e+13, -1.5669e+12,\n",
            "         -3.4068e+13,  1.4727e+14,  2.2272e+13,  5.5954e+13, -1.5234e+15,\n",
            "          1.5193e+13,  2.0632e+12,  9.9123e+13,  1.5681e+15,  7.3569e+13,\n",
            "          2.7088e+13,  1.8050e+13,  2.8657e+13, -3.1620e+13, -1.0352e+14,\n",
            "          1.5463e+15,  4.0413e+12,  5.6482e+13,  1.6008e+15,  9.1786e+13,\n",
            "          3.9261e+13,  5.6599e+13,  7.9581e+13,  6.1187e+13, -1.3562e+15,\n",
            "          1.1160e+14,  1.3753e+14, -7.8100e+12,  8.8819e+13, -7.2244e+13,\n",
            "         -1.1935e+12,  5.4837e+13, -4.3823e+13,  1.0815e+14,  3.6640e+13,\n",
            "         -3.1494e+13,  1.0342e+14,  8.5762e+13, -1.5213e+15,  4.2127e+13,\n",
            "          5.5959e+13, -1.5495e+15,  1.6727e+15,  1.5385e+15, -6.7713e+13],\n",
            "        [ 8.2347e+13, -1.3620e+13,  7.1549e+13,  9.3567e+12,  5.2255e+12,\n",
            "          5.5086e+13, -2.7990e+13, -5.7492e+13,  1.5430e+13,  2.4332e+13,\n",
            "          1.5553e+13,  2.3381e+12, -9.5040e+12, -1.5182e+15,  9.4753e+13,\n",
            "         -6.0200e+12,  6.7782e+13,  2.6444e+12, -2.3731e+13, -2.5852e+13,\n",
            "          6.5457e+12,  5.1425e+12,  7.4514e+12,  1.6121e+15,  1.6459e+13,\n",
            "          3.7656e+13,  5.3238e+13,  3.5109e+13,  8.8098e+12, -1.5285e+13,\n",
            "         -1.0471e+13, -3.4637e+13,  6.9689e+13, -2.1310e+13,  5.0419e+13,\n",
            "          2.5822e+13,  2.3088e+13,  1.6523e+15,  5.0276e+13,  4.4166e+13,\n",
            "          9.1106e+13,  2.7053e+14, -4.4513e+13, -1.5218e+15,  1.0555e+13,\n",
            "          4.5043e+13,  1.4110e+14, -1.3896e+15, -1.4876e+15, -1.4028e+15],\n",
            "        [ 1.0612e+14,  1.5890e+14,  1.7317e+15,  1.5441e+15,  1.4592e+13,\n",
            "          2.9014e+13,  2.0163e+14,  9.5347e+13,  1.0366e+14,  8.8095e+12,\n",
            "          1.5570e+15, -1.5333e+15,  2.1837e+14,  1.5423e+15,  1.6427e+15,\n",
            "          5.2115e+13, -1.4236e+15,  1.5347e+13, -1.4385e+15,  6.5111e+13,\n",
            "          7.6555e+12,  2.7092e+12,  7.7194e+13, -2.5160e+13,  4.8430e+13,\n",
            "          6.7366e+13,  8.0684e+13,  1.0870e+14,  6.3607e+13,  1.8204e+14,\n",
            "          7.5058e+13,  1.6518e+15,  2.3051e+14,  1.6076e+15,  1.5224e+15,\n",
            "          1.5792e+15,  3.5388e+13, -9.8189e+12,  5.7322e+13,  1.6155e+15,\n",
            "          1.5214e+15,  1.5504e+14, -1.4326e+15, -1.5258e+15, -1.4727e+15,\n",
            "         -3.3315e+13, -1.2420e+12,  1.5514e+15,  2.5668e+13, -1.8548e+14]])\n",
            "calculating 2.bias\n",
            "tensor([0.0910, 0.1680, 0.1500, 0.1650, 0.0230, 0.0490, 0.1130, 0.2270, 0.1160,\n",
            "        0.1150])\n",
            "Test Accuracy: 0.122400\n",
            "Avg loss (3/0): 7238638732855820915123945472.000000\n",
            "Avg loss (3/1): 3795327868202565721732939776.000000\n",
            "Avg loss (3/2): 2124194378545013845003337728.000000\n",
            "Avg loss (3/3): 2253514908938873182416273408.000000\n",
            "Avg loss (3/4): 2612301277267424757495627776.000000\n",
            "Avg loss (3/5): 2805368147369825897589768192.000000\n",
            "Avg loss (3/6): 2759472648114436533169160192.000000\n",
            "Avg loss (3/7): 2495981126653000374003171328.000000\n",
            "Avg loss (3/8): 2071783636856742856628371456.000000\n",
            "Avg loss (3/9): 1587630834620019733944074240.000000\n",
            "Avg loss (3/10): 1168434404716097465688784896.000000\n",
            "Avg loss (3/11): 886173837668495901620961280.000000\n",
            "Avg loss (3/12): 720660647828065834261020672.000000\n",
            "Avg loss (3/13): 611760441762874085972377600.000000\n",
            "Avg loss (3/14): 536198593540239841290092544.000000\n",
            "Avg loss (3/15): 486870782402031635438501888.000000\n",
            "Avg loss (3/16): 459300020455048307659505664.000000\n",
            "Avg loss (3/17): 442508060005214945080770560.000000\n",
            "Avg loss (3/18): 424112302732125126665437184.000000\n",
            "Avg loss (3/19): 392613380421621644708020224.000000\n",
            "sharing worker_0 model\n",
            "sharing worker_1 model\n",
            "sharing worker_2 model\n",
            "{'worker_0': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:42674549324 -> worker_0:89692980800]\n",
            "\t-> (Wrapper)>[PointerTensor | me:82792395408 -> worker_1:47316484423]\n",
            "\t-> (Wrapper)>[PointerTensor | me:70712662148 -> worker_2:83583054638]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:25317197150 -> worker_0:38023796816]\n",
            "\t-> (Wrapper)>[PointerTensor | me:31787807616 -> worker_1:5750460899]\n",
            "\t-> (Wrapper)>[PointerTensor | me:46894948966 -> worker_2:3210830610]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:25212753662 -> worker_0:19541081333]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87560817733 -> worker_1:41840756511]\n",
            "\t-> (Wrapper)>[PointerTensor | me:93080099275 -> worker_2:6176297540]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:54744157156 -> worker_0:84516263899]\n",
            "\t-> (Wrapper)>[PointerTensor | me:91463861925 -> worker_1:24205649859]\n",
            "\t-> (Wrapper)>[PointerTensor | me:16666175653 -> worker_2:87749356238]\n",
            "\t*crypto provider: me*}, 'worker_1': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:1333269974 -> worker_0:39383291218]\n",
            "\t-> (Wrapper)>[PointerTensor | me:76168028302 -> worker_1:54872837412]\n",
            "\t-> (Wrapper)>[PointerTensor | me:67560342827 -> worker_2:47545921477]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:78736693867 -> worker_0:42552721371]\n",
            "\t-> (Wrapper)>[PointerTensor | me:32925520035 -> worker_1:9922714548]\n",
            "\t-> (Wrapper)>[PointerTensor | me:32916009899 -> worker_2:70487305885]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:97141660293 -> worker_0:7337219551]\n",
            "\t-> (Wrapper)>[PointerTensor | me:24702762408 -> worker_1:82054042579]\n",
            "\t-> (Wrapper)>[PointerTensor | me:49439946442 -> worker_2:49376538418]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:68190581191 -> worker_0:8343888374]\n",
            "\t-> (Wrapper)>[PointerTensor | me:41333002148 -> worker_1:8783467098]\n",
            "\t-> (Wrapper)>[PointerTensor | me:50020373253 -> worker_2:22668309785]\n",
            "\t*crypto provider: me*}, 'worker_2': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:23672723848 -> worker_0:18475001920]\n",
            "\t-> (Wrapper)>[PointerTensor | me:4498490505 -> worker_1:59153362479]\n",
            "\t-> (Wrapper)>[PointerTensor | me:7916762519 -> worker_2:96260043226]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:89813382149 -> worker_0:83387599582]\n",
            "\t-> (Wrapper)>[PointerTensor | me:52258308977 -> worker_1:79862331293]\n",
            "\t-> (Wrapper)>[PointerTensor | me:30904379349 -> worker_2:91237402111]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:86352941955 -> worker_0:2599135120]\n",
            "\t-> (Wrapper)>[PointerTensor | me:54728859839 -> worker_1:14060567079]\n",
            "\t-> (Wrapper)>[PointerTensor | me:637626757 -> worker_2:41148301643]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:78841466679 -> worker_0:62919032038]\n",
            "\t-> (Wrapper)>[PointerTensor | me:6263366743 -> worker_1:83865012108]\n",
            "\t-> (Wrapper)>[PointerTensor | me:39450507083 -> worker_2:11918849390]\n",
            "\t*crypto provider: me*}}\n",
            "grad sum =  {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:42674549324 -> worker_0:89692980800]\n",
            "\t-> (Wrapper)>[PointerTensor | me:82792395408 -> worker_1:47316484423]\n",
            "\t-> (Wrapper)>[PointerTensor | me:70712662148 -> worker_2:83583054638]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:25317197150 -> worker_0:38023796816]\n",
            "\t-> (Wrapper)>[PointerTensor | me:31787807616 -> worker_1:5750460899]\n",
            "\t-> (Wrapper)>[PointerTensor | me:46894948966 -> worker_2:3210830610]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:25212753662 -> worker_0:19541081333]\n",
            "\t-> (Wrapper)>[PointerTensor | me:87560817733 -> worker_1:41840756511]\n",
            "\t-> (Wrapper)>[PointerTensor | me:93080099275 -> worker_2:6176297540]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:54744157156 -> worker_0:84516263899]\n",
            "\t-> (Wrapper)>[PointerTensor | me:91463861925 -> worker_1:24205649859]\n",
            "\t-> (Wrapper)>[PointerTensor | me:16666175653 -> worker_2:87749356238]\n",
            "\t*crypto provider: me*}\n",
            "calculating 0.weight\n",
            "tensor([[ 1.5372e+15, -1.5372e+15, -1.5372e+15,  ...,  1.5372e+15,\n",
            "          0.0000e+00,  1.0000e-03],\n",
            "        [ 0.0000e+00,  1.5372e+15,  0.0000e+00,  ..., -1.5372e+15,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.0000e-03,  0.0000e+00,  0.0000e+00,  ..., -1.5372e+15,\n",
            "          1.5372e+15,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 1.0000e-03,  0.0000e+00,  1.5372e+15,  ...,  0.0000e+00,\n",
            "          0.0000e+00, -1.0000e-03],\n",
            "        [-1.5372e+15,  0.0000e+00,  1.0000e-03,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.0000e-03,  0.0000e+00,  1.0000e-03,  ...,  0.0000e+00,\n",
            "          0.0000e+00, -1.0000e-03]])\n",
            "calculating 0.bias\n",
            "tensor([ 8.3824e+10,  2.2223e+11, -1.5370e+15,  1.4216e+11,  5.5703e+10,\n",
            "        -1.5368e+15,  5.1060e+11,  2.4567e+11,  1.0011e+11,  3.8335e+11,\n",
            "         9.8478e+09,  3.2629e+10,  3.8129e+11,  3.1595e+10,  7.0147e+10,\n",
            "         1.1391e+11,  6.7327e+11, -3.0554e+09,  3.5740e+11,  3.9958e+11,\n",
            "         7.6511e+10,  2.2423e+10,  4.5433e+11,  1.5373e+15,  1.7246e+11,\n",
            "         3.6727e+11,  1.5373e+15,  3.5405e+10,  2.4108e+10,  3.6922e+11,\n",
            "         1.9355e+11,  3.8056e+11,  5.5280e+11,  1.5373e+15, -2.5890e+09,\n",
            "        -1.5372e+15,  1.2853e+11, -1.5371e+15,  4.4393e+11,  1.6039e+11,\n",
            "         1.5373e+15,  2.5028e+11, -1.5372e+15,  8.5598e+10, -1.5370e+15,\n",
            "         1.5374e+15,  1.4988e+11,  7.1939e+11,  1.7285e+11,  1.5374e+15])\n",
            "calculating 2.weight\n",
            "tensor([[ 2.0772e+13,  4.6422e+13, -2.2937e+13,  2.0641e+13,  1.9553e+12,\n",
            "         -1.3973e+14,  1.6204e+14,  1.6050e+14, -1.3793e+15,  9.0366e+13,\n",
            "          1.8744e+13, -3.8283e+11,  7.1802e+13,  9.0284e+12,  2.8497e+13,\n",
            "          1.4647e+15, -5.8387e+13, -1.8261e+12,  2.6835e+13, -6.5871e+11,\n",
            "         -6.2730e+11,  5.1407e+12,  2.4461e+14, -1.5171e+15, -1.5501e+15,\n",
            "         -1.5021e+15,  4.2415e+13,  8.9944e+13,  5.0714e+12, -2.7615e+13,\n",
            "          1.4000e+14,  1.5670e+15,  7.4270e+13, -1.5106e+15,  1.0108e+13,\n",
            "         -1.4989e+15,  1.5731e+15,  7.5019e+13, -1.6420e+15,  2.2361e+13,\n",
            "          1.5480e+15,  6.2090e+13,  7.3863e+13, -3.3837e+12,  1.5894e+15,\n",
            "          6.4424e+13, -9.7436e+12, -1.6198e+15,  2.1155e+13,  3.8282e+13],\n",
            "        [ 1.0965e+14,  8.2357e+13,  3.0409e+13, -1.4968e+15,  9.0181e+12,\n",
            "          1.2319e+14,  2.0428e+14,  1.4703e+15,  5.3840e+13,  1.7460e+15,\n",
            "         -1.5702e+15,  2.2305e+12,  1.7535e+14,  5.7006e+12,  1.1816e+13,\n",
            "          1.2851e+14,  3.4972e+14,  1.1553e+13,  3.3519e+14,  1.3226e+13,\n",
            "          2.5343e+13,  1.5325e+15,  1.8123e+15,  4.9216e+13, -1.5216e+15,\n",
            "         -1.3842e+15, -1.5259e+15, -1.4851e+15,  3.2868e+12,  3.1391e+14,\n",
            "          2.0173e+14,  1.3126e+14, -6.6518e+12, -1.4934e+15,  4.7745e+12,\n",
            "          3.2562e+11,  7.1849e+13, -1.4173e+15,  2.1109e+14,  4.7210e+13,\n",
            "          1.5719e+15,  2.8757e+13,  1.2136e+13,  6.4606e+12, -1.4675e+15,\n",
            "         -1.0300e+14,  2.0658e+13,  3.9546e+14,  8.5156e+12, -1.4768e+15],\n",
            "        [ 1.5845e+14,  7.0510e+13,  1.3781e+12,  1.6008e+13, -1.5216e+15,\n",
            "         -1.5137e+15,  1.1875e+14,  3.9564e+13,  1.7106e+13, -1.4098e+15,\n",
            "          1.5630e+15, -8.1701e+12,  1.2260e+14,  3.9509e+13,  1.0476e+13,\n",
            "          5.8785e+13,  1.8029e+15, -1.0560e+13,  1.7772e+14,  9.8890e+13,\n",
            "          4.0647e+13, -1.5287e+15,  3.1103e+14,  3.8025e+13, -1.4667e+15,\n",
            "         -1.4451e+15, -7.5232e+13,  3.9548e+13,  1.4031e+13,  1.6099e+14,\n",
            "          1.6760e+15,  1.7690e+14, -1.7976e+14, -2.2247e+13, -1.5322e+15,\n",
            "          1.5519e+15,  1.3830e+14,  4.9692e+13, -1.3895e+15,  7.6514e+13,\n",
            "          3.4169e+13,  2.4264e+13,  1.4956e+14, -1.5283e+15,  7.1126e+13,\n",
            "          1.5089e+15,  1.6541e+13, -1.7511e+15,  1.0910e+14,  7.8125e+12],\n",
            "        [-2.6310e+13, -1.6718e+13,  2.7616e+13, -8.8406e+12,  7.0674e+11,\n",
            "          1.8247e+14,  4.2517e+13,  2.0397e+14,  1.2270e+14,  4.1770e+12,\n",
            "          4.5153e+13,  8.4490e+12,  1.0097e+13, -2.6450e+12,  1.5559e+15,\n",
            "          1.4413e+15,  1.2807e+12,  1.5284e+15, -1.4848e+15,  1.4646e+15,\n",
            "          3.9722e+13,  9.4246e+12,  1.5511e+14,  5.9655e+13,  1.3106e+14,\n",
            "         -4.4151e+13,  7.0644e+13,  1.5903e+15,  1.0799e+13,  4.5793e+13,\n",
            "         -1.4151e+15, -1.4347e+15,  4.6416e+13,  7.8108e+13,  3.2440e+13,\n",
            "          1.1104e+13, -1.4966e+15,  1.2495e+14, -7.9992e+12, -1.5393e+15,\n",
            "         -5.4686e+12,  1.5355e+15,  9.4012e+12,  6.2207e+12,  1.5416e+15,\n",
            "         -1.5838e+15,  8.3479e+13, -1.3749e+15, -1.3847e+15,  7.4992e+13],\n",
            "        [ 1.0514e+14,  1.1323e+14,  4.1992e+13,  7.6051e+13, -1.5310e+15,\n",
            "          2.0606e+14, -1.4323e+15, -1.4685e+14,  6.0628e+13,  1.2177e+14,\n",
            "          2.4608e+13,  2.8395e+12,  1.9107e+14, -1.5344e+15,  1.2134e+13,\n",
            "          8.1932e+13,  4.3523e+14,  3.1557e+13,  2.0942e+14,  2.2445e+14,\n",
            "          4.9459e+13,  8.7607e+11,  2.4137e+14,  3.5979e+13,  3.3784e+13,\n",
            "          1.6094e+14,  5.3592e+13, -1.4956e+15,  1.5478e+15,  1.9773e+14,\n",
            "         -5.4318e+13, -1.2482e+15,  5.2555e+13,  2.7384e+13, -1.5365e+15,\n",
            "          9.6121e+11,  1.7921e+13, -5.7411e+13,  2.6758e+14,  1.7474e+14,\n",
            "         -4.8574e+12, -1.2107e+13, -1.4242e+15,  1.6381e+13,  2.8379e+13,\n",
            "          1.6441e+14, -2.4794e+13, -1.6279e+15,  1.0774e+14, -2.8352e+13],\n",
            "        [ 9.6456e+13,  1.2308e+14, -5.4726e+13,  1.0717e+13,  7.8253e+12,\n",
            "          1.4261e+15,  6.6187e+13,  6.6880e+12, -1.4633e+15,  1.7158e+14,\n",
            "          1.6110e+15,  1.0638e+12,  1.3660e+14, -1.1808e+13,  4.4444e+13,\n",
            "          2.7171e+12,  3.2464e+14, -1.5105e+15,  1.8299e+15,  8.4640e+13,\n",
            "         -1.4092e+13,  1.5387e+15,  2.0575e+14, -3.4977e+13,  7.7892e+13,\n",
            "         -1.4462e+15, -9.2731e+13,  3.1257e+13, -4.3794e+11,  2.7622e+14,\n",
            "          1.3510e+14,  9.9240e+13,  6.3438e+12, -2.7880e+13, -1.5040e+15,\n",
            "         -1.5133e+15,  5.4456e+13,  1.1508e+14,  1.7441e+14,  1.6114e+15,\n",
            "          6.0508e+13, -2.2286e+12,  9.8899e+13, -1.4852e+13,  1.6125e+15,\n",
            "         -1.8594e+13,  1.5408e+15,  1.2748e+14,  1.6137e+15, -1.5039e+15],\n",
            "        [ 1.7868e+13,  1.0258e+14,  4.1991e+13,  5.1477e+13,  1.4346e+13,\n",
            "          3.9171e+13,  5.9007e+13, -6.3004e+13,  2.6625e+13, -1.3248e+15,\n",
            "          1.6419e+13,  9.3836e+12, -1.4405e+15,  3.8681e+12,  7.9636e+12,\n",
            "          1.7131e+15, -1.3003e+15, -1.5175e+15,  3.1438e+14, -1.5405e+15,\n",
            "          5.7122e+13,  9.3635e+11,  7.2703e+13,  7.6173e+13,  7.6335e+13,\n",
            "          7.4846e+13,  5.9099e+13,  1.5055e+15,  5.2001e+12,  1.7864e+15,\n",
            "          1.4221e+14, -1.4998e+15, -1.4555e+14,  5.2620e+13,  1.5521e+15,\n",
            "          1.0546e+13, -1.5003e+15,  8.7733e+13,  1.7197e+15,  1.5979e+15,\n",
            "          2.7392e+13,  1.0721e+14,  4.2524e+13,  1.5542e+13,  9.0801e+13,\n",
            "         -2.2300e+13,  1.5906e+15, -1.4324e+15,  1.5368e+15, -2.8072e+13],\n",
            "        [ 1.5418e+14,  3.5790e+13,  5.2983e+12,  7.1247e+13, -1.3202e+13,\n",
            "          1.5376e+15,  1.9368e+14,  3.8223e+13,  2.6782e+13, -1.4659e+15,\n",
            "          2.2246e+13, -1.5349e+15,  7.4701e+13,  1.5427e+15, -1.5177e+15,\n",
            "          8.6823e+13,  1.7824e+14,  3.2710e+13, -6.2691e+13, -3.9078e+14,\n",
            "         -1.5431e+15, -1.5353e+15, -1.0657e+14,  3.2574e+13,  1.6185e+15,\n",
            "          1.1343e+14,  1.2575e+14, -1.4955e+15,  1.4052e+13,  2.8189e+14,\n",
            "          2.9056e+13,  2.4325e+14, -8.0375e+13,  7.9744e+13, -3.1678e+13,\n",
            "          1.4342e+12,  7.0091e+13, -1.5690e+15,  1.6947e+15, -1.4987e+15,\n",
            "         -6.8420e+13, -1.5489e+13,  1.6513e+15,  1.6368e+13,  6.4943e+12,\n",
            "          5.4803e+13, -5.0182e+13, -3.8094e+13,  2.4843e+13, -2.5198e+13],\n",
            "        [ 1.1790e+14, -7.1116e+12,  1.8002e+14, -1.5037e+15,  6.0548e+12,\n",
            "          1.0460e+14, -8.1045e+13, -1.5370e+15,  2.9343e+13,  1.1124e+13,\n",
            "          3.1477e+13,  3.8752e+12,  1.2750e+12,  3.8000e+13,  4.8100e+13,\n",
            "          2.8621e+13, -1.3398e+13,  4.5970e+12, -2.4473e+13, -1.5220e+15,\n",
            "          2.6461e+13,  8.5138e+12,  3.5496e+13,  3.5494e+13,  1.5831e+15,\n",
            "          2.3708e+13, -1.4882e+15, -1.5075e+15,  3.3813e+12, -1.5615e+15,\n",
            "          1.5197e+15,  1.5267e+15,  1.0323e+13, -5.1168e+13,  1.5602e+15,\n",
            "          1.2711e+13, -1.5122e+15,  3.8048e+13, -1.5046e+15, -1.4885e+15,\n",
            "          7.6103e+13,  8.3588e+13, -4.0364e+13, -1.5047e+15, -1.5220e+15,\n",
            "         -5.0361e+12,  1.4744e+14,  3.5071e+14,  1.2812e+14,  8.8763e+13],\n",
            "        [ 1.5331e+14,  9.5047e+13,  9.3366e+13,  1.7871e+13, -1.5117e+15,\n",
            "          6.8884e+13,  9.8327e+13,  1.5647e+15, -1.3834e+15,  1.5401e+15,\n",
            "         -3.6787e+12,  1.3600e+13, -1.4329e+15,  2.8149e+12,  1.5495e+15,\n",
            "          1.5897e+15,  1.5216e+14,  1.8014e+13, -3.0758e+13, -1.6293e+15,\n",
            "          1.5441e+15,  2.2136e+12, -1.6958e+15, -2.7094e+13,  1.5960e+15,\n",
            "          1.6297e+15,  1.6388e+15,  5.4656e+13,  1.1047e+13,  9.9087e+13,\n",
            "          1.4440e+15, -9.7509e+11,  1.1434e+14,  1.6224e+12, -1.8626e+12,\n",
            "          5.9180e+12,  3.4661e+13, -1.5380e+15, -1.5828e+15,  2.8209e+13,\n",
            "         -4.2611e+13,  1.3287e+14,  5.5964e+13,  1.7298e+13,  5.2333e+13,\n",
            "          1.4846e+15,  7.7575e+12, -2.4335e+14,  1.5110e+15, -2.9360e+13]])\n",
            "calculating 2.bias\n",
            "tensor([-1.5372e+15,  6.6000e-02,  1.5372e+15,  1.6200e-01,  1.5372e+15,\n",
            "         6.0000e-02,  1.6100e-01,  1.3400e-01,  8.4000e-02,  1.0900e-01])\n",
            "Test Accuracy: 0.106600\n",
            "Avg loss (4/0): 10302141172060172538737590272.000000\n",
            "Avg loss (4/1): 5975499555722037854658887680.000000\n",
            "Avg loss (4/2): 2748389844274951834558267392.000000\n",
            "Avg loss (4/3): 2350595253058371093249654784.000000\n",
            "Avg loss (4/4): 2657248466303472861286760448.000000\n",
            "Avg loss (4/5): 2983639843281396439229399040.000000\n",
            "Avg loss (4/6): 3150429990533584976590405632.000000\n",
            "Avg loss (4/7): 3073448038165180275786514432.000000\n",
            "Avg loss (4/8): 2750840162183750821718523904.000000\n",
            "Avg loss (4/9): 2252881669108310880928399360.000000\n",
            "Avg loss (4/10): 1706355555217940304908779520.000000\n",
            "Avg loss (4/11): 1244726522643121724100444160.000000\n",
            "Avg loss (4/12): 936405502372827728082632704.000000\n",
            "Avg loss (4/13): 764077347401806403973677056.000000\n",
            "Avg loss (4/14): 671948256965644726470443008.000000\n",
            "Avg loss (4/15): 608212800836106412424691712.000000\n",
            "Avg loss (4/16): 542623631394601025537048576.000000\n",
            "Avg loss (4/17): 471116598880297024814579712.000000\n",
            "Avg loss (4/18): 398794072703982472234270720.000000\n",
            "Avg loss (4/19): 330283012788177787208859648.000000\n",
            "sharing worker_0 model\n",
            "sharing worker_1 model\n",
            "sharing worker_2 model\n",
            "{'worker_0': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:71258487376 -> worker_0:40821242959]\n",
            "\t-> (Wrapper)>[PointerTensor | me:30162007847 -> worker_1:40780218371]\n",
            "\t-> (Wrapper)>[PointerTensor | me:29014569547 -> worker_2:97872185037]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:93439596490 -> worker_0:612792652]\n",
            "\t-> (Wrapper)>[PointerTensor | me:91138665780 -> worker_1:15095937218]\n",
            "\t-> (Wrapper)>[PointerTensor | me:62074790364 -> worker_2:62223259794]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:80690492485 -> worker_0:98326723866]\n",
            "\t-> (Wrapper)>[PointerTensor | me:39228810775 -> worker_1:52192231408]\n",
            "\t-> (Wrapper)>[PointerTensor | me:24072156988 -> worker_2:72208979214]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:71348942897 -> worker_0:30673799264]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79659141205 -> worker_1:37500348234]\n",
            "\t-> (Wrapper)>[PointerTensor | me:44192772723 -> worker_2:33917077261]\n",
            "\t*crypto provider: me*}, 'worker_1': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:8436573081 -> worker_0:93951225282]\n",
            "\t-> (Wrapper)>[PointerTensor | me:90612000288 -> worker_1:59463486956]\n",
            "\t-> (Wrapper)>[PointerTensor | me:50555705455 -> worker_2:63900648580]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:30679573002 -> worker_0:87489535323]\n",
            "\t-> (Wrapper)>[PointerTensor | me:34249543162 -> worker_1:51078128257]\n",
            "\t-> (Wrapper)>[PointerTensor | me:99687536191 -> worker_2:41097071967]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:17623641247 -> worker_0:86288078467]\n",
            "\t-> (Wrapper)>[PointerTensor | me:7769179844 -> worker_1:70474516856]\n",
            "\t-> (Wrapper)>[PointerTensor | me:84031949401 -> worker_2:27847602994]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:43046383628 -> worker_0:67251289168]\n",
            "\t-> (Wrapper)>[PointerTensor | me:98584302397 -> worker_1:23271571127]\n",
            "\t-> (Wrapper)>[PointerTensor | me:68889290389 -> worker_2:4636104199]\n",
            "\t*crypto provider: me*}, 'worker_2': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:39240073300 -> worker_0:22711210061]\n",
            "\t-> (Wrapper)>[PointerTensor | me:82209739958 -> worker_1:46000382806]\n",
            "\t-> (Wrapper)>[PointerTensor | me:9674805931 -> worker_2:17273240775]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:88916476746 -> worker_0:35583901422]\n",
            "\t-> (Wrapper)>[PointerTensor | me:21547251173 -> worker_1:69289243393]\n",
            "\t-> (Wrapper)>[PointerTensor | me:55746608692 -> worker_2:29679902521]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:9946198417 -> worker_0:11810881239]\n",
            "\t-> (Wrapper)>[PointerTensor | me:82385329599 -> worker_1:13476856195]\n",
            "\t-> (Wrapper)>[PointerTensor | me:28935571996 -> worker_2:34851989594]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:78060556911 -> worker_0:26695669036]\n",
            "\t-> (Wrapper)>[PointerTensor | me:30438354186 -> worker_1:97538380682]\n",
            "\t-> (Wrapper)>[PointerTensor | me:71279537093 -> worker_2:97857896193]\n",
            "\t*crypto provider: me*}}\n",
            "grad sum =  {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:71258487376 -> worker_0:40821242959]\n",
            "\t-> (Wrapper)>[PointerTensor | me:30162007847 -> worker_1:40780218371]\n",
            "\t-> (Wrapper)>[PointerTensor | me:29014569547 -> worker_2:97872185037]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:93439596490 -> worker_0:612792652]\n",
            "\t-> (Wrapper)>[PointerTensor | me:91138665780 -> worker_1:15095937218]\n",
            "\t-> (Wrapper)>[PointerTensor | me:62074790364 -> worker_2:62223259794]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:80690492485 -> worker_0:98326723866]\n",
            "\t-> (Wrapper)>[PointerTensor | me:39228810775 -> worker_1:52192231408]\n",
            "\t-> (Wrapper)>[PointerTensor | me:24072156988 -> worker_2:72208979214]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:71348942897 -> worker_0:30673799264]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79659141205 -> worker_1:37500348234]\n",
            "\t-> (Wrapper)>[PointerTensor | me:44192772723 -> worker_2:33917077261]\n",
            "\t*crypto provider: me*}\n",
            "calculating 0.weight\n",
            "tensor([[-1.0000e-03,  0.0000e+00,  1.5372e+15,  ...,  1.5372e+15,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  1.5372e+15, -1.5372e+15,  ...,  1.5372e+15,\n",
            "          1.5372e+15,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00, -1.5372e+15,  ..., -1.0000e-03,\n",
            "         -1.0000e-03,  1.5372e+15],\n",
            "        ...,\n",
            "        [ 0.0000e+00, -1.0000e-03, -1.0000e-03,  ...,  0.0000e+00,\n",
            "          1.0000e-03, -1.0000e-03],\n",
            "        [ 0.0000e+00,  1.5372e+15,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.5372e+15, -1.5372e+15,  1.0000e-03,  ...,  0.0000e+00,\n",
            "          1.5372e+15, -1.0000e-03]])\n",
            "calculating 0.bias\n",
            "tensor([ 2.0813e+11,  3.4625e+11,  1.5017e+11, -1.5369e+15, -1.5372e+15,\n",
            "         3.7758e+11,  1.5374e+15,  2.9050e+11,  2.6564e+11, -1.5366e+15,\n",
            "         1.2866e+11,  3.9208e+10,  4.7474e+11,  5.7783e+09,  1.1650e+11,\n",
            "         4.1914e+11,  4.5266e+11,  8.6646e+10,  5.1371e+11,  1.4171e+11,\n",
            "         1.5374e+15,  1.0134e+11,  6.0590e+11, -1.5370e+15,  2.0340e+11,\n",
            "         6.8895e+11,  1.5374e+15,  3.1933e+11,  7.5565e+10,  3.5562e+11,\n",
            "         2.3172e+11,  5.3139e+11,  5.0427e+11,  6.2229e+10,  1.3613e+11,\n",
            "         1.1289e+11,  1.6080e+11,  1.4639e+11, -1.5368e+15,  1.8848e+11,\n",
            "         5.7520e+10,  1.7134e+11, -1.5371e+15,  9.5359e+10,  3.1129e+11,\n",
            "         3.5274e+11,  6.7468e+10,  5.9396e+11,  3.9043e+11,  2.7354e+11])\n",
            "calculating 2.weight\n",
            "tensor([[-6.4790e+12,  1.7758e+14,  2.1128e+13,  5.9669e+13, -4.1792e+12,\n",
            "         -2.4117e+14,  3.8137e+13,  1.7641e+14,  1.6613e+15,  2.4186e+14,\n",
            "          3.7335e+13, -1.6736e+12,  1.6908e+15,  4.9106e+12, -1.4752e+15,\n",
            "          1.2197e+15,  1.6160e+15,  1.8556e+13, -1.7083e+15,  7.8009e+13,\n",
            "          1.5731e+15, -1.5248e+15, -1.2105e+15,  1.2339e+14, -8.7936e+12,\n",
            "         -1.3407e+15, -5.7850e+13,  6.8606e+13,  3.9767e+12, -2.4871e+14,\n",
            "          1.4701e+14,  1.7890e+14,  2.4750e+13, -1.5339e+15, -1.5265e+15,\n",
            "          2.7670e+13, -2.2194e+12,  8.7501e+13, -2.7533e+12,  8.0023e+13,\n",
            "          1.5363e+15,  1.6630e+14,  1.5999e+15,  7.5020e+12,  7.2882e+13,\n",
            "          3.0069e+13, -2.5405e+13,  1.7957e+14, -3.2678e+12,  1.5846e+15],\n",
            "        [ 4.5278e+13, -1.4482e+15, -1.4250e+15, -1.4414e+15,  7.7464e+12,\n",
            "          1.6997e+14,  9.0799e+13, -7.3435e+13,  9.2638e+13, -1.4151e+15,\n",
            "          8.3952e+13,  3.2118e+12, -1.4266e+15,  1.4271e+13,  1.5930e+15,\n",
            "          2.3367e+14,  1.9329e+14,  2.3436e+12,  1.1904e+14, -1.6985e+13,\n",
            "         -3.2798e+13, -1.5320e+15, -1.4635e+15,  1.5907e+15,  3.0601e+13,\n",
            "          1.5825e+14,  1.4304e+14, -1.3827e+15,  4.5281e+13,  1.3329e+14,\n",
            "          9.8466e+13,  1.3266e+14,  7.1969e+13,  2.1749e+14,  4.0435e+13,\n",
            "         -1.5155e+15, -1.4319e+15,  1.6113e+15, -1.3956e+15,  8.0848e+13,\n",
            "          3.2225e+13,  6.4555e+13,  5.9085e+13, -1.5248e+15,  8.3620e+13,\n",
            "          3.9253e+13,  4.6630e+13,  2.9321e+14,  6.0677e+13,  1.7441e+15],\n",
            "        [ 6.8906e+13,  1.6679e+15, -3.2539e+13,  7.2539e+13,  1.2466e+13,\n",
            "          1.3368e+14,  4.7997e+13,  1.5960e+15,  1.6026e+15, -1.2485e+15,\n",
            "         -3.0918e+13,  8.3084e+12,  1.3754e+14,  1.9922e+13,  1.5807e+15,\n",
            "          6.4853e+13,  1.8935e+14, -1.5360e+15, -7.7063e+13,  8.0898e+13,\n",
            "          6.4752e+13,  2.3606e+13,  3.3147e+14,  1.6483e+15,  7.2091e+13,\n",
            "         -1.3736e+15,  1.4332e+15,  1.4121e+14,  1.6097e+15,  6.2987e+13,\n",
            "          5.7681e+13,  2.2009e+14, -2.5570e+14, -9.4367e+11,  1.9472e+13,\n",
            "          4.1719e+12,  1.7071e+15,  6.3511e+13,  9.9469e+13,  1.6156e+15,\n",
            "          1.5501e+15,  1.2546e+14,  1.5854e+15,  1.8776e+13,  1.0888e+14,\n",
            "          1.4761e+15,  1.5535e+15,  4.1656e+14, -2.1184e+13,  7.0212e+13],\n",
            "        [-1.5479e+15,  3.8723e+13,  4.2202e+13, -5.3640e+12,  1.5494e+12,\n",
            "          1.4216e+14,  3.4945e+13,  1.7169e+15,  9.2348e+13,  7.9110e+13,\n",
            "          6.9614e+13,  5.8487e+12,  7.3049e+13, -5.9866e+12,  1.5369e+15,\n",
            "         -1.8048e+15, -1.3710e+15, -4.7680e+13, -2.0352e+13,  1.5031e+15,\n",
            "          1.0079e+14, -1.5282e+15,  1.7946e+14,  8.2175e+13, -1.4389e+15,\n",
            "          2.8587e+13,  1.5593e+15, -2.1281e+13,  7.0999e+12,  8.6950e+13,\n",
            "          1.7931e+14,  2.6522e+14, -9.1484e+13,  1.5732e+15,  3.4784e+13,\n",
            "          7.6559e+12,  9.3030e+13,  1.0421e+14,  7.2298e+13,  1.5736e+15,\n",
            "         -9.7139e+11,  1.5780e+15,  6.2432e+12, -8.6145e+12, -1.5282e+15,\n",
            "          1.5661e+15,  1.5557e+15,  2.1781e+14,  1.2080e+14,  1.0526e+14],\n",
            "        [ 1.2763e+13,  3.4126e+14, -1.4442e+15,  1.7592e+14,  5.9496e+12,\n",
            "          1.7072e+15,  8.3415e+13,  3.2913e+13, -1.2976e+15,  3.0879e+14,\n",
            "          6.5045e+13,  4.6525e+12,  1.7529e+15,  8.4083e+12,  3.2780e+13,\n",
            "          1.5205e+14,  4.1677e+14,  3.8617e+13,  4.2041e+13,  2.2093e+14,\n",
            "         -1.3468e+15, -6.6813e+10, -1.0612e+15,  1.2205e+14,  1.0646e+13,\n",
            "          2.4387e+14,  9.5680e+13,  2.4271e+14,  5.9755e+12,  1.6555e+14,\n",
            "         -5.5623e+13,  5.0635e+14,  1.5760e+15, -2.6830e+12, -1.5139e+15,\n",
            "         -1.5399e+15,  1.2100e+14,  5.2436e+13,  1.1830e+14,  2.1366e+14,\n",
            "          1.5352e+15,  1.8895e+14, -2.3399e+13,  2.1553e+13,  8.0767e+13,\n",
            "          7.7392e+13,  1.3391e+13, -9.6113e+14,  1.5386e+14,  1.0863e+14],\n",
            "        [ 1.6563e+15,  1.9738e+14,  1.5156e+15, -1.4877e+15,  2.9505e+12,\n",
            "          1.4106e+15, -4.4274e+13,  2.8369e+13,  2.2188e+14,  1.5828e+14,\n",
            "          5.5102e+13,  1.5401e+15,  9.1365e+13, -3.2729e+12,  6.8033e+13,\n",
            "         -1.2833e+14, -1.3045e+15,  4.0582e+13, -1.2350e+14,  4.9122e+13,\n",
            "         -1.5739e+15,  1.7921e+12,  9.5784e+13,  9.7628e+12,  8.5792e+13,\n",
            "          1.2849e+14, -8.7763e+13, -1.4079e+15,  1.8587e+13, -8.3005e+13,\n",
            "          6.0492e+13,  1.3119e+14,  1.5912e+15, -3.7450e+12,  6.6760e+13,\n",
            "          2.1978e+13,  5.9258e+13,  7.6368e+13, -4.6477e+13,  2.7257e+13,\n",
            "          8.0688e+13,  6.9390e+12,  5.2073e+13, -1.2534e+13,  6.4903e+13,\n",
            "         -5.7616e+13,  1.5202e+15,  3.5987e+14,  1.4875e+15,  1.0649e+14],\n",
            "        [-3.6662e+13,  3.0895e+14,  9.4483e+13,  1.3805e+14,  1.2926e+13,\n",
            "          8.3103e+13,  6.6941e+13, -9.4460e+12,  1.6756e+14, -1.1771e+15,\n",
            "          3.2107e+13,  1.5408e+15,  1.5919e+14,  2.4535e+12,  4.6880e+13,\n",
            "          1.7783e+14,  1.8405e+15,  2.6327e+13,  2.1608e+14,  1.6348e+14,\n",
            "          1.6146e+15,  9.2974e+12, -1.3256e+15,  1.3543e+14, -1.4739e+15,\n",
            "          1.4714e+14,  1.5126e+15,  1.6385e+14,  8.6304e+12, -1.5879e+15,\n",
            "          1.3267e+14, -1.2606e+15, -1.4798e+14, -2.0359e+14,  1.5466e+15,\n",
            "         -1.5264e+15,  5.0690e+13,  1.0096e+14,  7.0542e+13,  1.7031e+15,\n",
            "          1.0354e+13,  1.7957e+15, -2.5416e+12,  5.1017e+12,  1.4937e+14,\n",
            "         -3.0603e+13,  2.0025e+13,  2.1382e+15, -1.5081e+15,  1.2393e+14],\n",
            "        [-1.3959e+15, -1.5526e+15, -1.4627e+15,  1.2389e+14, -5.3458e+12,\n",
            "          2.1410e+13,  1.6789e+15,  7.1338e+13,  7.6586e+13,  1.9888e+14,\n",
            "         -2.2996e+13, -1.5364e+15,  7.3987e+13, -1.5322e+15,  5.3326e+13,\n",
            "          1.9551e+14,  1.6240e+15,  1.5582e+15, -2.4549e+14, -2.6393e+14,\n",
            "          5.6909e+13,  1.5518e+15,  1.4861e+15,  5.1166e+13,  1.5237e+15,\n",
            "         -1.4233e+15,  1.3006e+14, -1.3813e+15,  3.9319e+13,  2.2542e+14,\n",
            "          1.5473e+15,  2.7256e+14, -1.7071e+15,  1.5937e+15, -7.2635e+12,\n",
            "         -1.5349e+15,  9.6569e+13, -7.0470e+12,  1.4485e+13,  3.5245e+13,\n",
            "         -3.3542e+13, -3.9383e+13,  4.2021e+13,  1.5600e+15,  5.9581e+13,\n",
            "         -1.5235e+15, -1.2193e+13,  2.2146e+14,  6.4607e+12,  1.5313e+15],\n",
            "        [ 4.9223e+13,  1.0132e+14,  1.1786e+14, -1.4526e+15,  5.7823e+12,\n",
            "          1.6297e+15,  1.6574e+13,  1.5651e+15,  1.4354e+14,  1.3169e+14,\n",
            "          6.4354e+12,  4.6353e+12, -1.4467e+15,  1.5959e+13,  5.0679e+13,\n",
            "         -1.0798e+14,  2.3696e+14,  2.0232e+12, -1.6619e+15,  5.3566e+13,\n",
            "         -1.4668e+15,  1.0833e+13,  9.5382e+13,  5.1292e+13,  2.8271e+12,\n",
            "          1.4908e+14,  6.0128e+13,  1.3679e+14,  8.2865e+10, -3.4184e+13,\n",
            "         -6.7143e+12,  8.7120e+13, -1.4353e+14, -8.6022e+13,  9.3592e+12,\n",
            "         -1.5356e+15,  5.8772e+13,  5.1855e+13,  1.4818e+15,  1.2412e+14,\n",
            "          2.5711e+13,  8.7789e+13, -5.2729e+13,  2.4084e+13, -1.4388e+15,\n",
            "         -1.2478e+14,  1.5753e+15,  4.0871e+14,  1.5388e+15,  1.3038e+14],\n",
            "        [ 1.0168e+14,  1.8864e+14,  1.3733e+14,  1.9290e+13,  1.5538e+15,\n",
            "          1.6654e+15,  1.2553e+14, -7.8724e+13,  1.3901e+14, -4.8932e+13,\n",
            "          1.2345e+12,  1.5496e+15,  1.1109e+14,  1.1514e+12, -1.5294e+15,\n",
            "         -8.3393e+13,  1.2515e+14,  1.5380e+15,  4.2243e+13,  6.8403e+13,\n",
            "          2.1083e+12, -4.8321e+12, -4.3060e+13, -1.5853e+15,  7.3308e+12,\n",
            "          5.9145e+13,  1.0939e+14,  6.6210e+13, -1.5014e+15, -1.5016e+15,\n",
            "         -9.7917e+13, -1.8891e+13,  6.5895e+13,  1.5538e+15, -2.7455e+12,\n",
            "          1.5323e+15,  1.6199e+15,  6.1442e+13, -7.5357e+13,  1.5596e+15,\n",
            "         -6.4682e+11,  6.7592e+13,  3.1782e+13,  1.0183e+13,  5.7991e+13,\n",
            "         -7.1177e+13,  1.0507e+13,  5.5566e+13,  1.4308e+15,  4.5254e+13]])\n",
            "calculating 2.bias\n",
            "tensor([ 1.5372e+15,  2.6100e-01,  1.2300e-01,  2.2200e-01,  2.5000e-02,\n",
            "        -1.5372e+15,  1.0400e-01,  1.6000e-01, -1.5372e+15,  1.5372e+15])\n",
            "Test Accuracy: 0.088800\n",
            "Avg loss (5/0): 7130910928056977851097808896.000000\n",
            "Avg loss (5/1): 4921707311992443856809361408.000000\n",
            "Avg loss (5/2): 2824731325689117339730051072.000000\n",
            "Avg loss (5/3): 2399834778079442525187211264.000000\n",
            "Avg loss (5/4): 2546017551018056058570539008.000000\n",
            "Avg loss (5/5): 2729284920372692326140608512.000000\n",
            "Avg loss (5/6): 2787470968695560300935512064.000000\n",
            "Avg loss (5/7): 2674104363168265701171396608.000000\n",
            "Avg loss (5/8): 2405334564144554585744211968.000000\n",
            "Avg loss (5/9): 2040624724933005989126340608.000000\n",
            "Avg loss (5/10): 1677632056233790867249299456.000000\n",
            "Avg loss (5/11): 1426044440084048366255734784.000000\n",
            "Avg loss (5/12): 1313732545595912183814815744.000000\n",
            "Avg loss (5/13): 1258977739244706775296901120.000000\n",
            "Avg loss (5/14): 1178042649621306117581701120.000000\n",
            "Avg loss (5/15): 1057694763118851695751200768.000000\n",
            "Avg loss (5/16): 926000358123634823559905280.000000\n",
            "Avg loss (5/17): 806719726446308941547700224.000000\n",
            "Avg loss (5/18): 704803457687423629500874752.000000\n",
            "Avg loss (5/19): 616153312503099131175108608.000000\n",
            "sharing worker_0 model\n",
            "sharing worker_1 model\n",
            "sharing worker_2 model\n",
            "{'worker_0': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:97718858165 -> worker_0:49842550607]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95875616669 -> worker_1:67796112855]\n",
            "\t-> (Wrapper)>[PointerTensor | me:55117352736 -> worker_2:6160791830]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:76166211274 -> worker_0:11885841320]\n",
            "\t-> (Wrapper)>[PointerTensor | me:63955020064 -> worker_1:47455151627]\n",
            "\t-> (Wrapper)>[PointerTensor | me:53460250389 -> worker_2:46436317360]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79677477351 -> worker_0:55393651077]\n",
            "\t-> (Wrapper)>[PointerTensor | me:12014124340 -> worker_1:30671558804]\n",
            "\t-> (Wrapper)>[PointerTensor | me:69105662149 -> worker_2:51989032086]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:49257142465 -> worker_0:58602165075]\n",
            "\t-> (Wrapper)>[PointerTensor | me:9743883484 -> worker_1:47458090565]\n",
            "\t-> (Wrapper)>[PointerTensor | me:7071867762 -> worker_2:34450344448]\n",
            "\t*crypto provider: me*}, 'worker_1': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:13534559639 -> worker_0:19181341019]\n",
            "\t-> (Wrapper)>[PointerTensor | me:3179121100 -> worker_1:26014183564]\n",
            "\t-> (Wrapper)>[PointerTensor | me:19559390770 -> worker_2:59658549023]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:49263105245 -> worker_0:70377057183]\n",
            "\t-> (Wrapper)>[PointerTensor | me:19502582584 -> worker_1:32201077467]\n",
            "\t-> (Wrapper)>[PointerTensor | me:20294190294 -> worker_2:76991579112]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:18367650456 -> worker_0:59289878404]\n",
            "\t-> (Wrapper)>[PointerTensor | me:28027119143 -> worker_1:76159335491]\n",
            "\t-> (Wrapper)>[PointerTensor | me:60762500657 -> worker_2:66748771167]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:75287584106 -> worker_0:89479529903]\n",
            "\t-> (Wrapper)>[PointerTensor | me:75938332884 -> worker_1:54804666541]\n",
            "\t-> (Wrapper)>[PointerTensor | me:20328848080 -> worker_2:72943864172]\n",
            "\t*crypto provider: me*}, 'worker_2': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:2902181995 -> worker_0:40757921152]\n",
            "\t-> (Wrapper)>[PointerTensor | me:63483465414 -> worker_1:79687050755]\n",
            "\t-> (Wrapper)>[PointerTensor | me:63483831389 -> worker_2:78133571636]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:82191271154 -> worker_0:8576117828]\n",
            "\t-> (Wrapper)>[PointerTensor | me:80717178045 -> worker_1:46927416733]\n",
            "\t-> (Wrapper)>[PointerTensor | me:47060055971 -> worker_2:60721531445]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:23062474289 -> worker_0:5731144367]\n",
            "\t-> (Wrapper)>[PointerTensor | me:86086331954 -> worker_1:36713528875]\n",
            "\t-> (Wrapper)>[PointerTensor | me:7957972601 -> worker_2:10908547734]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:50918816878 -> worker_0:96729825748]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79883883964 -> worker_1:64493331766]\n",
            "\t-> (Wrapper)>[PointerTensor | me:23553500269 -> worker_2:93388359125]\n",
            "\t*crypto provider: me*}}\n",
            "grad sum =  {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:97718858165 -> worker_0:49842550607]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95875616669 -> worker_1:67796112855]\n",
            "\t-> (Wrapper)>[PointerTensor | me:55117352736 -> worker_2:6160791830]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:76166211274 -> worker_0:11885841320]\n",
            "\t-> (Wrapper)>[PointerTensor | me:63955020064 -> worker_1:47455151627]\n",
            "\t-> (Wrapper)>[PointerTensor | me:53460250389 -> worker_2:46436317360]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79677477351 -> worker_0:55393651077]\n",
            "\t-> (Wrapper)>[PointerTensor | me:12014124340 -> worker_1:30671558804]\n",
            "\t-> (Wrapper)>[PointerTensor | me:69105662149 -> worker_2:51989032086]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:49257142465 -> worker_0:58602165075]\n",
            "\t-> (Wrapper)>[PointerTensor | me:9743883484 -> worker_1:47458090565]\n",
            "\t-> (Wrapper)>[PointerTensor | me:7071867762 -> worker_2:34450344448]\n",
            "\t*crypto provider: me*}\n",
            "calculating 0.weight\n",
            "tensor([[ 0.0000e+00,  0.0000e+00, -1.0000e-03,  ...,  0.0000e+00,\n",
            "         -1.0000e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  1.5372e+15,  ...,  0.0000e+00,\n",
            "          1.5372e+15, -1.5372e+15],\n",
            "        [-1.0000e-03,  0.0000e+00, -1.5372e+15,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00, -1.5372e+15,  1.0000e-03,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 1.5372e+15, -1.0000e-03,  0.0000e+00,  ...,  1.0000e-03,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.5372e+15,  1.0000e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          1.0000e-03,  0.0000e+00]])\n",
            "calculating 0.bias\n",
            "tensor([ 1.5373e+15,  9.7283e+10,  1.9985e+11,  4.4425e+11,  3.9768e+10,\n",
            "         4.4744e+11,  1.5373e+15,  1.5911e+11,  2.4579e+11,  1.5376e+15,\n",
            "         2.7135e+10,  1.2014e+10,  1.7206e+11,  1.3015e+10,  1.4874e+11,\n",
            "         2.0857e+11, -1.5369e+15,  7.9521e+10,  1.8695e+11,  1.9823e+11,\n",
            "         2.6455e+11,  9.2538e+10,  4.8766e+11, -1.5371e+15,  1.5611e+11,\n",
            "         5.2122e+11,  2.1183e+11,  2.6697e+11,  7.2796e+10,  3.1782e+11,\n",
            "         2.0647e+11,  1.5376e+15,  6.6653e+11,  1.5373e+15,  4.7381e+10,\n",
            "         1.5374e+15,  3.3640e+11,  2.0865e+11,  9.1993e+10,  4.7682e+10,\n",
            "         1.5611e+11,  9.4853e+10,  7.1241e+10,  1.5373e+15,  2.7742e+11,\n",
            "        -1.5370e+15,  1.5373e+15,  4.0196e+11,  1.2829e+11,  1.5374e+15])\n",
            "calculating 2.weight\n",
            "tensor([[ 1.3239e+13,  3.4128e+13,  5.2466e+11,  1.5409e+15, -5.6832e+12,\n",
            "         -2.7014e+14, -1.4838e+15,  1.7264e+14,  1.0985e+14,  7.9778e+13,\n",
            "          1.7261e+13,  2.7811e+12,  4.7538e+13,  1.5501e+15,  1.5842e+15,\n",
            "         -1.9873e+14, -3.0323e+13,  3.5325e+13, -4.0893e+13, -1.4501e+15,\n",
            "          1.7099e+13,  1.6613e+13,  3.6333e+14,  7.9293e+13,  1.7497e+13,\n",
            "          1.2476e+14, -6.1042e+13, -1.9271e+13, -8.9072e+11, -1.6179e+15,\n",
            "          1.7665e+14,  4.2497e+13, -1.6340e+15,  1.6944e+15,  2.0363e+13,\n",
            "          5.4688e+13,  1.4674e+15,  6.5370e+13,  1.5559e+15,  5.1205e+13,\n",
            "          1.5256e+15, -1.4481e+15,  5.2592e+13,  1.5492e+15,  4.2383e+13,\n",
            "          9.1514e+12, -7.8179e+13, -2.6205e+14,  1.5479e+15,  5.7438e+11],\n",
            "        [ 8.9164e+12,  1.3280e+14,  9.1206e+13,  1.3804e+14,  1.3797e+13,\n",
            "         -1.5109e+15,  8.7454e+13, -1.9103e+14,  1.8450e+13,  1.6054e+15,\n",
            "         -1.5126e+15,  1.5404e+15,  9.2694e+13, -1.5198e+15,  4.2900e+13,\n",
            "          1.9285e+14,  1.3572e+14,  1.4952e+15,  1.5983e+14, -1.2457e+14,\n",
            "         -4.6947e+13,  1.0742e+13,  1.7161e+15,  7.2431e+13,  2.6672e+13,\n",
            "          1.3425e+14,  1.6924e+15, -1.4031e+15,  1.9865e+12,  1.6220e+15,\n",
            "          1.6001e+14,  1.6033e+15, -9.2951e+13,  1.5940e+14,  3.5653e+13,\n",
            "         -1.5092e+15,  1.2704e+14,  6.0798e+13,  1.4290e+14,  4.4227e+13,\n",
            "          4.0970e+13,  7.2507e+13, -1.6350e+15,  3.3909e+12, -1.4429e+15,\n",
            "          1.5528e+15,  5.0107e+13,  3.2396e+14,  8.9110e+13,  7.1666e+13],\n",
            "        [ 6.1580e+13, -8.0371e+13,  1.4719e+15, -1.5542e+15, -1.5309e+15,\n",
            "          2.6601e+14,  6.3114e+13, -1.5368e+15, -1.2483e+14,  5.2504e+13,\n",
            "         -1.5532e+15,  1.4351e+12, -1.5285e+15,  2.3184e+13, -1.8103e+13,\n",
            "          9.6627e+13, -6.7126e+13,  1.5491e+15, -1.5567e+15,  1.6159e+15,\n",
            "          7.4934e+13,  2.4292e+13,  1.1806e+14,  3.1417e+13, -1.5023e+15,\n",
            "          4.9423e+13, -2.3614e+14,  1.3562e+13,  3.9745e+12,  2.3253e+13,\n",
            "          1.4958e+15,  4.3289e+13, -9.3225e+13, -2.6039e+13,  1.4016e+13,\n",
            "         -6.9008e+09,  9.2867e+13,  3.1389e+13,  1.0114e+14,  1.5488e+15,\n",
            "         -1.5356e+13,  1.3780e+11, -3.3985e+13,  3.4312e+13, -1.5045e+15,\n",
            "         -3.2630e+13,  2.7187e+13,  5.1181e+13, -1.0579e+13, -4.7386e+13],\n",
            "        [ 2.2570e+13, -4.9747e+12, -1.5165e+15, -1.4875e+15,  4.2244e+12,\n",
            "          1.2460e+14,  5.2857e+12, -1.4467e+15,  1.0698e+14,  1.5645e+15,\n",
            "          1.5471e+15,  6.0342e+12, -1.4992e+15, -1.5328e+15,  1.5267e+15,\n",
            "         -1.1271e+12,  1.7127e+14, -3.6398e+13,  1.8312e+14, -6.6175e+13,\n",
            "          8.9524e+13,  6.6410e+12,  1.2810e+14,  1.6078e+15,  5.8389e+13,\n",
            "          2.0682e+13, -1.5599e+15,  1.6703e+10,  5.3772e+12, -2.4055e+13,\n",
            "          1.1605e+14,  1.5367e+14, -1.3512e+15,  1.2622e+13,  7.4788e+12,\n",
            "          1.1766e+13,  1.0428e+14, -1.3795e+15,  5.8474e+12, -1.7198e+13,\n",
            "          1.2235e+13,  1.5303e+15, -6.1983e+13,  5.6762e+11,  2.0346e+13,\n",
            "         -7.8198e+13,  2.1648e+13,  3.5231e+14,  1.6431e+15,  6.2271e+13],\n",
            "        [ 4.7786e+13, -1.4577e+15,  1.6399e+14,  2.0028e+14, -6.3217e+12,\n",
            "          1.5482e+15,  1.1542e+14, -1.5379e+14,  1.9326e+14, -1.4381e+15,\n",
            "          1.7015e+13,  1.4231e+12,  5.9455e+13,  5.9438e+12,  2.4083e+13,\n",
            "          6.6686e+13,  5.0727e+14,  1.5448e+15,  6.0818e+13,  1.8690e+14,\n",
            "          2.7556e+14,  1.7098e+12,  5.1065e+14,  1.1044e+14,  2.2944e+13,\n",
            "          1.7035e+14, -3.2197e+13,  1.2002e+14,  1.5624e+15,  1.7074e+14,\n",
            "          8.7677e+13,  2.3914e+14,  3.7604e+12,  1.1523e+13,  2.4122e+13,\n",
            "          1.5642e+15, -2.2890e+13,  1.4756e+15, -1.5442e+15, -1.4402e+15,\n",
            "         -5.0876e+13, -8.6989e+13, -1.4625e+15,  3.3419e+13,  1.6010e+15,\n",
            "          3.8814e+13,  1.5229e+15, -9.3866e+14,  1.8159e+14, -5.2712e+12],\n",
            "        [-1.5358e+15,  1.5956e+15, -2.2427e+13,  2.6218e+13,  1.5308e+13,\n",
            "         -3.2067e+14, -8.7400e+13, -2.2061e+13,  1.3909e+14, -1.5079e+15,\n",
            "          3.0277e+13,  8.1590e+09, -1.5271e+15,  1.5326e+15,  7.0398e+13,\n",
            "         -7.9430e+13, -1.3398e+15,  8.8875e+13,  6.5702e+13,  1.5630e+15,\n",
            "         -1.5261e+15, -3.2297e+12,  7.8681e+13, -1.5578e+15,  2.4441e+13,\n",
            "          5.5933e+13, -8.7365e+13,  5.6404e+13,  7.4095e+12, -1.6230e+15,\n",
            "         -1.4886e+15, -1.5105e+15, -6.0320e+13,  2.2538e+13,  3.6771e+13,\n",
            "          2.8450e+13,  1.3524e+14,  1.4601e+14,  6.9106e+13, -2.1831e+13,\n",
            "         -1.3041e+15,  2.8399e+13,  1.1608e+14, -9.1112e+12,  5.0176e+13,\n",
            "          1.1616e+13, -3.6983e+13,  1.6747e+14, -3.0012e+13,  8.3776e+13],\n",
            "        [-1.1720e+13,  1.6476e+15,  5.5914e+13,  1.6531e+15,  1.2172e+13,\n",
            "          1.4068e+15, -5.5641e+11,  1.4634e+15,  5.5078e+12,  1.2130e+14,\n",
            "          1.3259e+13,  1.5396e+15,  3.3137e+13, -1.5341e+15,  2.4079e+11,\n",
            "          5.4306e+13, -1.4075e+15,  3.3102e+13,  1.7676e+15,  1.6130e+15,\n",
            "          5.4809e+13, -1.5294e+15,  1.7526e+15,  3.8532e+13,  1.6157e+15,\n",
            "          4.0335e+13, -1.6592e+15, -1.5072e+15, -5.5461e+12, -1.7107e+12,\n",
            "          6.1632e+13,  1.0203e+14,  1.2089e+13, -1.6538e+15,  5.0389e+12,\n",
            "          1.5723e+15,  3.2825e+13,  8.9678e+13,  3.4552e+13,  6.4474e+13,\n",
            "          1.5680e+15,  1.5163e+14, -2.7803e+13, -2.5750e+12,  1.2903e+14,\n",
            "          1.5065e+15,  1.5634e+15,  2.5365e+14, -3.3642e+13,  1.5654e+15],\n",
            "        [ 1.1512e+14,  3.6904e+13,  1.1417e+14,  1.6073e+15,  1.5446e+15,\n",
            "          1.4649e+15,  1.5568e+14,  1.2433e+14,  1.5690e+15,  3.1162e+13,\n",
            "          1.5261e+15,  1.5428e+15,  2.0507e+13, -1.5267e+15,  5.0697e+13,\n",
            "          1.4700e+14,  1.1325e+13, -1.5326e+15,  1.1453e+13, -1.0281e+14,\n",
            "          1.5344e+13,  1.3607e+13, -1.5529e+15,  9.5286e+12,  1.3582e+12,\n",
            "          5.7065e+13,  1.5955e+14, -1.4489e+15,  2.6460e+13,  3.3782e+14,\n",
            "         -1.1197e+13,  1.6098e+15,  4.4319e+13,  1.5676e+15,  1.5367e+15,\n",
            "          1.1413e+13,  1.1661e+14,  1.2905e+14,  1.5558e+15,  5.3153e+13,\n",
            "         -2.6515e+13,  1.3091e+14,  1.0131e+14,  2.6597e+13,  2.0173e+13,\n",
            "          6.0842e+13, -1.5200e+15,  1.0312e+14, -1.4546e+15, -4.7443e+13],\n",
            "        [ 5.4277e+13,  1.7244e+13,  1.0922e+14,  1.6595e+15,  8.1422e+12,\n",
            "          1.4744e+15,  7.1313e+12, -8.1184e+12,  1.0056e+14,  3.9321e+13,\n",
            "          2.8005e+13,  4.9870e+12,  3.5508e+13,  2.3589e+13,  7.7204e+13,\n",
            "         -1.0734e+13, -1.3496e+15,  1.6688e+13,  1.4347e+14,  1.5622e+15,\n",
            "          1.0656e+14,  1.1025e+13,  7.4432e+13,  3.7976e+13,  1.5424e+15,\n",
            "          1.6174e+15,  8.2887e+13, -1.4338e+15, -2.1458e+12, -2.4846e+12,\n",
            "         -5.1171e+13,  1.7130e+13, -1.2989e+15, -7.3675e+11,  1.3315e+13,\n",
            "         -1.5330e+15,  1.3601e+14,  6.3659e+13,  1.5685e+15,  6.4126e+13,\n",
            "          1.1571e+14, -1.4524e+15, -1.2284e+13,  4.5103e+13,  1.3734e+14,\n",
            "         -1.9156e+13,  3.9317e+13, -1.1564e+15,  2.6984e+13,  1.6450e+15],\n",
            "        [-1.5001e+15,  7.0285e+13,  1.6662e+15,  2.7255e+13,  1.9795e+13,\n",
            "         -2.3287e+13,  1.4363e+14, -3.8187e+13, -1.4751e+15, -1.5639e+15,\n",
            "         -4.4157e+12,  8.0048e+12,  7.4340e+13, -2.0317e+12,  7.3533e+12,\n",
            "         -4.7005e+13,  8.1771e+13, -1.3271e+12,  1.0043e+14,  3.2755e+13,\n",
            "         -2.9060e+12, -3.9811e+12, -5.1668e+12, -2.1150e+13, -4.1902e+12,\n",
            "          1.5668e+15, -1.4635e+15,  1.8306e+13,  4.2038e+13,  1.1182e+14,\n",
            "         -1.6468e+15, -1.6281e+13,  1.0922e+14,  1.3868e+11,  2.0728e+12,\n",
            "         -1.0365e+13,  6.2360e+11,  1.5990e+14,  5.7885e+12, -1.4166e+13,\n",
            "          1.5061e+15,  1.3354e+14,  1.1009e+14,  1.4306e+13,  3.4662e+13,\n",
            "         -1.0687e+14, -1.5111e+15, -4.7580e+13, -3.6474e+13,  2.5018e+12]])\n",
            "calculating 2.bias\n",
            "tensor([ 1.5372e+15,  1.1600e-01,  2.8000e-02,  1.4000e-01,  9.0000e-03,\n",
            "        -1.5372e+15,  1.0100e-01,  1.1800e-01,  1.5372e+15,  6.7000e-02])\n",
            "Test Accuracy: 0.100100\n",
            "Avg loss (6/0): 11134906889481820123946811392.000000\n",
            "Avg loss (6/1): 7214378165345888471544233984.000000\n",
            "Avg loss (6/2): 3401240482248990241593491456.000000\n",
            "Avg loss (6/3): 2510317640999182258166300672.000000\n",
            "Avg loss (6/4): 2641178548310172637977378816.000000\n",
            "Avg loss (6/5): 2829173891957876958464835584.000000\n",
            "Avg loss (6/6): 2958316743312818325476605952.000000\n",
            "Avg loss (6/7): 2959603293031495124444512256.000000\n",
            "Avg loss (6/8): 2815790410197519204576395264.000000\n",
            "Avg loss (6/9): 2563466104876449038929494016.000000\n",
            "Avg loss (6/10): 2267115914705348119337369600.000000\n",
            "Avg loss (6/11): 1998035472511435735060971520.000000\n",
            "Avg loss (6/12): 1820409265068492438051487744.000000\n",
            "Avg loss (6/13): 1725211374288228439541415936.000000\n",
            "Avg loss (6/14): 1637974950676224893832396800.000000\n",
            "Avg loss (6/15): 1492752441447112384867991552.000000\n",
            "Avg loss (6/16): 1292851569174235919767568384.000000\n",
            "Avg loss (6/17): 1083252431885071100162342912.000000\n",
            "Avg loss (6/18): 900921050324734855241269248.000000\n",
            "Avg loss (6/19): 754678657509275092587118592.000000\n",
            "sharing worker_0 model\n",
            "sharing worker_1 model\n",
            "sharing worker_2 model\n",
            "{'worker_0': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:84351786393 -> worker_0:60320544353]\n",
            "\t-> (Wrapper)>[PointerTensor | me:7418055822 -> worker_1:58912603567]\n",
            "\t-> (Wrapper)>[PointerTensor | me:22596678216 -> worker_2:29519765327]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95466807147 -> worker_0:83471258641]\n",
            "\t-> (Wrapper)>[PointerTensor | me:75610253549 -> worker_1:13415472720]\n",
            "\t-> (Wrapper)>[PointerTensor | me:37110932823 -> worker_2:14910953378]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:55203407312 -> worker_0:43949121891]\n",
            "\t-> (Wrapper)>[PointerTensor | me:38122972019 -> worker_1:13074383097]\n",
            "\t-> (Wrapper)>[PointerTensor | me:85856364566 -> worker_2:52499759309]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:73112916716 -> worker_0:75187704997]\n",
            "\t-> (Wrapper)>[PointerTensor | me:13178926617 -> worker_1:4524967078]\n",
            "\t-> (Wrapper)>[PointerTensor | me:80004188460 -> worker_2:88816855091]\n",
            "\t*crypto provider: me*}, 'worker_1': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:73233971929 -> worker_0:59474920737]\n",
            "\t-> (Wrapper)>[PointerTensor | me:15193948471 -> worker_1:93290502770]\n",
            "\t-> (Wrapper)>[PointerTensor | me:75112089011 -> worker_2:59881365621]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:63313088829 -> worker_0:15981536270]\n",
            "\t-> (Wrapper)>[PointerTensor | me:83598754249 -> worker_1:53192800265]\n",
            "\t-> (Wrapper)>[PointerTensor | me:35690172814 -> worker_2:95110244572]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:30440058934 -> worker_0:79469628794]\n",
            "\t-> (Wrapper)>[PointerTensor | me:80127051860 -> worker_1:72717622241]\n",
            "\t-> (Wrapper)>[PointerTensor | me:34327457160 -> worker_2:69324351495]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:99002692644 -> worker_0:81544471840]\n",
            "\t-> (Wrapper)>[PointerTensor | me:65427456128 -> worker_1:86645244027]\n",
            "\t-> (Wrapper)>[PointerTensor | me:45358952224 -> worker_2:26571729529]\n",
            "\t*crypto provider: me*}, 'worker_2': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:71017472991 -> worker_0:88382493787]\n",
            "\t-> (Wrapper)>[PointerTensor | me:36987978305 -> worker_1:93594892454]\n",
            "\t-> (Wrapper)>[PointerTensor | me:40420358064 -> worker_2:49786105698]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:82098240988 -> worker_0:70944794425]\n",
            "\t-> (Wrapper)>[PointerTensor | me:65119824013 -> worker_1:26588795111]\n",
            "\t-> (Wrapper)>[PointerTensor | me:69284165846 -> worker_2:35167828443]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:8151975593 -> worker_0:35445204231]\n",
            "\t-> (Wrapper)>[PointerTensor | me:19151108064 -> worker_1:28634018995]\n",
            "\t-> (Wrapper)>[PointerTensor | me:388122954 -> worker_2:69012822860]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:65645796490 -> worker_0:60583738488]\n",
            "\t-> (Wrapper)>[PointerTensor | me:92204523731 -> worker_1:56809475495]\n",
            "\t-> (Wrapper)>[PointerTensor | me:78388757599 -> worker_2:5922508006]\n",
            "\t*crypto provider: me*}}\n",
            "grad sum =  {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:84351786393 -> worker_0:60320544353]\n",
            "\t-> (Wrapper)>[PointerTensor | me:7418055822 -> worker_1:58912603567]\n",
            "\t-> (Wrapper)>[PointerTensor | me:22596678216 -> worker_2:29519765327]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95466807147 -> worker_0:83471258641]\n",
            "\t-> (Wrapper)>[PointerTensor | me:75610253549 -> worker_1:13415472720]\n",
            "\t-> (Wrapper)>[PointerTensor | me:37110932823 -> worker_2:14910953378]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:55203407312 -> worker_0:43949121891]\n",
            "\t-> (Wrapper)>[PointerTensor | me:38122972019 -> worker_1:13074383097]\n",
            "\t-> (Wrapper)>[PointerTensor | me:85856364566 -> worker_2:52499759309]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:73112916716 -> worker_0:75187704997]\n",
            "\t-> (Wrapper)>[PointerTensor | me:13178926617 -> worker_1:4524967078]\n",
            "\t-> (Wrapper)>[PointerTensor | me:80004188460 -> worker_2:88816855091]\n",
            "\t*crypto provider: me*}\n",
            "calculating 0.weight\n",
            "tensor([[-1.5372e+15,  0.0000e+00, -1.5372e+15,  ...,  1.5372e+15,\n",
            "          1.0000e-03,  1.0000e-03],\n",
            "        [-1.5372e+15, -1.5372e+15,  1.0000e-03,  ...,  0.0000e+00,\n",
            "         -1.5372e+15, -1.5372e+15],\n",
            "        [-1.5372e+15,  0.0000e+00, -1.5372e+15,  ..., -1.0000e-03,\n",
            "          1.0000e-03, -1.5372e+15],\n",
            "        ...,\n",
            "        [ 1.0000e-03,  0.0000e+00, -1.0000e-03,  ...,  0.0000e+00,\n",
            "         -1.5372e+15,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.5372e+15,  0.0000e+00,  ...,  1.0000e-03,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.0000e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5372e+15,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "calculating 0.bias\n",
            "tensor([ 1.5374e+15,  1.0834e+11,  2.3689e+11,  5.3288e+11,  1.6786e+11,\n",
            "         1.5377e+15,  1.6087e+11,  3.4867e+11,  4.2762e+11,  1.5375e+15,\n",
            "         8.3867e+09,  2.2495e+10,  1.7821e+11, -1.5371e+15, -1.5372e+15,\n",
            "        -1.5370e+15,  6.5818e+11,  1.1985e+11,  2.3034e+11,  1.7630e+11,\n",
            "         6.6431e+11, -1.5371e+15,  4.2612e+11,  8.3916e+10,  1.9279e+11,\n",
            "         4.8950e+11, -1.5370e+15, -1.5367e+15,  1.5373e+15,  3.3330e+11,\n",
            "        -1.5369e+15,  3.7414e+11,  4.5921e+11,  2.2593e+11,  1.0560e+11,\n",
            "        -1.5370e+15,  1.9091e+11, -1.5370e+15,  6.1997e+10,  1.0762e+11,\n",
            "         1.2572e+11,  1.2210e+11,  1.5373e+15,  1.3924e+11,  2.9715e+11,\n",
            "         3.2498e+11,  1.1089e+11,  6.7734e+11, -1.5371e+15, -1.5371e+15])\n",
            "calculating 2.weight\n",
            "tensor([[ 1.5514e+15,  5.6174e+13,  4.3244e+13,  5.2232e+13,  3.1451e+13,\n",
            "         -1.6967e+15,  8.5331e+13,  2.8893e+14,  1.9397e+14,  4.5167e+13,\n",
            "         -6.1662e+12, -2.8842e+12,  1.5890e+15,  1.1426e+13,  2.5810e+12,\n",
            "         -1.7326e+15,  6.6617e+13, -1.4913e+15, -5.2299e+12,  2.3484e+14,\n",
            "          9.7805e+13,  1.5789e+15, -1.2180e+15,  1.4876e+14,  5.9016e+13,\n",
            "         -1.3944e+15, -1.4524e+15,  1.5562e+15,  1.5352e+15,  3.1539e+13,\n",
            "          2.4754e+14,  6.9879e+13,  2.4145e+14,  1.5295e+15,  1.9788e+13,\n",
            "          6.7903e+13, -3.0413e+13,  1.5882e+15, -1.5193e+15,  8.0712e+13,\n",
            "          1.5184e+15,  1.1171e+14,  1.0655e+14,  1.3075e+13, -1.4613e+15,\n",
            "          7.7474e+13, -1.5605e+15,  8.6393e+13, -5.2216e+13,  3.7684e+13],\n",
            "        [-1.5302e+15,  1.0463e+14,  7.2742e+13,  1.6769e+14,  7.3300e+13,\n",
            "         -1.4725e+15,  7.4375e+13, -1.8087e+14,  9.3476e+12,  2.4269e+13,\n",
            "          1.8364e+13, -1.5321e+15,  6.3406e+13,  1.8619e+13,  1.5409e+15,\n",
            "          1.9546e+14, -1.3789e+15, -8.1328e+13,  5.1031e+13, -1.4759e+14,\n",
            "          1.5315e+15,  3.0048e+13,  4.9133e+13,  6.7103e+13,  3.4213e+13,\n",
            "          8.9068e+13,  7.1366e+13,  1.6855e+15, -3.7006e+11, -2.8072e+13,\n",
            "          9.7525e+13,  1.3629e+13, -8.2783e+13,  1.1473e+14,  2.5336e+13,\n",
            "          7.3538e+13, -1.4546e+15,  5.7602e+13,  7.9524e+13,  1.9703e+13,\n",
            "         -1.5401e+15,  6.5353e+13,  1.4963e+15,  6.3185e+12,  1.5365e+14,\n",
            "          1.5309e+15,  1.3079e+13,  3.4609e+14,  1.5871e+15,  1.0808e+14],\n",
            "        [ 3.8165e+13,  1.5259e+15, -7.6632e+13,  1.1768e+13,  7.3562e+13,\n",
            "          1.7312e+15,  4.9228e+13,  5.9502e+13, -5.1229e+13,  3.4171e+13,\n",
            "          1.2337e+13,  1.5426e+15,  2.3681e+13,  3.1906e+13, -2.2807e+10,\n",
            "          1.2370e+14,  1.4996e+15, -3.7621e+13, -1.4174e+15,  7.2227e+13,\n",
            "          1.8983e+13,  3.8624e+13,  5.1309e+13,  5.9373e+13,  1.1249e+14,\n",
            "          1.5598e+15, -2.2035e+14,  2.9040e+13,  1.1703e+12, -1.4839e+15,\n",
            "         -1.6108e+15,  3.6776e+13,  1.1373e+13,  1.5909e+13,  6.7666e+12,\n",
            "          7.3418e+12, -1.4443e+15,  1.0894e+13,  7.4026e+13, -1.5624e+15,\n",
            "          3.8245e+12, -1.5958e+15, -3.0044e+13,  3.6858e+13,  1.6395e+15,\n",
            "         -8.1271e+12,  3.8638e+12,  1.9877e+14,  1.4272e+13, -1.5656e+15],\n",
            "        [-6.4018e+12,  2.7204e+12,  9.7629e+13,  1.5738e+14,  1.5715e+15,\n",
            "          2.2040e+14,  4.4730e+13,  1.2667e+14,  1.7840e+14,  2.4161e+12,\n",
            "          6.9966e+12,  1.5445e+15,  5.1565e+13,  1.8513e+13,  8.9791e+10,\n",
            "          1.5574e+15,  2.4158e+14,  1.4581e+15,  3.1379e+14, -1.4440e+14,\n",
            "          2.2835e+14,  6.1201e+12, -1.3936e+15,  1.1537e+14,  8.7457e+13,\n",
            "          6.0431e+13,  1.4443e+14, -1.5123e+15,  1.1012e+12,  4.5834e+13,\n",
            "          1.2454e+14,  1.7666e+15,  2.9321e+14, -6.4038e+13,  5.8894e+12,\n",
            "          1.7863e+13,  7.3454e+13,  1.5867e+14, -2.6362e+13, -1.5159e+15,\n",
            "          6.1034e+13,  7.7801e+11, -1.7581e+13,  3.6830e+13,  6.4042e+12,\n",
            "         -1.0812e+14,  7.0732e+12,  4.1216e+14, -1.5063e+15,  6.8437e+13],\n",
            "        [ 1.6460e+15, -1.5195e+15,  6.3468e+13,  4.7021e+14,  1.5618e+15,\n",
            "         -3.9214e+13,  2.0116e+14, -7.0142e+12,  3.5099e+14,  6.1030e+13,\n",
            "          1.5503e+15, -1.5304e+15,  8.7542e+13,  1.0938e+13, -5.7789e+12,\n",
            "         -1.4802e+15,  6.6136e+14,  1.2241e+14,  1.5397e+15,  3.2855e+14,\n",
            "          6.8193e+14,  4.1165e+13,  4.7541e+14,  9.5202e+13,  6.2762e+13,\n",
            "         -1.2927e+15, -5.7059e+13, -1.3084e+15,  2.8415e+13,  3.2906e+14,\n",
            "          3.9212e+14,  3.9799e+14, -4.9949e+13, -1.4949e+15,  5.4131e+13,\n",
            "          5.1607e+13, -5.5482e+13, -1.5638e+15, -1.4681e+15,  1.4305e+14,\n",
            "         -3.8792e+13,  1.5622e+15,  1.8397e+15, -1.4967e+15,  8.5556e+13,\n",
            "         -1.4169e+15, -1.9913e+13,  1.0546e+15,  9.8160e+13,  6.3004e+13],\n",
            "        [ 1.5464e+15,  4.4391e+13, -4.6267e+13, -1.2340e+15,  1.6263e+15,\n",
            "         -3.3186e+14,  6.7959e+13,  1.6355e+15, -1.2091e+15,  1.5839e+15,\n",
            "          1.2347e+13, -2.3403e+12,  6.9775e+13, -1.6612e+13,  6.8879e+12,\n",
            "         -2.3187e+14, -9.7034e+14, -1.4283e+15, -3.6130e+13,  1.5090e+14,\n",
            "          5.1946e+14,  2.0693e+13,  3.7813e+14,  1.4830e+14,  4.1368e+13,\n",
            "         -1.3465e+15, -1.1361e+14,  1.5199e+14, -1.5403e+15, -2.0193e+13,\n",
            "          3.6408e+14,  3.2252e+14, -4.1768e+13,  5.3006e+12,  3.4526e+13,\n",
            "          8.4071e+13, -4.4605e+13,  1.1974e+13,  6.6135e+13,  6.4021e+13,\n",
            "         -1.4680e+15, -1.9052e+13,  1.9106e+14, -3.6773e+13,  1.2917e+14,\n",
            "          8.1366e+13, -1.5783e+13,  6.7949e+14, -3.9589e+13,  4.7912e+13],\n",
            "        [-1.5438e+15,  1.5569e+15,  7.5855e+12,  7.5837e+13,  5.0605e+13,\n",
            "          1.5087e+15,  1.2227e+12, -3.8828e+13, -1.5559e+15,  5.1938e+13,\n",
            "         -6.3087e+12, -3.0619e+12,  1.5302e+15,  3.8810e+12, -4.5590e+11,\n",
            "          1.3394e+14,  1.0595e+14,  8.2195e+12,  5.6191e+13, -3.3930e+12,\n",
            "          1.6080e+13,  1.6256e+13, -5.9185e+13,  6.4319e+12,  4.0881e+13,\n",
            "          1.4084e+13,  1.3925e+14,  4.6089e+13,  1.5344e+15, -1.4205e+15,\n",
            "          5.1716e+12, -1.4907e+15,  1.6982e+15,  7.7186e+13,  5.8342e+12,\n",
            "          2.3318e+13,  1.1366e+14,  8.3970e+13,  4.7540e+13,  1.2454e+13,\n",
            "          2.1875e+13,  8.0269e+13, -2.7116e+13, -7.3405e+12,  1.0191e+14,\n",
            "         -8.0575e+13,  6.4801e+12,  1.6888e+15, -3.1560e+13,  1.4974e+15],\n",
            "        [ 1.4342e+14,  7.8521e+13,  1.5783e+14,  3.4365e+13,  1.5372e+15,\n",
            "         -1.1411e+14, -1.4627e+15,  1.4441e+14, -1.7694e+12, -1.6442e+12,\n",
            "         -1.5426e+15,  8.0973e+12, -1.5270e+15, -1.5152e+15,  9.6623e+12,\n",
            "          1.7064e+15,  1.4960e+15,  1.5944e+15,  1.6798e+14, -6.5490e+13,\n",
            "          1.2299e+13,  2.8910e+13, -5.8385e+13, -1.5479e+15, -5.7447e+12,\n",
            "         -1.5019e+15,  1.9504e+14,  4.3678e+13,  2.9098e+13,  3.7663e+14,\n",
            "          6.3310e+12,  1.5737e+15, -1.3384e+15,  1.4879e+15, -4.6344e+13,\n",
            "         -1.5391e+15,  1.0813e+14, -1.4361e+15,  6.8175e+12,  1.8828e+13,\n",
            "          8.8910e+13,  9.4723e+13, -1.4116e+15, -1.5114e+15, -1.1219e+13,\n",
            "          9.6704e+13, -1.5001e+15, -1.4777e+15,  1.6506e+15, -1.5495e+15],\n",
            "        [ 3.5226e+13, -1.5483e+15,  1.2131e+14,  1.6658e+14, -1.4740e+15,\n",
            "         -1.1336e+14, -1.4758e+15, -8.8349e+13,  1.4585e+14,  2.4760e+13,\n",
            "          2.0604e+13,  1.5249e+13,  3.3226e+13,  1.5889e+15,  1.5572e+15,\n",
            "         -1.4677e+15, -1.2382e+15,  2.7579e+12,  2.5666e+14, -1.1194e+14,\n",
            "          2.2677e+14,  1.6150e+13,  5.9544e+13,  1.1678e+14,  3.5757e+12,\n",
            "          7.0435e+13,  1.9108e+14,  1.6458e+14,  9.3226e+12, -3.3160e+13,\n",
            "          1.8896e+12,  2.4625e+13, -1.1462e+15, -5.3797e+13,  1.9292e+13,\n",
            "          1.5639e+15, -1.4837e+15,  4.7867e+13, -3.6043e+13,  1.6597e+13,\n",
            "          1.9117e+14,  8.3133e+13, -1.5401e+15,  8.0440e+13,  1.6656e+14,\n",
            "         -4.4587e+13,  1.9076e+13,  6.1316e+14,  1.5737e+15,  5.0883e+12],\n",
            "        [ 4.7516e+13, -1.4520e+15,  8.6574e+13,  3.8685e+13,  5.3503e+13,\n",
            "         -2.7961e+13,  9.6329e+13, -6.6132e+13,  1.0231e+14, -2.0492e+13,\n",
            "         -1.5391e+15,  2.0762e+13,  5.0969e+13,  2.1216e+12,  4.2251e+12,\n",
            "         -5.7033e+13,  7.1425e+13,  1.9880e+13,  1.8429e+14,  1.5750e+15,\n",
            "          5.9610e+13,  1.2418e+13, -4.2667e+13, -1.4057e+13, -1.8473e+13,\n",
            "          2.3879e+13,  1.1742e+14,  2.0959e+13,  5.2771e+13, -1.4674e+15,\n",
            "         -6.9069e+12, -1.5612e+15,  1.7857e+14, -7.1215e+13,  7.3691e+11,\n",
            "         -2.9410e+12,  1.4750e+15,  1.4239e+14, -1.2708e+13, -1.5394e+15,\n",
            "          1.4310e+15,  1.3452e+14,  1.1463e+14,  1.5622e+15, -1.4641e+15,\n",
            "         -1.4969e+14,  1.5894e+15, -2.8467e+13, -3.7061e+13, -1.9782e+13]])\n",
            "calculating 2.bias\n",
            "tensor([ 8.3000e-02,  5.3000e-02, -1.5372e+15,  1.4800e-01,  5.8000e-02,\n",
            "         3.1000e-02, -1.5372e+15,  1.4400e-01,  1.3100e-01,  6.8000e-02])\n",
            "Test Accuracy: 0.119200\n",
            "Avg loss (7/0): 10065397134357710050061975552.000000\n",
            "Avg loss (7/1): 6875286869156002520857837568.000000\n",
            "Avg loss (7/2): 3844845717698221283385278464.000000\n",
            "Avg loss (7/3): 2992775851538318126600945664.000000\n",
            "Avg loss (7/4): 2995816465257475819412914176.000000\n",
            "Avg loss (7/5): 3107341052708546078190862336.000000\n",
            "Avg loss (7/6): 3153846622683941164902514688.000000\n",
            "Avg loss (7/7): 3076404534731361853043113984.000000\n",
            "Avg loss (7/8): 2886090509140568536755732480.000000\n",
            "Avg loss (7/9): 2627097632049876073361440768.000000\n",
            "Avg loss (7/10): 2345933096748158036012433408.000000\n",
            "Avg loss (7/11): 2064110971913700400567418880.000000\n",
            "Avg loss (7/12): 1799978979498025045769322496.000000\n",
            "Avg loss (7/13): 1566867622212510031674343424.000000\n",
            "Avg loss (7/14): 1371384818932263299024158720.000000\n",
            "Avg loss (7/15): 1218703848352292249312165888.000000\n",
            "Avg loss (7/16): 1100361934587389298962595840.000000\n",
            "Avg loss (7/17): 1001762021478075490105294848.000000\n",
            "Avg loss (7/18): 909890447589182797942620160.000000\n",
            "Avg loss (7/19): 815641457324070418767675392.000000\n",
            "sharing worker_0 model\n",
            "sharing worker_1 model\n",
            "sharing worker_2 model\n",
            "{'worker_0': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:53982067368 -> worker_0:17022078885]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79159898620 -> worker_1:21276186034]\n",
            "\t-> (Wrapper)>[PointerTensor | me:96824346209 -> worker_2:81273798099]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:51312233566 -> worker_0:19503383814]\n",
            "\t-> (Wrapper)>[PointerTensor | me:18284301541 -> worker_1:27573532470]\n",
            "\t-> (Wrapper)>[PointerTensor | me:94415234159 -> worker_2:75018985114]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:67355565800 -> worker_0:18395305800]\n",
            "\t-> (Wrapper)>[PointerTensor | me:23656026095 -> worker_1:55601602277]\n",
            "\t-> (Wrapper)>[PointerTensor | me:74156352522 -> worker_2:58444983949]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79560147672 -> worker_0:7582545889]\n",
            "\t-> (Wrapper)>[PointerTensor | me:92730692362 -> worker_1:23258394276]\n",
            "\t-> (Wrapper)>[PointerTensor | me:60647876365 -> worker_2:90707959807]\n",
            "\t*crypto provider: me*}, 'worker_1': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:76797317460 -> worker_0:5358608101]\n",
            "\t-> (Wrapper)>[PointerTensor | me:62238702618 -> worker_1:671647512]\n",
            "\t-> (Wrapper)>[PointerTensor | me:83808003264 -> worker_2:91793489904]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:98314264975 -> worker_0:94354686422]\n",
            "\t-> (Wrapper)>[PointerTensor | me:54921269184 -> worker_1:93380227461]\n",
            "\t-> (Wrapper)>[PointerTensor | me:94198995937 -> worker_2:17142113808]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:27924710353 -> worker_0:35220805751]\n",
            "\t-> (Wrapper)>[PointerTensor | me:57091342804 -> worker_1:14892272988]\n",
            "\t-> (Wrapper)>[PointerTensor | me:40329921598 -> worker_2:52723701549]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:24947320423 -> worker_0:74431967579]\n",
            "\t-> (Wrapper)>[PointerTensor | me:62141505822 -> worker_1:57430309946]\n",
            "\t-> (Wrapper)>[PointerTensor | me:59589217912 -> worker_2:39911046903]\n",
            "\t*crypto provider: me*}, 'worker_2': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:89374278914 -> worker_0:39226055288]\n",
            "\t-> (Wrapper)>[PointerTensor | me:78890094130 -> worker_1:20677887409]\n",
            "\t-> (Wrapper)>[PointerTensor | me:56489496361 -> worker_2:47375605908]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:45578559892 -> worker_0:78573545842]\n",
            "\t-> (Wrapper)>[PointerTensor | me:94478374473 -> worker_1:8406161601]\n",
            "\t-> (Wrapper)>[PointerTensor | me:57921093246 -> worker_2:86684049700]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:28318013639 -> worker_0:35405193842]\n",
            "\t-> (Wrapper)>[PointerTensor | me:64829697734 -> worker_1:30132542710]\n",
            "\t-> (Wrapper)>[PointerTensor | me:3439977019 -> worker_2:79726554360]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:74920138336 -> worker_0:74972530581]\n",
            "\t-> (Wrapper)>[PointerTensor | me:66996901369 -> worker_1:34588987844]\n",
            "\t-> (Wrapper)>[PointerTensor | me:4978314453 -> worker_2:53058798556]\n",
            "\t*crypto provider: me*}}\n",
            "grad sum =  {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:53982067368 -> worker_0:17022078885]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79159898620 -> worker_1:21276186034]\n",
            "\t-> (Wrapper)>[PointerTensor | me:96824346209 -> worker_2:81273798099]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:51312233566 -> worker_0:19503383814]\n",
            "\t-> (Wrapper)>[PointerTensor | me:18284301541 -> worker_1:27573532470]\n",
            "\t-> (Wrapper)>[PointerTensor | me:94415234159 -> worker_2:75018985114]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:67355565800 -> worker_0:18395305800]\n",
            "\t-> (Wrapper)>[PointerTensor | me:23656026095 -> worker_1:55601602277]\n",
            "\t-> (Wrapper)>[PointerTensor | me:74156352522 -> worker_2:58444983949]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79560147672 -> worker_0:7582545889]\n",
            "\t-> (Wrapper)>[PointerTensor | me:92730692362 -> worker_1:23258394276]\n",
            "\t-> (Wrapper)>[PointerTensor | me:60647876365 -> worker_2:90707959807]\n",
            "\t*crypto provider: me*}\n",
            "calculating 0.weight\n",
            "tensor([[-1.5372e+15,  0.0000e+00,  1.5372e+15,  ..., -1.5372e+15,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.5372e+15,  1.0000e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
            "         -1.5372e+15,  1.0000e-03],\n",
            "        [-1.5372e+15,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "         -1.5372e+15,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  1.0000e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          1.5372e+15,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "         -1.5372e+15, -1.0000e-03],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "         -1.5372e+15,  0.0000e+00]])\n",
            "calculating 0.bias\n",
            "tensor([ 2.2081e+11,  1.5374e+15,  1.5331e+11,  4.6805e+11,  1.5375e+15,\n",
            "         3.3924e+11,  2.7518e+11,  1.6599e+11,  3.1819e+11,  2.6164e+11,\n",
            "         2.8225e+10,  1.4987e+10, -1.5370e+15, -1.5371e+15,  1.5372e+15,\n",
            "         1.5374e+15,  6.1704e+11,  9.3971e+10, -1.5371e+15,  1.3218e+11,\n",
            "         4.1017e+11,  1.6491e+11,  5.3252e+11,  7.5824e+10,  2.1398e+11,\n",
            "        -1.5369e+15, -1.5371e+15,  4.2481e+11,  1.5373e+15,  1.5376e+15,\n",
            "         9.6540e+10,  4.8260e+11,  1.5375e+15,  2.9185e+11,  6.6494e+10,\n",
            "         1.4176e+11, -1.5369e+15,  3.1573e+11, -1.5371e+15,  5.0323e+10,\n",
            "         7.4109e+10,  1.7616e+11,  2.7312e+11,  1.1977e+11,  3.3574e+11,\n",
            "         2.3962e+11,  1.0356e+10,  1.5379e+15,  8.8277e+10,  1.5846e+11])\n",
            "calculating 2.weight\n",
            "tensor([[-1.5526e+15, -1.3560e+15,  2.1726e+13, -1.4655e+15,  8.4258e+13,\n",
            "         -1.6908e+15,  1.6759e+15,  1.6200e+14,  1.5502e+14,  7.8282e+13,\n",
            "         -2.1023e+13,  3.6411e+12,  4.0051e+13,  2.4844e+13,  3.1445e+12,\n",
            "         -3.6545e+13,  7.7778e+13,  3.5696e+13, -1.5246e+15,  3.4325e+14,\n",
            "         -1.4469e+15,  2.8177e+13,  4.9133e+14, -1.3624e+15,  3.9617e+13,\n",
            "         -1.4795e+15,  1.6144e+15,  1.5634e+15, -1.8070e+13,  8.3450e+12,\n",
            "          7.3214e+13, -1.5358e+15, -1.4659e+15, -4.9850e+13, -1.4826e+15,\n",
            "          1.3710e+14, -4.4725e+12,  3.9646e+13,  9.0684e+13,  7.3038e+13,\n",
            "         -7.2280e+11,  1.5541e+14,  1.7201e+14, -2.7959e+12,  1.6783e+15,\n",
            "          1.2450e+14, -2.0216e+12,  1.3927e+14,  1.4894e+15,  7.0205e+13],\n",
            "        [ 5.7928e+13,  1.2640e+14,  1.1518e+14,  1.5891e+14,  1.3934e+14,\n",
            "          2.5141e+14, -1.4288e+15, -1.4802e+14, -1.4945e+15,  2.9060e+13,\n",
            "          1.5605e+15,  2.5819e+13,  1.0735e+14,  2.2207e+12,  7.2035e+12,\n",
            "          1.7433e+14,  2.0238e+14, -3.7662e+13,  9.5135e+13, -1.1606e+14,\n",
            "          6.9310e+12,  2.8753e+13,  2.7513e+13,  2.4913e+13, -1.4954e+15,\n",
            "          5.6984e+13,  1.0634e+14,  1.6469e+15, -9.4593e+11, -1.4859e+15,\n",
            "         -1.4750e+15,  1.5640e+15,  1.5738e+15, -1.3178e+15,  1.5704e+15,\n",
            "          7.8671e+13,  1.7644e+14,  3.8451e+13,  3.8136e+13,  1.5567e+15,\n",
            "          1.0514e+14,  9.3225e+13,  1.5554e+13,  1.2892e+13,  1.2320e+14,\n",
            "          5.3894e+12,  1.5416e+15, -1.2019e+15,  8.8349e+12,  1.8721e+14],\n",
            "        [ 1.0848e+14, -6.7045e+11, -6.5586e+13,  1.5245e+15,  1.7393e+15,\n",
            "          1.5685e+14,  1.4507e+13,  1.1655e+14, -5.9333e+13,  6.0374e+13,\n",
            "          1.3778e+13,  1.6244e+12,  4.5380e+13,  7.3195e+13,  4.2283e+12,\n",
            "          1.2718e+14, -1.7454e+14, -3.4545e+13,  3.0776e+14,  1.5413e+14,\n",
            "          4.5619e+13,  5.4728e+13,  4.4516e+13,  2.7007e+13,  1.4721e+14,\n",
            "         -8.2085e+12, -1.3614e+14,  1.8941e+13,  3.8277e+13,  1.0968e+14,\n",
            "          1.7446e+13,  4.1906e+13,  1.3353e+14, -6.0892e+13,  1.5498e+15,\n",
            "         -1.5227e+15,  2.7955e+14,  4.1419e+13, -1.4971e+15, -1.5354e+15,\n",
            "          2.1812e+13,  6.3039e+13, -1.4738e+15,  2.6594e+13, -1.4823e+15,\n",
            "          1.0333e+13, -2.8842e+12,  1.7187e+14,  1.5237e+15, -1.4669e+15],\n",
            "        [ 1.5318e+15,  3.7610e+13, -1.4793e+15,  6.9868e+13, -1.5138e+15,\n",
            "          1.1280e+14,  1.5323e+15,  1.8817e+14,  1.0393e+14, -1.3351e+13,\n",
            "          1.5399e+15, -1.5318e+15,  6.8188e+13,  8.9918e+13,  1.5339e+15,\n",
            "         -2.4471e+14,  1.3018e+14, -5.6960e+13,  1.5891e+15, -1.7233e+14,\n",
            "         -1.4308e+15, -2.2863e+12, -1.4507e+15,  4.1950e+13,  1.6061e+15,\n",
            "          1.5314e+15,  1.4120e+13, -1.5579e+15,  1.5271e+15,  2.8034e+13,\n",
            "          1.3113e+14,  1.7968e+14,  4.3931e+13, -1.6310e+15,  9.9157e+12,\n",
            "          4.4222e+13,  6.9619e+13, -1.3029e+15,  5.2431e+11,  2.2462e+13,\n",
            "          4.5425e+13, -1.7867e+13, -1.4812e+15,  1.4751e+13, -1.5725e+15,\n",
            "         -1.2441e+14,  1.6845e+12,  1.7470e+14,  1.8008e+14,  1.5012e+14],\n",
            "        [ 5.1371e+13,  1.5042e+14,  1.6303e+15,  4.5977e+14, -1.5334e+15,\n",
            "         -1.4931e+15,  2.7641e+14, -2.0735e+14, -1.2926e+15,  1.6304e+15,\n",
            "          9.4545e+12,  1.5382e+15,  1.3118e+14,  2.2768e+13, -4.2809e+13,\n",
            "          2.8897e+14,  7.7695e+14,  6.6098e+13, -1.1230e+11, -1.1491e+15,\n",
            "          4.5047e+14,  1.7377e+13,  2.0819e+15,  1.6587e+15,  3.7765e+13,\n",
            "          1.6112e+15, -1.4295e+15,  1.8393e+14,  8.0447e+13,  7.5981e+13,\n",
            "          5.6516e+13,  3.3555e+14,  2.3817e+13,  1.0640e+14,  7.9116e+13,\n",
            "          1.2529e+14,  2.4451e+13, -3.9850e+13, -1.3556e+15,  7.1645e+13,\n",
            "          1.5109e+15, -1.5161e+15,  2.6164e+14,  1.5748e+15,  1.1112e+14,\n",
            "          1.9181e+14, -1.3020e+12,  7.9471e+14,  1.6825e+15,  1.2254e+14],\n",
            "        [-7.0889e+13, -1.3881e+15, -3.1543e+12,  2.6977e+14,  9.8468e+13,\n",
            "         -2.3134e+14, -1.4167e+15, -5.2625e+13,  2.5711e+14,  6.2588e+12,\n",
            "          6.5547e+12, -6.5249e+12,  7.9123e+13,  8.6256e+12,  8.5304e+12,\n",
            "         -4.2597e+13,  5.9516e+14,  6.1443e+13,  1.7972e+13,  8.5793e+13,\n",
            "          2.7602e+14, -2.4615e+12,  2.8786e+14,  1.9623e+13,  1.6262e+13,\n",
            "          5.5719e+13, -4.4065e+13,  1.0707e+14,  2.6132e+13, -1.6758e+15,\n",
            "          9.2433e+13,  1.8039e+14,  1.5751e+15,  9.0486e+13,  2.1555e+13,\n",
            "          1.3751e+14, -7.2598e+12,  5.9006e+13,  1.0448e+14, -1.5121e+15,\n",
            "          1.5932e+14, -7.2944e+13,  8.0237e+13, -1.4377e+13,  8.6183e+13,\n",
            "          6.8930e+13,  2.9762e+11,  1.9446e+15,  1.4577e+15,  9.0272e+13],\n",
            "        [ 5.9884e+13, -7.9657e+13, -2.6710e+13,  8.1974e+12,  7.8226e+13,\n",
            "          1.3780e+15, -6.2362e+13,  4.7403e+13,  1.4922e+15,  5.4887e+13,\n",
            "         -1.0415e+13,  6.0605e+12, -7.2900e+13,  1.8934e+13,  1.9724e+13,\n",
            "          1.0680e+14,  1.5119e+15,  1.5378e+15,  4.2048e+13,  7.0538e+13,\n",
            "         -3.2204e+13,  1.8396e+13, -5.2804e+13,  5.7728e+13,  2.5569e+13,\n",
            "         -1.1990e+13,  1.6220e+15,  1.3295e+13,  6.1262e+12,  2.8624e+14,\n",
            "          4.3998e+13,  1.8135e+15,  1.6902e+13,  4.9208e+13, -4.0028e+11,\n",
            "          1.5741e+13,  1.1208e+14,  1.3287e+14,  2.5903e+13,  1.5490e+15,\n",
            "          2.2920e+13,  1.6467e+14, -6.1050e+13, -1.5463e+15,  2.9599e+13,\n",
            "         -3.8957e+13,  5.5935e+12, -1.5958e+15,  6.1544e+13, -1.0323e+14],\n",
            "        [ 2.1280e+14,  8.2321e+12,  1.1572e+14,  3.5605e+13,  1.5335e+15,\n",
            "         -6.4412e+13,  9.4426e+13,  1.9053e+14,  2.1290e+13,  2.3050e+13,\n",
            "         -4.4433e+12,  8.4838e+12,  1.5670e+15,  5.4966e+13,  4.3789e+13,\n",
            "          8.0681e+13, -4.2002e+13, -1.5240e+15,  1.5394e+15, -3.0605e+13,\n",
            "         -1.1329e+13,  1.5646e+13, -1.5998e+15,  2.2569e+13,  1.5134e+15,\n",
            "          2.5677e+13,  9.6206e+13,  4.7060e+13,  7.7480e+13,  3.0485e+14,\n",
            "          1.5486e+15, -1.5247e+15,  1.7544e+14, -1.5651e+15, -3.5487e+13,\n",
            "          3.7949e+13,  1.2395e+14, -1.3458e+15,  2.5517e+12,  1.5388e+15,\n",
            "          5.2083e+13,  1.5010e+14,  3.2931e+14,  3.3524e+13, -1.5341e+15,\n",
            "         -1.4091e+15,  1.5478e+15,  1.3553e+14, -1.4074e+15,  1.5731e+15],\n",
            "        [-8.8287e+12,  1.8015e+14, -1.4387e+15,  2.1579e+14,  1.0819e+14,\n",
            "          1.5426e+15, -1.4120e+15, -6.2099e+13,  1.7797e+15,  4.1005e+13,\n",
            "          1.7356e+13,  1.5579e+15, -1.4638e+15, -1.5020e+15,  1.9519e+13,\n",
            "          8.9629e+13,  4.7210e+14,  2.0489e+13,  1.0008e+14,  1.3986e+15,\n",
            "         -1.3099e+15,  1.8214e+13,  1.8258e+14,  6.9182e+13, -1.1032e+13,\n",
            "          5.2830e+13,  1.2477e+14,  1.6941e+15, -1.2487e+13, -2.8591e+13,\n",
            "         -1.5532e+15, -6.4857e+13,  2.2160e+14, -6.6916e+12,  2.8494e+13,\n",
            "         -1.4599e+15, -1.4032e+15,  1.3972e+13,  2.1430e+13,  6.0659e+13,\n",
            "          1.6625e+15, -1.4559e+15, -1.1595e+13,  2.8376e+13,  1.8218e+14,\n",
            "          2.1346e+12,  2.7458e+12,  7.2507e+14, -5.3844e+13, -1.7695e+13],\n",
            "        [ 1.6441e+15,  1.2945e+14,  8.5463e+13,  5.5945e+13,  9.3841e+13,\n",
            "          4.7698e+12,  1.1229e+14,  2.6434e+13,  1.3652e+14, -1.4883e+13,\n",
            "          3.6026e+12,  4.1453e+13,  7.4381e+13, -7.3049e+12, -1.5278e+15,\n",
            "         -1.5197e+15,  1.3131e+14,  5.4952e+12,  9.0439e+13, -1.0236e+14,\n",
            "          1.2348e+13,  1.5458e+15, -3.5596e+13, -6.5443e+13,  9.3772e+11,\n",
            "          1.3908e+13,  9.0376e+13,  3.7005e+13,  1.3921e+14,  1.7981e+14,\n",
            "         -7.1170e+12,  1.8373e+13,  1.6394e+15,  1.5391e+15,  1.5468e+15,\n",
            "          9.1916e+12,  1.4759e+15,  1.3580e+14,  1.9788e+12,  1.5389e+15,\n",
            "          1.5061e+15,  1.5730e+14,  1.7090e+15, -1.5287e+15,  1.2169e+14,\n",
            "         -8.0685e+13, -1.5390e+15, -1.6078e+15,  1.4799e+15,  1.3005e+13]])\n",
            "calculating 2.bias\n",
            "tensor([7.1000e-02, 9.7000e-02, 1.2800e-01, 1.1500e-01, 4.2000e-02, 4.4000e-02,\n",
            "        1.5372e+15, 1.4500e-01, 1.0700e-01, 6.8000e-02])\n",
            "Test Accuracy: 0.134100\n",
            "Avg loss (8/0): 7993464742956756799609372672.000000\n",
            "Avg loss (8/1): 6001057593423138733261062144.000000\n",
            "Avg loss (8/2): 3866160413966558606410121216.000000\n",
            "Avg loss (8/3): 3093895294740195480856166400.000000\n",
            "Avg loss (8/4): 3120940877883400297700655104.000000\n",
            "Avg loss (8/5): 3286100332959472910698807296.000000\n",
            "Avg loss (8/6): 3344862509993250982209781760.000000\n",
            "Avg loss (8/7): 3207784312059657535122046976.000000\n",
            "Avg loss (8/8): 2880558847101697106093539328.000000\n",
            "Avg loss (8/9): 2444543192460200532542226432.000000\n",
            "Avg loss (8/10): 2030994934230719244477136896.000000\n",
            "Avg loss (8/11): 1752138455547503746226323456.000000\n",
            "Avg loss (8/12): 1614181602300191365776015360.000000\n",
            "Avg loss (8/13): 1540454098308246798906425344.000000\n",
            "Avg loss (8/14): 1453097254350930077244456960.000000\n",
            "Avg loss (8/15): 1323853019690559602537005056.000000\n",
            "Avg loss (8/16): 1163207999398157780686733312.000000\n",
            "Avg loss (8/17): 999645073128176581961842688.000000\n",
            "Avg loss (8/18): 857514238205817793848279040.000000\n",
            "Avg loss (8/19): 749078373582449462392913920.000000\n",
            "sharing worker_0 model\n",
            "sharing worker_1 model\n",
            "sharing worker_2 model\n",
            "{'worker_0': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95468328609 -> worker_0:95610817311]\n",
            "\t-> (Wrapper)>[PointerTensor | me:63231461847 -> worker_1:32453551356]\n",
            "\t-> (Wrapper)>[PointerTensor | me:89553149115 -> worker_2:33494707343]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:10112112072 -> worker_0:746970464]\n",
            "\t-> (Wrapper)>[PointerTensor | me:18899144938 -> worker_1:15056537514]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95163287536 -> worker_2:26601033028]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:18445616351 -> worker_0:21769169884]\n",
            "\t-> (Wrapper)>[PointerTensor | me:54193780309 -> worker_1:241876665]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79234850402 -> worker_2:34786558652]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:29766607443 -> worker_0:11854969697]\n",
            "\t-> (Wrapper)>[PointerTensor | me:63850390026 -> worker_1:69254123124]\n",
            "\t-> (Wrapper)>[PointerTensor | me:32366356858 -> worker_2:66946158396]\n",
            "\t*crypto provider: me*}, 'worker_1': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:8061091997 -> worker_0:84382656105]\n",
            "\t-> (Wrapper)>[PointerTensor | me:4514194478 -> worker_1:48363561995]\n",
            "\t-> (Wrapper)>[PointerTensor | me:47019871168 -> worker_2:74501532379]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:80959577184 -> worker_0:72695882498]\n",
            "\t-> (Wrapper)>[PointerTensor | me:84762302048 -> worker_1:53799364755]\n",
            "\t-> (Wrapper)>[PointerTensor | me:66460985426 -> worker_2:28420400987]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:96300649386 -> worker_0:23126276595]\n",
            "\t-> (Wrapper)>[PointerTensor | me:77064249649 -> worker_1:73144838042]\n",
            "\t-> (Wrapper)>[PointerTensor | me:50665211763 -> worker_2:52208640359]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:59042632169 -> worker_0:62157307270]\n",
            "\t-> (Wrapper)>[PointerTensor | me:86638014907 -> worker_1:20927768546]\n",
            "\t-> (Wrapper)>[PointerTensor | me:186176831 -> worker_2:13504013851]\n",
            "\t*crypto provider: me*}, 'worker_2': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:37797008340 -> worker_0:18248593829]\n",
            "\t-> (Wrapper)>[PointerTensor | me:16770548881 -> worker_1:84183164850]\n",
            "\t-> (Wrapper)>[PointerTensor | me:34784131929 -> worker_2:23661035292]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:36891539384 -> worker_0:91455027107]\n",
            "\t-> (Wrapper)>[PointerTensor | me:56130628218 -> worker_1:69370985688]\n",
            "\t-> (Wrapper)>[PointerTensor | me:81972243978 -> worker_2:64046659992]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:14937329927 -> worker_0:19602507543]\n",
            "\t-> (Wrapper)>[PointerTensor | me:37266008746 -> worker_1:71455688935]\n",
            "\t-> (Wrapper)>[PointerTensor | me:82770138588 -> worker_2:61120495451]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:54035280727 -> worker_0:35268966564]\n",
            "\t-> (Wrapper)>[PointerTensor | me:27223536760 -> worker_1:40876077352]\n",
            "\t-> (Wrapper)>[PointerTensor | me:9511094859 -> worker_2:27842869505]\n",
            "\t*crypto provider: me*}}\n",
            "grad sum =  {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95468328609 -> worker_0:95610817311]\n",
            "\t-> (Wrapper)>[PointerTensor | me:63231461847 -> worker_1:32453551356]\n",
            "\t-> (Wrapper)>[PointerTensor | me:89553149115 -> worker_2:33494707343]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:10112112072 -> worker_0:746970464]\n",
            "\t-> (Wrapper)>[PointerTensor | me:18899144938 -> worker_1:15056537514]\n",
            "\t-> (Wrapper)>[PointerTensor | me:95163287536 -> worker_2:26601033028]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:18445616351 -> worker_0:21769169884]\n",
            "\t-> (Wrapper)>[PointerTensor | me:54193780309 -> worker_1:241876665]\n",
            "\t-> (Wrapper)>[PointerTensor | me:79234850402 -> worker_2:34786558652]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:29766607443 -> worker_0:11854969697]\n",
            "\t-> (Wrapper)>[PointerTensor | me:63850390026 -> worker_1:69254123124]\n",
            "\t-> (Wrapper)>[PointerTensor | me:32366356858 -> worker_2:66946158396]\n",
            "\t*crypto provider: me*}\n",
            "calculating 0.weight\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          1.0000e-03,  0.0000e+00],\n",
            "        [-1.0000e-03, -1.5372e+15,  0.0000e+00,  ...,  0.0000e+00,\n",
            "         -1.0000e-03, -1.0000e-03],\n",
            "        [ 1.0000e-03, -1.5372e+15,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  1.5372e+15],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.5372e+15,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  1.5372e+15,  ...,  0.0000e+00,\n",
            "          1.0000e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.5372e+15, -1.5372e+15,  ...,  1.0000e-03,\n",
            "          1.0000e-03,  1.5372e+15]])\n",
            "calculating 0.bias\n",
            "tensor([ 1.3900e+11, -1.5371e+15,  1.5375e+15,  3.4744e+11, -1.5371e+15,\n",
            "        -1.5370e+15,  1.5375e+15,  5.4586e+10,  3.5391e+11,  1.5375e+15,\n",
            "         9.0317e+09,  1.9031e+10,  2.3810e+11,  1.1950e+11, -1.5372e+15,\n",
            "        -1.5371e+15,  4.3094e+11,  8.2468e+10,  1.0797e+11,  3.9730e+11,\n",
            "         3.9714e+11,  1.3638e+11,  3.9794e+11,  1.5373e+15, -1.5369e+15,\n",
            "         3.0001e+11,  7.5468e+10,  1.2576e+11,  1.5373e+15,  3.7777e+11,\n",
            "         2.0369e+10,  4.9535e+11, -1.5369e+15,  5.6184e+10,  1.5374e+15,\n",
            "         1.5374e+15,  2.0996e+11, -1.5370e+15,  9.4812e+10,  1.0700e+11,\n",
            "        -1.5372e+15,  1.7222e+11,  2.7657e+11,  1.6013e+11,  2.9863e+11,\n",
            "         2.1206e+11, -4.8211e+09,  5.0327e+11,  2.7800e+11,  8.2532e+10])\n",
            "calculating 2.weight\n",
            "tensor([[-2.9703e+13, -1.2915e+15,  1.5351e+15,  1.1456e+14,  3.6969e+12,\n",
            "          1.6567e+15,  2.8983e+13,  1.4091e+14,  1.1282e+14,  8.9541e+13,\n",
            "         -2.3343e+13, -1.3715e+13,  7.1275e+13,  2.3631e+13,  1.5525e+15,\n",
            "         -6.3548e+13, -1.5158e+13,  3.1974e+13,  7.7320e+13,  3.0801e+14,\n",
            "         -1.4563e+15,  1.5539e+15,  1.9733e+15,  2.0486e+14,  1.5303e+15,\n",
            "          6.4931e+13,  8.0526e+13, -1.5303e+15, -4.6083e+13,  2.6869e+13,\n",
            "          4.8095e+13, -1.4821e+15,  3.4020e+14, -5.7772e+12,  1.2858e+14,\n",
            "         -1.4722e+15, -3.9881e+11,  2.0767e+13,  2.9337e+13,  5.7705e+13,\n",
            "         -1.5432e+15,  1.7199e+14,  1.7606e+14,  5.5360e+12,  1.2153e+14,\n",
            "          1.3569e+14,  5.8155e+12, -1.0766e+14, -1.9402e+14, -1.4604e+15],\n",
            "        [ 1.1567e+14,  9.9668e+13,  1.0071e+14, -1.3939e+15,  1.1766e+14,\n",
            "          1.6657e+15, -1.3689e+15, -7.6883e+13,  6.5063e+13, -1.5127e+15,\n",
            "          9.9313e+12,  1.3030e+13,  1.0941e+14,  1.3666e+13, -5.1792e+11,\n",
            "          1.5100e+14,  1.1041e+14, -5.9048e+13, -1.5279e+15, -5.4077e+13,\n",
            "         -1.4027e+12,  1.5555e+15, -1.5090e+15,  1.2086e+13,  9.2581e+13,\n",
            "         -1.4804e+15,  7.4584e+13,  6.4918e+13,  8.3468e+13,  1.3221e+14,\n",
            "          5.1704e+13, -6.3164e+13, -1.4014e+13,  2.5824e+14,  1.4598e+13,\n",
            "          4.7667e+13,  8.3447e+13,  1.5353e+15, -1.5349e+15, -3.4171e+13,\n",
            "          2.1808e+13,  6.7307e+13,  1.5928e+15,  1.5557e+15,  1.1342e+14,\n",
            "          2.7528e+13,  3.1557e+12,  3.2979e+14,  2.6350e+13,  7.2227e+13],\n",
            "        [ 1.5147e+14, -2.7480e+13, -1.5059e+15, -5.6407e+12,  1.0342e+14,\n",
            "         -1.3928e+15,  3.2293e+13,  1.5869e+15, -9.4139e+13,  6.0808e+13,\n",
            "          7.3589e+12,  1.5615e+15,  8.6839e+13,  6.1741e+13,  4.1061e+12,\n",
            "          1.6770e+15, -1.9459e+14, -1.5680e+15,  1.6462e+15,  1.5151e+14,\n",
            "          4.7783e+13,  3.5489e+13, -1.4568e+15,  1.5846e+13,  3.1736e+14,\n",
            "         -1.5311e+15, -8.8692e+13,  2.7357e+13,  9.8360e+13,  8.0008e+13,\n",
            "         -1.5483e+15,  1.4740e+14,  3.0706e+14,  5.8733e+12,  1.2508e+13,\n",
            "          4.3538e+13,  1.6079e+14,  3.2767e+13,  8.3439e+13,  5.4234e+13,\n",
            "          1.2698e+13,  7.4779e+13,  3.4095e+14,  1.6052e+15, -1.4440e+15,\n",
            "          5.6716e+12, -1.0076e+12,  1.0337e+14, -5.3482e+13,  1.6812e+15],\n",
            "        [-5.7887e+13,  4.6916e+13,  1.6205e+14,  5.6037e+13,  5.5034e+13,\n",
            "          5.0747e+13,  1.8682e+13,  1.3520e+14,  1.5568e+14, -1.0464e+13,\n",
            "         -5.8056e+10,  1.0909e+13,  8.2489e+13,  4.5684e+13, -1.1396e+12,\n",
            "          1.3185e+15,  1.3835e+14, -1.4191e+13, -9.8958e+12, -2.2356e+14,\n",
            "         -1.3904e+15,  4.9323e+12, -1.3350e+15,  2.6195e+13,  1.1987e+14,\n",
            "         -1.2227e+13,  9.1432e+13,  1.5408e+15, -6.8488e+13, -1.5181e+15,\n",
            "          1.0357e+14,  1.4166e+14,  1.0118e+14,  2.7282e+13,  2.9180e+13,\n",
            "          4.1958e+13,  1.5709e+15,  1.0621e+14,  3.6195e+13,  3.0218e+13,\n",
            "         -1.5336e+15,  1.5458e+15,  1.2583e+14,  2.4008e+13, -1.5595e+15,\n",
            "         -8.5391e+13,  2.7176e+12,  1.7019e+15,  2.2957e+13,  1.1082e+14],\n",
            "        [ 1.3727e+13,  2.0169e+14,  6.7850e+13, -1.0972e+15,  4.7690e+13,\n",
            "          3.7759e+13,  3.3384e+14,  1.4198e+13,  3.0613e+14,  7.1576e+13,\n",
            "          1.5440e+15, -1.5203e+15,  1.1619e+14,  6.5754e+12, -3.3781e+13,\n",
            "          1.9167e+14, -1.0019e+15,  2.3304e+13,  1.5562e+15,  4.8361e+14,\n",
            "          3.6029e+14, -1.5287e+15,  3.5912e+14,  1.6291e+15,  8.5498e+13,\n",
            "          5.0450e+13,  1.4132e+14,  6.0542e+13,  4.1724e+13,  2.6931e+13,\n",
            "         -1.5312e+15,  2.9348e+14,  2.9193e+13,  1.5064e+15, -1.3996e+15,\n",
            "          3.4698e+13,  2.5266e+13,  8.6477e+12,  7.3231e+13, -1.4516e+15,\n",
            "          3.4316e+11,  1.4500e+14,  1.9321e+14, -1.5020e+15,  7.4610e+13,\n",
            "          1.1516e+14, -4.7416e+12,  4.6799e+14,  1.6129e+15, -1.5149e+15],\n",
            "        [-9.7348e+13,  1.4604e+14,  1.5361e+15,  1.8146e+14,  7.1619e+13,\n",
            "         -1.6094e+15,  1.4562e+14, -8.7568e+12, -1.2611e+15, -2.5420e+13,\n",
            "          1.5384e+15,  1.5177e+15,  7.4104e+13,  1.2657e+13, -1.5263e+15,\n",
            "          1.4374e+15, -1.1488e+15,  1.6105e+15,  1.3552e+13, -1.5149e+15,\n",
            "          1.3065e+14, -5.6135e+12,  1.7087e+15,  2.0511e+13,  1.5839e+15,\n",
            "         -1.5175e+15,  3.0117e+13,  3.3490e+13,  3.6280e+13, -1.5143e+15,\n",
            "          7.5466e+13,  1.6006e+15, -1.3902e+13,  1.1487e+12,  5.2817e+13,\n",
            "          7.2804e+13, -1.6168e+12,  2.8887e+13, -1.5165e+15,  3.3714e+13,\n",
            "          1.6341e+15, -1.4920e+13,  6.6291e+13, -1.7787e+13,  1.5787e+15,\n",
            "          2.3090e+13,  2.0976e+11,  7.9108e+13, -1.6607e+15,  4.3155e+13],\n",
            "        [ 1.6324e+15, -9.1122e+13, -1.5535e+15, -2.1031e+13,  1.5889e+15,\n",
            "         -8.6193e+13, -7.0784e+13, -4.6611e+13, -1.6366e+15, -1.4675e+15,\n",
            "         -5.6300e+12, -7.9975e+11, -7.7081e+13, -1.5257e+15,  1.5034e+13,\n",
            "          1.1029e+14, -1.5796e+15, -1.5593e+15,  5.8700e+12,  1.1569e+13,\n",
            "          1.5095e+15,  8.7792e+12, -1.1096e+14,  5.0914e+13,  9.1492e+13,\n",
            "         -9.3193e+12, -4.7893e+13, -1.5282e+15,  1.9453e+13,  2.3463e+14,\n",
            "          2.3550e+13,  1.8253e+15,  3.8704e+13,  7.4978e+13,  7.6799e+11,\n",
            "          1.0368e+13,  5.1494e+13,  1.6883e+13,  1.0675e+13,  1.5083e+12,\n",
            "          1.5470e+15,  1.0296e+14, -6.2845e+13,  1.5364e+15,  7.6844e+13,\n",
            "         -4.3374e+13, -1.5331e+15,  1.6153e+14, -2.2013e+12, -6.7296e+13],\n",
            "        [-1.2797e+15,  1.5244e+15,  1.0017e+14,  1.0562e+14, -9.9124e+12,\n",
            "         -2.1136e+13,  1.3410e+14,  1.5401e+15,  4.2476e+13,  5.9501e+13,\n",
            "          3.1871e+11,  1.5996e+13,  3.8160e+13,  4.2484e+13,  2.5148e+13,\n",
            "          6.5989e+13,  1.4675e+15,  1.9304e+13, -1.0036e+14, -2.8923e+14,\n",
            "          5.1531e+13, -1.5241e+15,  6.3216e+13,  4.1492e+13,  1.4523e+15,\n",
            "          4.6811e+13,  6.4682e+13,  4.6857e+13,  6.6623e+13, -1.3164e+15,\n",
            "          2.2830e+12,  1.4144e+14, -1.3621e+15, -2.0678e+12, -1.5745e+15,\n",
            "          2.9350e+13,  5.4044e+13,  5.4068e+13, -1.5264e+15, -2.4049e+13,\n",
            "         -1.7839e+13, -1.4521e+15,  2.2080e+14,  5.1838e+13, -1.5037e+15,\n",
            "          1.4929e+14,  6.3961e+12, -1.3543e+15,  8.7256e+13, -3.6884e+13],\n",
            "        [ 2.8722e+13,  1.2061e+14,  1.7756e+14,  1.4537e+14,  9.4078e+13,\n",
            "          1.9732e+13, -1.3280e+15, -1.5010e+15,  1.4160e+14,  3.4798e+13,\n",
            "          1.5446e+15,  2.8770e+13,  1.2108e+14,  3.3837e+13, -1.5240e+15,\n",
            "         -1.4676e+15,  2.7201e+14, -1.5037e+15,  4.4416e+13, -2.3753e+14,\n",
            "          1.9316e+14,  1.2455e+13,  1.0574e+14,  4.9969e+13,  1.4413e+15,\n",
            "          3.9180e+13,  1.6752e+14,  7.5491e+13,  2.2789e+13,  2.1641e+13,\n",
            "          3.7124e+13, -2.0608e+14,  3.0008e+14,  1.0543e+13,  1.5930e+15,\n",
            "         -1.4787e+15,  1.3820e+14,  1.9120e+13,  3.4260e+12,  4.5193e+13,\n",
            "          1.6529e+13,  1.7969e+14,  1.2386e+13,  5.2698e+13,  1.5405e+14,\n",
            "         -1.2756e+13,  3.8410e+12,  3.6600e+14, -2.1720e+13,  4.7398e+13],\n",
            "        [ 3.5946e+13,  6.3162e+13,  7.5809e+13,  3.6602e+13,  1.2306e+14,\n",
            "          1.9124e+13,  1.6803e+15, -3.4136e+13, -1.3482e+15,  1.5117e+15,\n",
            "          1.5400e+15,  6.3291e+13,  5.7406e+13, -2.5096e+13,  1.0915e+13,\n",
            "          2.7810e+13,  1.6563e+15, -1.5309e+15,  2.2143e+12, -3.3813e+14,\n",
            "          4.8091e+12, -1.5422e+15, -6.7371e+13, -8.8785e+13,  1.4874e+13,\n",
            "          1.0189e+13, -1.4739e+15, -1.5137e+15,  2.1847e+14,  1.6996e+15,\n",
            "          1.9530e+13, -6.2695e+13,  7.0976e+13,  5.1540e+12, -1.5543e+15,\n",
            "         -1.0927e+12,  1.5307e+15,  4.7761e+13, -1.4251e+13, -2.2591e+13,\n",
            "         -1.3162e+13,  1.1640e+14,  3.1115e+13,  3.3856e+13,  1.2985e+14,\n",
            "         -8.6705e+13,  4.5699e+12,  4.7836e+13, -1.7074e+14,  1.5702e+15]])\n",
            "calculating 2.bias\n",
            "tensor([-1.5372e+15,  1.2700e-01,  2.0100e-01,  1.5372e+15,  2.4000e-02,\n",
            "         6.7000e-02,  1.1400e-01,  1.0500e-01,  1.2700e-01, -1.5372e+15])\n",
            "Test Accuracy: 0.134400\n",
            "Avg loss (9/0): 9989355228067301588008435712.000000\n",
            "Avg loss (9/1): 6988408206774093078423666688.000000\n",
            "Avg loss (9/2): 4271085335850414742962176000.000000\n",
            "Avg loss (9/3): 3622613512197617214451351552.000000\n",
            "Avg loss (9/4): 3743760806801249913410682880.000000\n",
            "Avg loss (9/5): 3952300510684773446047498240.000000\n",
            "Avg loss (9/6): 4041584522888958751985893376.000000\n",
            "Avg loss (9/7): 3922614829529739318175727616.000000\n",
            "Avg loss (9/8): 3586642951549693947505803264.000000\n",
            "Avg loss (9/9): 3092248959725105050793541632.000000\n",
            "Avg loss (9/10): 2549499115707551704504336384.000000\n",
            "Avg loss (9/11): 2087572426470359746371125248.000000\n",
            "Avg loss (9/12): 1765441804651605305822543872.000000\n",
            "Avg loss (9/13): 1564070948236983073972944896.000000\n",
            "Avg loss (9/14): 1437802247182678244779360256.000000\n",
            "Avg loss (9/15): 1358218270882212369462722560.000000\n",
            "Avg loss (9/16): 1286059035284438293833318400.000000\n",
            "Avg loss (9/17): 1192076637364679165098328064.000000\n",
            "Avg loss (9/18): 1068357202554384701099868160.000000\n",
            "Avg loss (9/19): 926656471916848524891783168.000000\n",
            "sharing worker_0 model\n",
            "sharing worker_1 model\n",
            "sharing worker_2 model\n",
            "{'worker_0': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:35179221843 -> worker_0:87590191403]\n",
            "\t-> (Wrapper)>[PointerTensor | me:46637636751 -> worker_1:82015357768]\n",
            "\t-> (Wrapper)>[PointerTensor | me:10073048997 -> worker_2:46746348575]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:37363108397 -> worker_0:58487656618]\n",
            "\t-> (Wrapper)>[PointerTensor | me:14420347808 -> worker_1:82809876132]\n",
            "\t-> (Wrapper)>[PointerTensor | me:42117023095 -> worker_2:41765608036]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:4608133743 -> worker_0:45445342392]\n",
            "\t-> (Wrapper)>[PointerTensor | me:77981175749 -> worker_1:31456808252]\n",
            "\t-> (Wrapper)>[PointerTensor | me:62266954606 -> worker_2:77175433581]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:72968574416 -> worker_0:78385328238]\n",
            "\t-> (Wrapper)>[PointerTensor | me:90911074281 -> worker_1:54513385449]\n",
            "\t-> (Wrapper)>[PointerTensor | me:64756550354 -> worker_2:92275896076]\n",
            "\t*crypto provider: me*}, 'worker_1': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:11639253247 -> worker_0:27197449714]\n",
            "\t-> (Wrapper)>[PointerTensor | me:26200613726 -> worker_1:95492906507]\n",
            "\t-> (Wrapper)>[PointerTensor | me:97764191984 -> worker_2:83571541689]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:13020990539 -> worker_0:53410308537]\n",
            "\t-> (Wrapper)>[PointerTensor | me:26451163277 -> worker_1:78760252013]\n",
            "\t-> (Wrapper)>[PointerTensor | me:48485447258 -> worker_2:69571654499]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:59820144134 -> worker_0:60939260550]\n",
            "\t-> (Wrapper)>[PointerTensor | me:10887450769 -> worker_1:96175086652]\n",
            "\t-> (Wrapper)>[PointerTensor | me:52867816052 -> worker_2:91735642220]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:60002804812 -> worker_0:5932375139]\n",
            "\t-> (Wrapper)>[PointerTensor | me:59715767162 -> worker_1:17459426328]\n",
            "\t-> (Wrapper)>[PointerTensor | me:97985255087 -> worker_2:16972841390]\n",
            "\t*crypto provider: me*}, 'worker_2': {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:64403836754 -> worker_0:72324593641]\n",
            "\t-> (Wrapper)>[PointerTensor | me:11765324834 -> worker_1:89961642983]\n",
            "\t-> (Wrapper)>[PointerTensor | me:10337745138 -> worker_2:59332597692]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:44145748592 -> worker_0:91264157651]\n",
            "\t-> (Wrapper)>[PointerTensor | me:94777150945 -> worker_1:84739174327]\n",
            "\t-> (Wrapper)>[PointerTensor | me:32691625928 -> worker_2:38886819844]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:67770572396 -> worker_0:59136949359]\n",
            "\t-> (Wrapper)>[PointerTensor | me:59550561051 -> worker_1:38531208651]\n",
            "\t-> (Wrapper)>[PointerTensor | me:92700767495 -> worker_2:77498547504]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:7618176515 -> worker_0:10103974324]\n",
            "\t-> (Wrapper)>[PointerTensor | me:14535853336 -> worker_1:71895230436]\n",
            "\t-> (Wrapper)>[PointerTensor | me:13654029384 -> worker_2:92783061870]\n",
            "\t*crypto provider: me*}}\n",
            "grad sum =  {'0.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:35179221843 -> worker_0:87590191403]\n",
            "\t-> (Wrapper)>[PointerTensor | me:46637636751 -> worker_1:82015357768]\n",
            "\t-> (Wrapper)>[PointerTensor | me:10073048997 -> worker_2:46746348575]\n",
            "\t*crypto provider: me*, '0.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:37363108397 -> worker_0:58487656618]\n",
            "\t-> (Wrapper)>[PointerTensor | me:14420347808 -> worker_1:82809876132]\n",
            "\t-> (Wrapper)>[PointerTensor | me:42117023095 -> worker_2:41765608036]\n",
            "\t*crypto provider: me*, '2.weight': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:4608133743 -> worker_0:45445342392]\n",
            "\t-> (Wrapper)>[PointerTensor | me:77981175749 -> worker_1:31456808252]\n",
            "\t-> (Wrapper)>[PointerTensor | me:62266954606 -> worker_2:77175433581]\n",
            "\t*crypto provider: me*, '2.bias': (Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
            "\t-> (Wrapper)>[PointerTensor | me:72968574416 -> worker_0:78385328238]\n",
            "\t-> (Wrapper)>[PointerTensor | me:90911074281 -> worker_1:54513385449]\n",
            "\t-> (Wrapper)>[PointerTensor | me:64756550354 -> worker_2:92275896076]\n",
            "\t*crypto provider: me*}\n",
            "calculating 0.weight\n",
            "tensor([[ 1.0000e-03,  1.0000e-03,  0.0000e+00,  ..., -1.0000e-03,\n",
            "          0.0000e+00, -1.5372e+15],\n",
            "        [ 0.0000e+00,  1.5372e+15, -1.0000e-03,  ...,  1.5372e+15,\n",
            "          0.0000e+00, -1.0000e-03],\n",
            "        [ 0.0000e+00,  1.5372e+15,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          1.0000e-03,  0.0000e+00],\n",
            "        ...,\n",
            "        [-1.0000e-03,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.5372e+15,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          1.5372e+15,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00, -1.0000e-03,  ...,  1.5372e+15,\n",
            "          0.0000e+00,  1.0000e-03]])\n",
            "calculating 0.bias\n",
            "tensor([-1.5370e+15,  2.4089e+11,  3.0785e+11,  3.9816e+11,  9.3746e+10,\n",
            "         1.1010e+11,  3.8568e+11,  9.4379e+10,  5.5584e+11, -1.5369e+15,\n",
            "        -1.5372e+15,  3.4283e+10,  1.5375e+15,  1.6811e+11, -1.5372e+15,\n",
            "         1.7587e+11,  5.2942e+11,  6.9758e+10,  1.9499e+10,  4.1827e+11,\n",
            "         4.3234e+11,  1.8582e+11, -1.5368e+15,  1.5376e+15,  1.5377e+15,\n",
            "         2.4094e+11,  1.5373e+15,  2.3481e+11, -2.9396e+09,  1.2373e+11,\n",
            "         6.1384e+10,  4.3166e+11,  3.0869e+11,  2.3367e+11, -1.5371e+15,\n",
            "         1.9644e+11,  1.8078e+11,  2.4937e+11,  1.0235e+11,  2.8870e+11,\n",
            "         2.4980e+10,  1.9561e+11,  3.5112e+11,  1.5373e+15,  2.8319e+11,\n",
            "         3.6226e+11,  1.7044e+10,  1.5380e+15,  3.8275e+11,  7.0955e+10])\n",
            "calculating 2.weight\n",
            "tensor([[ 1.5599e+13,  1.8487e+14,  1.4532e+15,  8.6356e+13, -4.0299e+13,\n",
            "         -1.4808e+15,  9.4807e+13,  9.3870e+13,  1.4878e+14,  1.0062e+14,\n",
            "         -2.2537e+13, -2.4001e+13,  7.6898e+13,  6.7488e+13,  1.5472e+15,\n",
            "         -1.4877e+14, -1.5846e+15,  3.9010e+13,  2.0746e+13,  2.4130e+14,\n",
            "          1.1845e+14,  2.7849e+13,  3.2117e+14,  1.0046e+14, -1.0416e+14,\n",
            "          4.7479e+13,  5.4668e+13,  1.9181e+13, -1.1238e+14,  4.3734e+13,\n",
            "          1.6096e+15, -1.5893e+15,  2.7537e+14, -6.5155e+13, -1.4633e+15,\n",
            "          2.5218e+13, -1.5364e+15,  5.0817e+13,  2.2553e+13,  1.6499e+15,\n",
            "          1.5227e+15,  2.0457e+14,  1.0746e+14, -6.0899e+12,  1.5904e+15,\n",
            "          1.8270e+14,  1.1721e+13, -1.9261e+14, -2.1367e+14, -1.4484e+15],\n",
            "        [ 5.5368e+13,  7.0548e+13, -1.3578e+15,  1.6696e+15,  1.6127e+15,\n",
            "          3.0895e+13,  1.9084e+14, -1.1540e+13,  1.2985e+14,  4.4662e+13,\n",
            "          1.1562e+13, -4.1142e+11,  1.5967e+14,  4.1882e+13,  6.0083e+11,\n",
            "         -1.3851e+15, -1.4499e+15, -1.0849e+14,  1.7597e+13, -1.6017e+15,\n",
            "          1.5616e+15,  2.5661e+13, -2.4892e+13,  4.8039e+13, -1.4878e+15,\n",
            "          4.4284e+13,  6.4775e+13, -1.4463e+15,  1.0885e+14, -1.4718e+15,\n",
            "          1.6015e+15, -2.0943e+13,  3.9114e+13,  2.1959e+14, -1.5293e+15,\n",
            "          4.2228e+12,  1.2147e+13, -2.3791e+13,  1.7891e+13,  1.3395e+13,\n",
            "          1.0845e+14,  3.9675e+13,  1.3985e+15,  1.0457e+13, -1.4751e+15,\n",
            "         -3.1173e+13,  9.3382e+12,  1.5761e+14,  2.7180e+13,  5.7476e+13],\n",
            "        [ 6.0280e+13, -7.4945e+13,  6.1894e+13, -7.2545e+11,  6.7682e+13,\n",
            "          7.5558e+13,  4.9067e+13, -7.5258e+13, -1.1069e+14, -1.4819e+15,\n",
            "         -1.5282e+15,  1.5623e+15,  7.9765e+13, -1.4679e+15,  1.3324e+11,\n",
            "          3.2777e+13,  1.3040e+15,  1.5604e+13,  2.2089e+13,  1.1527e+14,\n",
            "          9.4360e+12, -1.4794e+15,  1.3044e+14,  5.5969e+13, -1.1980e+15,\n",
            "          1.0691e+13, -6.7766e+13,  2.8674e+11,  7.9546e+13,  2.9649e+13,\n",
            "          2.9440e+13,  1.6055e+15,  1.7361e+15, -2.8753e+12,  4.4532e+11,\n",
            "          6.4629e+13,  1.2529e+14,  2.7767e+13,  6.2236e+13,  3.5360e+13,\n",
            "          4.2069e+13,  6.0428e+13,  1.7787e+15,  1.4853e+13,  6.4146e+13,\n",
            "         -2.1126e+13, -2.2492e+12, -4.2455e+12, -7.7425e+13, -2.4495e+12],\n",
            "        [-1.6068e+13,  6.6719e+13, -1.3058e+15, -5.0030e+12,  1.5665e+15,\n",
            "         -1.1087e+11, -2.3362e+13,  1.6655e+15,  1.4509e+14, -1.5906e+13,\n",
            "         -4.5221e+12,  1.5535e+15,  1.2409e+14,  6.0773e+13,  1.5311e+15,\n",
            "         -2.6759e+14,  1.2894e+14, -2.0090e+13,  1.5339e+15, -1.1775e+14,\n",
            "          1.0825e+14, -1.5254e+15,  3.3779e+14,  4.2969e+13,  1.9450e+14,\n",
            "         -1.5755e+13,  9.8822e+13, -4.1601e+13, -1.1353e+14,  2.0379e+13,\n",
            "          7.8377e+13,  1.5711e+14, -1.4177e+13,  1.8352e+12,  3.7246e+13,\n",
            "          2.5165e+13,  1.0882e+13,  1.9301e+14, -1.5234e+15,  1.8327e+13,\n",
            "          2.1754e+13, -1.5258e+15,  5.8278e+13,  2.3278e+11,  2.2548e+13,\n",
            "          1.4873e+15,  1.4309e+12, -9.7310e+12,  1.0273e+13,  1.6603e+15],\n",
            "        [ 1.5455e+15,  2.5879e+14,  1.6517e+15,  2.8757e+14,  2.3168e+13,\n",
            "         -2.4304e+12, -1.1587e+15, -1.5228e+15,  4.9475e+14,  7.3273e+13,\n",
            "         -1.5327e+15,  6.8370e+13,  1.8256e+14,  1.5635e+15, -1.1265e+13,\n",
            "          1.9175e+14,  7.8653e+14,  3.8894e+13,  1.5384e+15,  3.3269e+14,\n",
            "          2.8902e+14,  3.1995e+13,  1.9435e+14,  2.8696e+13, -2.2411e+13,\n",
            "          2.9381e+13,  1.5740e+15,  1.6697e+15,  1.1994e+14,  1.5887e+15,\n",
            "          9.8313e+13,  1.3012e+14,  3.3047e+13, -3.4444e+13,  8.8627e+13,\n",
            "          1.9187e+13, -1.5212e+15,  1.5613e+15, -1.4899e+15,  2.1796e+14,\n",
            "         -6.0617e+12,  2.2797e+14,  1.1451e+14,  4.6113e+13,  4.2662e+13,\n",
            "          1.3814e+14, -1.5413e+15,  4.3347e+14, -1.4931e+15,  8.5888e+12],\n",
            "        [-3.3257e+13,  1.8453e+14, -6.4245e+12,  1.4180e+14,  3.4631e+13,\n",
            "          1.5131e+15,  2.2405e+14, -1.3102e+13,  4.8405e+14, -3.4093e+13,\n",
            "         -1.5362e+15, -4.0808e+13,  1.1903e+14,  1.5430e+15,  1.0833e+13,\n",
            "         -1.2213e+14,  6.3276e+14,  9.9321e+13,  2.9484e+12,  5.4375e+13,\n",
            "          1.5521e+14, -2.2492e+12,  1.0030e+14, -9.9057e+13, -3.8239e+11,\n",
            "         -1.5270e+15,  4.9327e+13,  6.2268e+13,  9.3139e+12,  1.3047e+13,\n",
            "          1.5623e+14, -9.6162e+13, -4.2383e+12, -5.6708e+13,  5.4007e+12,\n",
            "          5.6489e+13, -6.8322e+12, -1.4943e+15,  2.5291e+13,  1.6273e+15,\n",
            "          1.6331e+15,  5.3801e+13,  2.4094e+13, -1.3978e+13,  1.6432e+12,\n",
            "          9.0864e+13, -1.2594e+12,  9.6469e+13, -1.4034e+14,  7.8231e+13],\n",
            "        [-1.8517e+13, -1.2840e+14, -1.4895e+15, -4.2805e+12,  1.2812e+13,\n",
            "         -4.0926e+13,  1.4487e+15, -6.1528e+13, -2.2198e+13, -1.4559e+15,\n",
            "          1.5310e+15,  1.5418e+15, -6.4797e+13,  1.3097e+13,  1.5431e+15,\n",
            "          9.4283e+13,  4.3484e+13,  1.5135e+15,  3.6121e+12,  1.5069e+15,\n",
            "         -3.6698e+13,  1.0519e+13,  1.3716e+15,  1.6456e+14,  4.5159e+13,\n",
            "         -2.8874e+12, -3.2207e+13, -1.5072e+15,  1.5516e+15,  3.5135e+13,\n",
            "          1.5692e+15,  1.3762e+14, -1.4854e+15,  1.5683e+15, -5.2016e+12,\n",
            "          1.5399e+15,  3.7240e+13, -1.5266e+15,  1.5442e+15, -3.4666e+12,\n",
            "          8.8071e+12,  1.9651e+13, -1.0017e+14, -2.6354e+12,  1.5618e+15,\n",
            "         -7.6347e+13,  6.6405e+12,  1.0402e+14, -6.9525e+13, -7.3087e+13],\n",
            "        [ 1.4536e+14, -3.0180e+13,  1.4847e+14,  8.7227e+13, -6.3283e+12,\n",
            "         -2.9292e+12,  1.9991e+14,  2.1060e+12,  6.5551e+13,  9.1303e+13,\n",
            "          2.2994e+12,  5.1630e+13,  1.6019e+15, -1.4186e+15,  1.8999e+13,\n",
            "          1.0849e+14,  1.6129e+13,  5.0808e+13, -1.5510e+15,  1.3024e+15,\n",
            "         -1.4528e+15,  3.2384e+13,  1.1859e+14, -1.4971e+15, -8.4426e+13,\n",
            "          4.6869e+13,  5.4130e+13,  7.7071e+13,  9.8347e+13,  1.3851e+14,\n",
            "          2.5359e+13,  1.2786e+14,  3.4018e+14, -1.0196e+13, -7.0936e+12,\n",
            "          3.5588e+13,  5.9082e+13,  7.5461e+13,  1.9307e+13, -1.5366e+15,\n",
            "          1.6846e+13,  1.3705e+14,  2.4405e+14,  4.5210e+13,  7.2276e+13,\n",
            "          1.7195e+14,  5.9415e+12,  2.9132e+14,  1.0695e+14, -1.1936e+13],\n",
            "        [ 4.2216e+13,  9.6018e+13, -1.3561e+15,  1.6758e+15,  5.4813e+13,\n",
            "          6.3992e+12,  3.0645e+14,  2.0289e+13,  2.3245e+14, -1.4872e+15,\n",
            "          6.3446e+12,  7.6122e+13,  1.6695e+14,  5.5980e+13,  8.2845e+12,\n",
            "          1.0898e+14, -1.0951e+15,  7.2705e+13,  2.1992e+13, -2.2793e+14,\n",
            "          2.0106e+14,  3.0759e+13,  7.0299e+13,  7.2541e+13, -1.6523e+14,\n",
            "          4.0341e+13,  9.7622e+13,  9.9382e+13, -1.5196e+15,  4.6670e+13,\n",
            "          7.0204e+13, -1.5190e+14,  2.6320e+14, -3.9990e+13,  3.0688e+13,\n",
            "          1.0365e+14, -1.4020e+15,  3.8225e+13,  1.4535e+13,  1.1501e+14,\n",
            "         -1.1404e+13, -1.3739e+15, -1.0240e+12,  4.3498e+13,  1.6187e+15,\n",
            "          1.5502e+15,  1.0091e+13,  2.7938e+14, -1.7148e+13,  7.9312e+13],\n",
            "        [ 1.5141e+15,  1.0439e+14,  9.4513e+13, -1.4992e+15,  9.1044e+13,\n",
            "         -1.9112e+13,  1.3496e+14, -5.3644e+13,  1.9256e+15, -2.4919e+13,\n",
            "         -1.5329e+15,  8.5197e+13,  1.0978e+14,  1.4259e+15,  1.3956e+13,\n",
            "          1.5214e+13,  1.7878e+14,  2.2116e+13,  6.2168e+12, -1.8739e+15,\n",
            "          2.6540e+13, -1.5443e+15, -1.4474e+14, -4.0542e+13, -4.4995e+13,\n",
            "          1.0786e+13,  1.0334e+14,  5.4125e+13,  1.7656e+14,  1.5436e+15,\n",
            "         -1.4975e+15, -1.5552e+15,  4.8288e+13, -3.1148e+13,  1.5168e+12,\n",
            "         -1.5798e+12, -1.3924e+13,  7.9836e+13,  1.5276e+15,  1.5251e+15,\n",
            "         -2.3392e+13,  5.3226e+13, -9.1765e+13,  2.1775e+13,  7.0014e+13,\n",
            "         -1.4070e+14,  1.5908e+15, -6.8539e+13,  1.4095e+15, -1.4909e+15]])\n",
            "calculating 2.bias\n",
            "tensor([ 7.5000e-02,  1.5372e+15, -1.5372e+15,  9.5000e-02,  1.0000e-02,\n",
            "         3.8000e-02,  7.5000e-02,  1.4800e-01,  1.0900e-01,  6.8000e-02])\n",
            "Test Accuracy: 0.157800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Wrapper)>[AdditiveSharingTensor]\n",
              "\t-> (Wrapper)>[PointerTensor | me:29034010016 -> worker_0:51334403573]\n",
              "\t-> (Wrapper)>[PointerTensor | me:82552229219 -> worker_1:45373275418]\n",
              "\t-> (Wrapper)>[PointerTensor | me:77865090764 -> worker_2:54095967242]\n",
              "\t*crypto provider: me*"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DULgohtbqKGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}