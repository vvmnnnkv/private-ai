{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Section 3 - Encrypted Federated Learning Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvmnnnkv/private-ai/blob/master/Section%203%20-%20Encrypted%20Federated%20Learning%20Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWjkPM2zZDy5",
        "colab_type": "text"
      },
      "source": [
        "# Federated Learning with Encrypted Gradients Aggregation Project\n",
        "\n",
        "This project is improvement of naive Federated Learning protocol implemented previuosly. Now the gradients are securely aggregated using additive secret sharing method.\n",
        "\n",
        "Aggregation is done among workers so we don't need separate dedicated aggregator worker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "746q9LdrQV9s",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# install dependency\n",
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMfNO4USQOkW",
        "colab_type": "code",
        "outputId": "e6ae0ac6-94e9-4546-cdca-27190c64c02e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "hook = sy.TorchHook(torch)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0708 21:44:12.342126 140709261846400 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0708 21:44:12.363773 140709261846400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO0qJA23QRCn",
        "colab_type": "code",
        "outputId": "01ad5c7c-db2c-444f-f3c4-389d4ff72a3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load MNIST included with Colab\n",
        "def mnist_to_torch(df, train=True):\n",
        "  y = pd.get_dummies(df[0])\n",
        "  X = df.drop(0, axis=1)\n",
        "  X, y = torch.tensor(X.values).type(torch.float), torch.tensor(y.values).type(torch.float)\n",
        "  return X, y\n",
        "\n",
        "# Train & test datasets\n",
        "X_train, y_train = mnist_to_torch(pd.read_csv(\"sample_data/mnist_train_small.csv\", header=None))\n",
        "X_test, y_test = mnist_to_torch(pd.read_csv(\"sample_data/mnist_test.csv\", header=None))\n",
        "\n",
        "num_train = X_train.size(0)\n",
        "num_features = X_train.size(1)\n",
        "\n",
        "print(\"Train size %d, test size: %d\" % (num_train, y_test.size(0)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size 20000, test size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7mxYqKeUFJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of workers\n",
        "# NOTE: there's a bug in pysyft that prevents sharing to more than 2 workers: \n",
        "# https://github.com/OpenMined/PySyft/issues/2341\n",
        "num_workers = 2\n",
        "# Create workers\n",
        "workers = []\n",
        "for i in range(num_workers):\n",
        "  worker = sy.VirtualWorker(hook, id=\"worker_%d\" % i)\n",
        "  workers.append(worker)\n",
        "  \n",
        "crypto_provider = sy.VirtualWorker(hook, id=\"crypto-provider\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fPvEoC4Zvfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data and send chunk to each worker\n",
        "fed_dataset = []\n",
        "chunk_size = num_train // num_workers\n",
        "for i in range(num_workers):\n",
        "  start = i * chunk_size\n",
        "  if i + 1 < num_workers:\n",
        "    end = (i + 1) * chunk_size\n",
        "  else:\n",
        "    end = num_train\n",
        "  fed_dataset.append((\n",
        "      X_train[start:end].send(workers[i]), \n",
        "      y_train[start:end].send(workers[i])\n",
        "  ))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfClHa20aGZe",
        "colab_type": "code",
        "outputId": "c86b98cb-c232-402d-97b6-7d82dfd4fa34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(workers)\n",
        "print(fed_dataset)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<VirtualWorker id:worker_0 #objects:2>, <VirtualWorker id:worker_1 #objects:2>]\n",
            "[((Wrapper)>[PointerTensor | me:41187151049 -> worker_0:66122510366], (Wrapper)>[PointerTensor | me:81037035040 -> worker_0:93398699170]), ((Wrapper)>[PointerTensor | me:61797687527 -> worker_1:33244494485], (Wrapper)>[PointerTensor | me:24216443568 -> worker_1:72551751488])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_o5xum8Vw97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take model gradients and share among workers\n",
        "def share_model_grads(model, workers, crypto_provider):\n",
        "  out = {}\n",
        "  for name, param in model.named_parameters():\n",
        "    if not param.requires_grad: continue\n",
        "    out[name] = param.grad.fix_prec().share(*workers, crypto_provider=crypto_provider)\n",
        "  return out\n",
        "\n",
        "  \n",
        "# Federated training procedure\n",
        "def fed_train(model, criteria, fed_dataset, test_dataset, opt, avg_epochs = 50, worker_epochs = 1, lr=0.001):\n",
        "  for global_epoch in range(avg_epochs):\n",
        "    # copy latest model to workers\n",
        "    fed_models = {}\n",
        "    for X, y in fed_dataset:\n",
        "      fed_model = model.copy().send(X.location)\n",
        "      optimizer = opt(params=fed_model.parameters(), lr=lr)\n",
        "      fed_models[fed_model.location.id] = (fed_model, optimizer)\n",
        "    \n",
        "    # train in parallel on workers\n",
        "    for local_epoch in range(worker_epochs):\n",
        "      losses = []\n",
        "      for X, y in fed_dataset:\n",
        "        fed_model, optimizer = fed_models[X.location.id]\n",
        "        pred = fed_model(X)\n",
        "        loss = criteria(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss = loss.get()\n",
        "        losses.append(loss)\n",
        "      print('Avg loss (%d/%d): %f' % (global_epoch, local_epoch, sum(losses) / len(losses)))\n",
        "      \n",
        "    # aggregate worker's models\n",
        "    # share each model to each worker\n",
        "    all_grads = {}\n",
        "    for w, fm in fed_models.items():\n",
        "      # print('sharing %s model' % w)\n",
        "      all_grads[w] = share_model_grads(fm[0], workers, crypto_provider)\n",
        "      # move shared pointers to my machine\n",
        "      all_grads[w] = { k:v.get() for k,v in all_grads[w].items() }\n",
        "      \n",
        "    # prepare avg model placeholder\n",
        "    with torch.no_grad():\n",
        "      # calc grads sum\n",
        "      grads_sum = None\n",
        "      for w, grads in all_grads.items():\n",
        "        if not grads_sum:\n",
        "          grads_sum = grads\n",
        "          continue\n",
        "        for n, data in grads_sum.items():\n",
        "          grads_sum[n] += grads[n]\n",
        "      \n",
        "      state = model.state_dict()\n",
        "      # cal avg, retrieve and apply to local model\n",
        "      for n, data in grads_sum.items():\n",
        "        grads_sum[n] /= len(workers)\n",
        "        grads_sum[n] = grads_sum[n].get().float_prec()\n",
        "        state[n] -= lr * grads_sum[n]\n",
        "      model.load_state_dict(state)\n",
        "\n",
        "      # calculate accuracy on test set\n",
        "      X_test, y_test = test_dataset\n",
        "      y_pred = torch.softmax(model(X_test), dim=1)\n",
        "      valid = (torch.argmax(y_pred, dim=1) == torch.argmax(y_test, dim=1)).sum()\n",
        "      print('Test Accuracy: %f' % (float(valid) / float(y_test.size(0))))\n",
        "\n",
        "  return model\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD2irlGvQhkU",
        "colab_type": "code",
        "outputId": "11d7da91-dd85-43e6-db4b-9057b623ed9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define a simple MLP model (softmax is included in loss)\n",
        "model = torch.nn.Sequential(\n",
        "  torch.nn.Linear(num_features, 50),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(50, 10)\n",
        ")\n",
        "loss = torch.nn.modules.loss.BCEWithLogitsLoss()\n",
        "\n",
        "# Train!\n",
        "fed_train(model, loss, fed_dataset, (X_test, y_test), torch.optim.SGD, 200, 1)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg loss (0/0): 9.773861\n",
            "Test Accuracy: 0.146000\n",
            "Avg loss (1/0): 3.588379\n",
            "Test Accuracy: 0.161300\n",
            "Avg loss (2/0): 2.208213\n",
            "Test Accuracy: 0.176100\n",
            "Avg loss (3/0): 1.864568\n",
            "Test Accuracy: 0.188900\n",
            "Avg loss (4/0): 1.680759\n",
            "Test Accuracy: 0.199700\n",
            "Avg loss (5/0): 1.553097\n",
            "Test Accuracy: 0.214000\n",
            "Avg loss (6/0): 1.452741\n",
            "Test Accuracy: 0.225700\n",
            "Avg loss (7/0): 1.368572\n",
            "Test Accuracy: 0.239200\n",
            "Avg loss (8/0): 1.295168\n",
            "Test Accuracy: 0.248600\n",
            "Avg loss (9/0): 1.229687\n",
            "Test Accuracy: 0.260400\n",
            "Avg loss (10/0): 1.170548\n",
            "Test Accuracy: 0.270900\n",
            "Avg loss (11/0): 1.116734\n",
            "Test Accuracy: 0.280800\n",
            "Avg loss (12/0): 1.067501\n",
            "Test Accuracy: 0.292400\n",
            "Avg loss (13/0): 1.022332\n",
            "Test Accuracy: 0.302000\n",
            "Avg loss (14/0): 0.980860\n",
            "Test Accuracy: 0.312500\n",
            "Avg loss (15/0): 0.942804\n",
            "Test Accuracy: 0.322600\n",
            "Avg loss (16/0): 0.907807\n",
            "Test Accuracy: 0.331800\n",
            "Avg loss (17/0): 0.875643\n",
            "Test Accuracy: 0.340800\n",
            "Avg loss (18/0): 0.846064\n",
            "Test Accuracy: 0.350200\n",
            "Avg loss (19/0): 0.818938\n",
            "Test Accuracy: 0.355700\n",
            "Avg loss (20/0): 0.793886\n",
            "Test Accuracy: 0.363200\n",
            "Avg loss (21/0): 0.770811\n",
            "Test Accuracy: 0.369600\n",
            "Avg loss (22/0): 0.749541\n",
            "Test Accuracy: 0.377600\n",
            "Avg loss (23/0): 0.729889\n",
            "Test Accuracy: 0.384300\n",
            "Avg loss (24/0): 0.711693\n",
            "Test Accuracy: 0.391300\n",
            "Avg loss (25/0): 0.694771\n",
            "Test Accuracy: 0.397200\n",
            "Avg loss (26/0): 0.679053\n",
            "Test Accuracy: 0.404600\n",
            "Avg loss (27/0): 0.664379\n",
            "Test Accuracy: 0.411300\n",
            "Avg loss (28/0): 0.650661\n",
            "Test Accuracy: 0.418000\n",
            "Avg loss (29/0): 0.637732\n",
            "Test Accuracy: 0.424900\n",
            "Avg loss (30/0): 0.625569\n",
            "Test Accuracy: 0.429700\n",
            "Avg loss (31/0): 0.614114\n",
            "Test Accuracy: 0.435100\n",
            "Avg loss (32/0): 0.603269\n",
            "Test Accuracy: 0.442400\n",
            "Avg loss (33/0): 0.592988\n",
            "Test Accuracy: 0.448500\n",
            "Avg loss (34/0): 0.583256\n",
            "Test Accuracy: 0.454800\n",
            "Avg loss (35/0): 0.574019\n",
            "Test Accuracy: 0.459700\n",
            "Avg loss (36/0): 0.565249\n",
            "Test Accuracy: 0.463600\n",
            "Avg loss (37/0): 0.556890\n",
            "Test Accuracy: 0.468800\n",
            "Avg loss (38/0): 0.548918\n",
            "Test Accuracy: 0.472400\n",
            "Avg loss (39/0): 0.541314\n",
            "Test Accuracy: 0.476500\n",
            "Avg loss (40/0): 0.534062\n",
            "Test Accuracy: 0.480000\n",
            "Avg loss (41/0): 0.527118\n",
            "Test Accuracy: 0.484000\n",
            "Avg loss (42/0): 0.520461\n",
            "Test Accuracy: 0.489000\n",
            "Avg loss (43/0): 0.514075\n",
            "Test Accuracy: 0.493100\n",
            "Avg loss (44/0): 0.507953\n",
            "Test Accuracy: 0.497100\n",
            "Avg loss (45/0): 0.502072\n",
            "Test Accuracy: 0.500400\n",
            "Avg loss (46/0): 0.496399\n",
            "Test Accuracy: 0.503300\n",
            "Avg loss (47/0): 0.490930\n",
            "Test Accuracy: 0.507000\n",
            "Avg loss (48/0): 0.485661\n",
            "Test Accuracy: 0.509800\n",
            "Avg loss (49/0): 0.480575\n",
            "Test Accuracy: 0.512900\n",
            "Avg loss (50/0): 0.475654\n",
            "Test Accuracy: 0.514700\n",
            "Avg loss (51/0): 0.470894\n",
            "Test Accuracy: 0.516800\n",
            "Avg loss (52/0): 0.466284\n",
            "Test Accuracy: 0.520100\n",
            "Avg loss (53/0): 0.461821\n",
            "Test Accuracy: 0.522900\n",
            "Avg loss (54/0): 0.457484\n",
            "Test Accuracy: 0.526000\n",
            "Avg loss (55/0): 0.453269\n",
            "Test Accuracy: 0.528100\n",
            "Avg loss (56/0): 0.449185\n",
            "Test Accuracy: 0.531900\n",
            "Avg loss (57/0): 0.445223\n",
            "Test Accuracy: 0.534000\n",
            "Avg loss (58/0): 0.441379\n",
            "Test Accuracy: 0.536700\n",
            "Avg loss (59/0): 0.437648\n",
            "Test Accuracy: 0.538600\n",
            "Avg loss (60/0): 0.434019\n",
            "Test Accuracy: 0.541800\n",
            "Avg loss (61/0): 0.430487\n",
            "Test Accuracy: 0.544000\n",
            "Avg loss (62/0): 0.427046\n",
            "Test Accuracy: 0.547200\n",
            "Avg loss (63/0): 0.423690\n",
            "Test Accuracy: 0.550700\n",
            "Avg loss (64/0): 0.420423\n",
            "Test Accuracy: 0.552100\n",
            "Avg loss (65/0): 0.417242\n",
            "Test Accuracy: 0.555100\n",
            "Avg loss (66/0): 0.414141\n",
            "Test Accuracy: 0.557300\n",
            "Avg loss (67/0): 0.411113\n",
            "Test Accuracy: 0.559000\n",
            "Avg loss (68/0): 0.408154\n",
            "Test Accuracy: 0.560800\n",
            "Avg loss (69/0): 0.405268\n",
            "Test Accuracy: 0.562700\n",
            "Avg loss (70/0): 0.402451\n",
            "Test Accuracy: 0.566300\n",
            "Avg loss (71/0): 0.399695\n",
            "Test Accuracy: 0.568800\n",
            "Avg loss (72/0): 0.396996\n",
            "Test Accuracy: 0.570100\n",
            "Avg loss (73/0): 0.394357\n",
            "Test Accuracy: 0.571600\n",
            "Avg loss (74/0): 0.391772\n",
            "Test Accuracy: 0.573700\n",
            "Avg loss (75/0): 0.389240\n",
            "Test Accuracy: 0.576100\n",
            "Avg loss (76/0): 0.386761\n",
            "Test Accuracy: 0.578000\n",
            "Avg loss (77/0): 0.384326\n",
            "Test Accuracy: 0.579300\n",
            "Avg loss (78/0): 0.381938\n",
            "Test Accuracy: 0.581300\n",
            "Avg loss (79/0): 0.379597\n",
            "Test Accuracy: 0.582500\n",
            "Avg loss (80/0): 0.377304\n",
            "Test Accuracy: 0.583500\n",
            "Avg loss (81/0): 0.375060\n",
            "Test Accuracy: 0.585200\n",
            "Avg loss (82/0): 0.372859\n",
            "Test Accuracy: 0.586800\n",
            "Avg loss (83/0): 0.370701\n",
            "Test Accuracy: 0.589600\n",
            "Avg loss (84/0): 0.368584\n",
            "Test Accuracy: 0.591300\n",
            "Avg loss (85/0): 0.366507\n",
            "Test Accuracy: 0.593400\n",
            "Avg loss (86/0): 0.364464\n",
            "Test Accuracy: 0.595100\n",
            "Avg loss (87/0): 0.362457\n",
            "Test Accuracy: 0.596500\n",
            "Avg loss (88/0): 0.360488\n",
            "Test Accuracy: 0.598100\n",
            "Avg loss (89/0): 0.358551\n",
            "Test Accuracy: 0.599200\n",
            "Avg loss (90/0): 0.356642\n",
            "Test Accuracy: 0.601900\n",
            "Avg loss (91/0): 0.354766\n",
            "Test Accuracy: 0.603800\n",
            "Avg loss (92/0): 0.352920\n",
            "Test Accuracy: 0.605200\n",
            "Avg loss (93/0): 0.351102\n",
            "Test Accuracy: 0.606900\n",
            "Avg loss (94/0): 0.349316\n",
            "Test Accuracy: 0.607900\n",
            "Avg loss (95/0): 0.347560\n",
            "Test Accuracy: 0.609700\n",
            "Avg loss (96/0): 0.345833\n",
            "Test Accuracy: 0.610600\n",
            "Avg loss (97/0): 0.344134\n",
            "Test Accuracy: 0.611900\n",
            "Avg loss (98/0): 0.342465\n",
            "Test Accuracy: 0.612800\n",
            "Avg loss (99/0): 0.340822\n",
            "Test Accuracy: 0.613800\n",
            "Avg loss (100/0): 0.339205\n",
            "Test Accuracy: 0.614800\n",
            "Avg loss (101/0): 0.337610\n",
            "Test Accuracy: 0.616800\n",
            "Avg loss (102/0): 0.336040\n",
            "Test Accuracy: 0.618300\n",
            "Avg loss (103/0): 0.334493\n",
            "Test Accuracy: 0.620000\n",
            "Avg loss (104/0): 0.332969\n",
            "Test Accuracy: 0.621800\n",
            "Avg loss (105/0): 0.331469\n",
            "Test Accuracy: 0.623400\n",
            "Avg loss (106/0): 0.329988\n",
            "Test Accuracy: 0.625300\n",
            "Avg loss (107/0): 0.328526\n",
            "Test Accuracy: 0.626700\n",
            "Avg loss (108/0): 0.327087\n",
            "Test Accuracy: 0.627600\n",
            "Avg loss (109/0): 0.325668\n",
            "Test Accuracy: 0.628900\n",
            "Avg loss (110/0): 0.324269\n",
            "Test Accuracy: 0.630000\n",
            "Avg loss (111/0): 0.322891\n",
            "Test Accuracy: 0.631300\n",
            "Avg loss (112/0): 0.321531\n",
            "Test Accuracy: 0.632300\n",
            "Avg loss (113/0): 0.320189\n",
            "Test Accuracy: 0.633400\n",
            "Avg loss (114/0): 0.318864\n",
            "Test Accuracy: 0.633800\n",
            "Avg loss (115/0): 0.317556\n",
            "Test Accuracy: 0.634400\n",
            "Avg loss (116/0): 0.316263\n",
            "Test Accuracy: 0.635900\n",
            "Avg loss (117/0): 0.314987\n",
            "Test Accuracy: 0.636900\n",
            "Avg loss (118/0): 0.313727\n",
            "Test Accuracy: 0.638100\n",
            "Avg loss (119/0): 0.312485\n",
            "Test Accuracy: 0.639000\n",
            "Avg loss (120/0): 0.311258\n",
            "Test Accuracy: 0.639800\n",
            "Avg loss (121/0): 0.310046\n",
            "Test Accuracy: 0.640400\n",
            "Avg loss (122/0): 0.308848\n",
            "Test Accuracy: 0.641400\n",
            "Avg loss (123/0): 0.307662\n",
            "Test Accuracy: 0.642100\n",
            "Avg loss (124/0): 0.306493\n",
            "Test Accuracy: 0.643300\n",
            "Avg loss (125/0): 0.305338\n",
            "Test Accuracy: 0.643900\n",
            "Avg loss (126/0): 0.304196\n",
            "Test Accuracy: 0.645100\n",
            "Avg loss (127/0): 0.303069\n",
            "Test Accuracy: 0.646300\n",
            "Avg loss (128/0): 0.301953\n",
            "Test Accuracy: 0.647400\n",
            "Avg loss (129/0): 0.300852\n",
            "Test Accuracy: 0.648400\n",
            "Avg loss (130/0): 0.299764\n",
            "Test Accuracy: 0.649000\n",
            "Avg loss (131/0): 0.298688\n",
            "Test Accuracy: 0.649900\n",
            "Avg loss (132/0): 0.297625\n",
            "Test Accuracy: 0.650800\n",
            "Avg loss (133/0): 0.296573\n",
            "Test Accuracy: 0.652300\n",
            "Avg loss (134/0): 0.295532\n",
            "Test Accuracy: 0.653200\n",
            "Avg loss (135/0): 0.294501\n",
            "Test Accuracy: 0.654000\n",
            "Avg loss (136/0): 0.293481\n",
            "Test Accuracy: 0.655500\n",
            "Avg loss (137/0): 0.292471\n",
            "Test Accuracy: 0.656500\n",
            "Avg loss (138/0): 0.291472\n",
            "Test Accuracy: 0.657700\n",
            "Avg loss (139/0): 0.290484\n",
            "Test Accuracy: 0.658400\n",
            "Avg loss (140/0): 0.289508\n",
            "Test Accuracy: 0.659200\n",
            "Avg loss (141/0): 0.288540\n",
            "Test Accuracy: 0.660800\n",
            "Avg loss (142/0): 0.287583\n",
            "Test Accuracy: 0.662000\n",
            "Avg loss (143/0): 0.286635\n",
            "Test Accuracy: 0.662200\n",
            "Avg loss (144/0): 0.285698\n",
            "Test Accuracy: 0.663100\n",
            "Avg loss (145/0): 0.284770\n",
            "Test Accuracy: 0.663700\n",
            "Avg loss (146/0): 0.283852\n",
            "Test Accuracy: 0.664400\n",
            "Avg loss (147/0): 0.282943\n",
            "Test Accuracy: 0.664800\n",
            "Avg loss (148/0): 0.282041\n",
            "Test Accuracy: 0.665600\n",
            "Avg loss (149/0): 0.281147\n",
            "Test Accuracy: 0.666000\n",
            "Avg loss (150/0): 0.280262\n",
            "Test Accuracy: 0.666700\n",
            "Avg loss (151/0): 0.279387\n",
            "Test Accuracy: 0.667700\n",
            "Avg loss (152/0): 0.278520\n",
            "Test Accuracy: 0.668400\n",
            "Avg loss (153/0): 0.277662\n",
            "Test Accuracy: 0.669200\n",
            "Avg loss (154/0): 0.276810\n",
            "Test Accuracy: 0.669600\n",
            "Avg loss (155/0): 0.275967\n",
            "Test Accuracy: 0.670500\n",
            "Avg loss (156/0): 0.275130\n",
            "Test Accuracy: 0.671500\n",
            "Avg loss (157/0): 0.274303\n",
            "Test Accuracy: 0.672100\n",
            "Avg loss (158/0): 0.273484\n",
            "Test Accuracy: 0.673400\n",
            "Avg loss (159/0): 0.272671\n",
            "Test Accuracy: 0.674300\n",
            "Avg loss (160/0): 0.271866\n",
            "Test Accuracy: 0.675100\n",
            "Avg loss (161/0): 0.271069\n",
            "Test Accuracy: 0.675500\n",
            "Avg loss (162/0): 0.270279\n",
            "Test Accuracy: 0.676700\n",
            "Avg loss (163/0): 0.269495\n",
            "Test Accuracy: 0.677000\n",
            "Avg loss (164/0): 0.268720\n",
            "Test Accuracy: 0.678000\n",
            "Avg loss (165/0): 0.267949\n",
            "Test Accuracy: 0.678700\n",
            "Avg loss (166/0): 0.267187\n",
            "Test Accuracy: 0.679600\n",
            "Avg loss (167/0): 0.266433\n",
            "Test Accuracy: 0.680200\n",
            "Avg loss (168/0): 0.265685\n",
            "Test Accuracy: 0.681000\n",
            "Avg loss (169/0): 0.264944\n",
            "Test Accuracy: 0.681300\n",
            "Avg loss (170/0): 0.264211\n",
            "Test Accuracy: 0.682100\n",
            "Avg loss (171/0): 0.263486\n",
            "Test Accuracy: 0.682600\n",
            "Avg loss (172/0): 0.262766\n",
            "Test Accuracy: 0.682700\n",
            "Avg loss (173/0): 0.262053\n",
            "Test Accuracy: 0.683100\n",
            "Avg loss (174/0): 0.261347\n",
            "Test Accuracy: 0.683900\n",
            "Avg loss (175/0): 0.260648\n",
            "Test Accuracy: 0.684700\n",
            "Avg loss (176/0): 0.259954\n",
            "Test Accuracy: 0.685800\n",
            "Avg loss (177/0): 0.259266\n",
            "Test Accuracy: 0.686500\n",
            "Avg loss (178/0): 0.258584\n",
            "Test Accuracy: 0.687200\n",
            "Avg loss (179/0): 0.257907\n",
            "Test Accuracy: 0.687800\n",
            "Avg loss (180/0): 0.257237\n",
            "Test Accuracy: 0.689300\n",
            "Avg loss (181/0): 0.256571\n",
            "Test Accuracy: 0.689700\n",
            "Avg loss (182/0): 0.255910\n",
            "Test Accuracy: 0.690100\n",
            "Avg loss (183/0): 0.255254\n",
            "Test Accuracy: 0.690500\n",
            "Avg loss (184/0): 0.254605\n",
            "Test Accuracy: 0.691000\n",
            "Avg loss (185/0): 0.253960\n",
            "Test Accuracy: 0.691700\n",
            "Avg loss (186/0): 0.253321\n",
            "Test Accuracy: 0.691900\n",
            "Avg loss (187/0): 0.252686\n",
            "Test Accuracy: 0.692800\n",
            "Avg loss (188/0): 0.252058\n",
            "Test Accuracy: 0.693600\n",
            "Avg loss (189/0): 0.251434\n",
            "Test Accuracy: 0.694200\n",
            "Avg loss (190/0): 0.250817\n",
            "Test Accuracy: 0.695100\n",
            "Avg loss (191/0): 0.250204\n",
            "Test Accuracy: 0.695700\n",
            "Avg loss (192/0): 0.249597\n",
            "Test Accuracy: 0.697000\n",
            "Avg loss (193/0): 0.248995\n",
            "Test Accuracy: 0.697600\n",
            "Avg loss (194/0): 0.248397\n",
            "Test Accuracy: 0.698000\n",
            "Avg loss (195/0): 0.247803\n",
            "Test Accuracy: 0.698400\n",
            "Avg loss (196/0): 0.247213\n",
            "Test Accuracy: 0.698800\n",
            "Avg loss (197/0): 0.246629\n",
            "Test Accuracy: 0.699200\n",
            "Avg loss (198/0): 0.246050\n",
            "Test Accuracy: 0.699800\n",
            "Avg loss (199/0): 0.245474\n",
            "Test Accuracy: 0.700300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DULgohtbqKGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}