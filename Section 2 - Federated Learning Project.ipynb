{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated Learning Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvmnnnkv/private-ai/blob/master/Section%202%20-%20Federated%20Learning%20Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWjkPM2zZDy5",
        "colab_type": "text"
      },
      "source": [
        "# Federated Learning Project\n",
        "\n",
        "This project explores remote learning and aggregation of model weights.\n",
        "\n",
        "\"Server\" only sees aggregated model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "746q9LdrQV9s",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "# install dependency\n",
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMfNO4USQOkW",
        "colab_type": "code",
        "outputId": "fe4390bc-fae6-48d3-935a-fa0431a9fdb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "hook = sy.TorchHook(torch)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0703 19:52:06.109340 139941403436928 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0703 19:52:06.127920 139941403436928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO0qJA23QRCn",
        "colab_type": "code",
        "outputId": "712fcb0f-bd35-476a-c99b-a4647f899a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load MNIST included with Colab\n",
        "def mnist_to_torch(df, train=True):\n",
        "  y = pd.get_dummies(df[0])\n",
        "  X = df.drop(0, axis=1)\n",
        "  X, y = torch.tensor(X.values).type(torch.float), torch.tensor(y.values).type(torch.float)\n",
        "  return X, y\n",
        "\n",
        "# Train & test datasets\n",
        "X_train, y_train = mnist_to_torch(pd.read_csv(\"sample_data/mnist_train_small.csv\", header=None))\n",
        "X_test, y_test = mnist_to_torch(pd.read_csv(\"sample_data/mnist_test.csv\", header=None))\n",
        "\n",
        "num_train = X_train.size(0)\n",
        "num_features = X_train.size(1)\n",
        "\n",
        "print(\"Train size %d, test size: %d\" % (num_train, y_test.size(0)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size 20000, test size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7mxYqKeUFJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of workers\n",
        "num_workers = 3\n",
        "# Create workers\n",
        "workers = []\n",
        "for i in range(num_workers):\n",
        "  worker = sy.VirtualWorker(hook, id=\"worker_%d\" % i)\n",
        "  workers.append(worker)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fPvEoC4Zvfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data and send chunk to each worker\n",
        "fed_dataset = []\n",
        "chunk_size = num_train // num_workers\n",
        "for i in range(num_workers):\n",
        "  start = i * chunk_size\n",
        "  if i + 1 < num_workers:\n",
        "    end = (i + 1) * chunk_size\n",
        "  else:\n",
        "    end = num_train\n",
        "  fed_dataset.append((\n",
        "      X_train[start:end].send(workers[i]), \n",
        "      y_train[start:end].send(workers[i])\n",
        "  ))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfClHa20aGZe",
        "colab_type": "code",
        "outputId": "c012c85d-4815-48e1-e9b2-1fb7bb627e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(workers)\n",
        "print(fed_dataset)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<VirtualWorker id:worker_0 #objects:2>, <VirtualWorker id:worker_1 #objects:2>, <VirtualWorker id:worker_2 #objects:2>]\n",
            "[((Wrapper)>[PointerTensor | me:66185758196 -> worker_0:94614901028], (Wrapper)>[PointerTensor | me:71706664369 -> worker_0:32131384440]), ((Wrapper)>[PointerTensor | me:12279854652 -> worker_1:67993701178], (Wrapper)>[PointerTensor | me:88871755116 -> worker_1:597763287]), ((Wrapper)>[PointerTensor | me:79878815535 -> worker_2:90803368877], (Wrapper)>[PointerTensor | me:24469173731 -> worker_2:60846637558])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWUDZPOGLPo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create aggregator worker\n",
        "aggregator = sy.VirtualWorker(hook, id=\"aggregator\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_o5xum8Vw97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate mean for all parameters of list of models and set to target_model\n",
        "def avg_params(source_models, target_model):\n",
        "  # get params and emptify\n",
        "  avg_dict = target_model.state_dict()\n",
        "  for param, _ in avg_dict.items():\n",
        "    avg_dict[param].zero_()\n",
        "  \n",
        "  # sum up params\n",
        "  for _, m in source_models.items():\n",
        "    m_dict = m[0].state_dict()\n",
        "    for param, _ in m_dict.items():\n",
        "      avg_dict[param] += m_dict[param]\n",
        "  \n",
        "  # calc avg\n",
        "  for param, _ in avg_dict.items():\n",
        "    avg_dict[param] /= len(source_models)\n",
        "  \n",
        "  # set\n",
        "  target_model.load_state_dict(avg_dict)\n",
        "\n",
        "# Federated training procedure\n",
        "def fed_train(model, criteria, fed_dataset, test_dataset, aggregator, opt, avg_epochs = 10, worker_epochs = 30, lr=0.001):\n",
        "  for global_epoch in range(avg_epochs):\n",
        "    # copy latest model to workers\n",
        "    fed_models = {}\n",
        "    for X, y in fed_dataset:\n",
        "      fed_model = model.copy().send(X.location)\n",
        "      optimizer = opt(params=fed_model.parameters(), lr=lr)\n",
        "      fed_models[fed_model.location.id] = (fed_model, optimizer)\n",
        "    \n",
        "    # train in parallel on workers\n",
        "    for local_epoch in range(worker_epochs):\n",
        "      losses = []\n",
        "      for X, y in fed_dataset:\n",
        "        fed_model, optimizer = fed_models[X.location.id]\n",
        "        pred = fed_model(X)\n",
        "        loss = criteria(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss = loss.get()\n",
        "        losses.append(loss)\n",
        "      print('Avg loss (%d/%d): %f' % (global_epoch, local_epoch, sum(losses) / len(losses)))\n",
        "      \n",
        "    # aggregate worker models on aggregator\n",
        "    # move models to aggregator\n",
        "    for _, fm in fed_models.items():\n",
        "      fm[0].move(aggregator)\n",
        "\n",
        "    # prepare avg model placeholder on aggregator\n",
        "    avg_model = model.copy().send(aggregator)\n",
        "    with torch.no_grad():\n",
        "      # make model with average params on aggregator\n",
        "      avg_params(fed_models, avg_model)\n",
        "      # retrieve weights and apply to local model\n",
        "      avg_model = avg_model.get()\n",
        "      model.load_state_dict(avg_model.state_dict())\n",
        "      # calculate accuracy on test set\n",
        "      X_test, y_test = test_dataset\n",
        "      y_pred = torch.softmax(model(X_test), dim=1)\n",
        "      valid = (torch.argmax(y_pred, dim=1) == torch.argmax(y_test, dim=1)).sum()\n",
        "      print('Accuracy: %f' % (float(valid) / float(y_test.size(0))))\n",
        "\n",
        "  return model\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD2irlGvQhkU",
        "colab_type": "code",
        "outputId": "f21da4fa-a0b6-4cfb-c7e8-36be18b45603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define a simple MLP model (softmax is included in loss)\n",
        "model = torch.nn.Sequential(\n",
        "  torch.nn.Linear(num_features, 50),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(50, 10)\n",
        ")\n",
        "loss = torch.nn.modules.loss.BCEWithLogitsLoss()\n",
        "\n",
        "# Train!\n",
        "fed_train(model, loss, fed_dataset, (X_test, y_test), aggregator, torch.optim.SGD, 10, 20)\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg loss (0/0): 6.511034\n",
            "Avg loss (0/1): 2.853069\n",
            "Avg loss (0/2): 2.754401\n",
            "Avg loss (0/3): 2.743122\n",
            "Avg loss (0/4): 2.623974\n",
            "Avg loss (0/5): 2.333915\n",
            "Avg loss (0/6): 2.259338\n",
            "Avg loss (0/7): 2.097608\n",
            "Avg loss (0/8): 1.591617\n",
            "Avg loss (0/9): 1.129129\n",
            "Avg loss (0/10): 0.943001\n",
            "Avg loss (0/11): 0.873390\n",
            "Avg loss (0/12): 0.860417\n",
            "Avg loss (0/13): 0.879201\n",
            "Avg loss (0/14): 0.832374\n",
            "Avg loss (0/15): 0.713959\n",
            "Avg loss (0/16): 0.606388\n",
            "Avg loss (0/17): 0.550632\n",
            "Avg loss (0/18): 0.522244\n",
            "Avg loss (0/19): 0.501486\n",
            "Accuracy: 0.569400\n",
            "Avg loss (1/0): 0.484057\n",
            "Avg loss (1/1): 0.464428\n",
            "Avg loss (1/2): 0.433690\n",
            "Avg loss (1/3): 0.401670\n",
            "Avg loss (1/4): 0.373823\n",
            "Avg loss (1/5): 0.350277\n",
            "Avg loss (1/6): 0.328791\n",
            "Avg loss (1/7): 0.308742\n",
            "Avg loss (1/8): 0.290810\n",
            "Avg loss (1/9): 0.278112\n",
            "Avg loss (1/10): 0.274149\n",
            "Avg loss (1/11): 0.277833\n",
            "Avg loss (1/12): 0.282963\n",
            "Avg loss (1/13): 0.283684\n",
            "Avg loss (1/14): 0.280111\n",
            "Avg loss (1/15): 0.277553\n",
            "Avg loss (1/16): 0.277600\n",
            "Avg loss (1/17): 0.275951\n",
            "Avg loss (1/18): 0.267630\n",
            "Avg loss (1/19): 0.255288\n",
            "Accuracy: 0.581900\n",
            "Avg loss (2/0): 0.249289\n",
            "Avg loss (2/1): 0.245248\n",
            "Avg loss (2/2): 0.238011\n",
            "Avg loss (2/3): 0.228926\n",
            "Avg loss (2/4): 0.219411\n",
            "Avg loss (2/5): 0.210671\n",
            "Avg loss (2/6): 0.203562\n",
            "Avg loss (2/7): 0.198598\n",
            "Avg loss (2/8): 0.195827\n",
            "Avg loss (2/9): 0.194868\n",
            "Avg loss (2/10): 0.195056\n",
            "Avg loss (2/11): 0.195681\n",
            "Avg loss (2/12): 0.196181\n",
            "Avg loss (2/13): 0.196278\n",
            "Avg loss (2/14): 0.195880\n",
            "Avg loss (2/15): 0.195036\n",
            "Avg loss (2/16): 0.193691\n",
            "Avg loss (2/17): 0.191839\n",
            "Avg loss (2/18): 0.189582\n",
            "Avg loss (2/19): 0.186995\n",
            "Accuracy: 0.703900\n",
            "Avg loss (3/0): 0.185647\n",
            "Avg loss (3/1): 0.183911\n",
            "Avg loss (3/2): 0.180694\n",
            "Avg loss (3/3): 0.176423\n",
            "Avg loss (3/4): 0.171580\n",
            "Avg loss (3/5): 0.166610\n",
            "Avg loss (3/6): 0.161938\n",
            "Avg loss (3/7): 0.157961\n",
            "Avg loss (3/8): 0.155019\n",
            "Avg loss (3/9): 0.153381\n",
            "Avg loss (3/10): 0.153145\n",
            "Avg loss (3/11): 0.154179\n",
            "Avg loss (3/12): 0.156172\n",
            "Avg loss (3/13): 0.158607\n",
            "Avg loss (3/14): 0.160935\n",
            "Avg loss (3/15): 0.162734\n",
            "Avg loss (3/16): 0.163745\n",
            "Avg loss (3/17): 0.163849\n",
            "Avg loss (3/18): 0.162976\n",
            "Avg loss (3/19): 0.161127\n",
            "Accuracy: 0.735900\n",
            "Avg loss (4/0): 0.160067\n",
            "Avg loss (4/1): 0.158838\n",
            "Avg loss (4/2): 0.156658\n",
            "Avg loss (4/3): 0.153934\n",
            "Avg loss (4/4): 0.151013\n",
            "Avg loss (4/5): 0.148126\n",
            "Avg loss (4/6): 0.145413\n",
            "Avg loss (4/7): 0.142970\n",
            "Avg loss (4/8): 0.140952\n",
            "Avg loss (4/9): 0.139507\n",
            "Avg loss (4/10): 0.138704\n",
            "Avg loss (4/11): 0.138544\n",
            "Avg loss (4/12): 0.138961\n",
            "Avg loss (4/13): 0.139840\n",
            "Avg loss (4/14): 0.140966\n",
            "Avg loss (4/15): 0.142190\n",
            "Avg loss (4/16): 0.143359\n",
            "Avg loss (4/17): 0.144297\n",
            "Avg loss (4/18): 0.144804\n",
            "Avg loss (4/19): 0.144703\n",
            "Accuracy: 0.768400\n",
            "Avg loss (5/0): 0.145424\n",
            "Avg loss (5/1): 0.144489\n",
            "Avg loss (5/2): 0.142758\n",
            "Avg loss (5/3): 0.140468\n",
            "Avg loss (5/4): 0.137895\n",
            "Avg loss (5/5): 0.135309\n",
            "Avg loss (5/6): 0.132911\n",
            "Avg loss (5/7): 0.130876\n",
            "Avg loss (5/8): 0.129345\n",
            "Avg loss (5/9): 0.128379\n",
            "Avg loss (5/10): 0.128013\n",
            "Avg loss (5/11): 0.128226\n",
            "Avg loss (5/12): 0.128904\n",
            "Avg loss (5/13): 0.129871\n",
            "Avg loss (5/14): 0.130893\n",
            "Avg loss (5/15): 0.131769\n",
            "Avg loss (5/16): 0.132380\n",
            "Avg loss (5/17): 0.132717\n",
            "Avg loss (5/18): 0.132793\n",
            "Avg loss (5/19): 0.132602\n",
            "Accuracy: 0.786500\n",
            "Avg loss (6/0): 0.133621\n",
            "Avg loss (6/1): 0.132928\n",
            "Avg loss (6/2): 0.131670\n",
            "Avg loss (6/3): 0.130049\n",
            "Avg loss (6/4): 0.128271\n",
            "Avg loss (6/5): 0.126484\n",
            "Avg loss (6/6): 0.124781\n",
            "Avg loss (6/7): 0.123222\n",
            "Avg loss (6/8): 0.121876\n",
            "Avg loss (6/9): 0.120809\n",
            "Avg loss (6/10): 0.120061\n",
            "Avg loss (6/11): 0.119694\n",
            "Avg loss (6/12): 0.119737\n",
            "Avg loss (6/13): 0.120128\n",
            "Avg loss (6/14): 0.120767\n",
            "Avg loss (6/15): 0.121517\n",
            "Avg loss (6/16): 0.122256\n",
            "Avg loss (6/17): 0.122874\n",
            "Avg loss (6/18): 0.123274\n",
            "Avg loss (6/19): 0.123355\n",
            "Accuracy: 0.806600\n",
            "Avg loss (7/0): 0.124647\n",
            "Avg loss (7/1): 0.124100\n",
            "Avg loss (7/2): 0.123068\n",
            "Avg loss (7/3): 0.121658\n",
            "Avg loss (7/4): 0.120017\n",
            "Avg loss (7/5): 0.118316\n",
            "Avg loss (7/6): 0.116705\n",
            "Avg loss (7/7): 0.115314\n",
            "Avg loss (7/8): 0.114246\n",
            "Avg loss (7/9): 0.113553\n",
            "Avg loss (7/10): 0.113237\n",
            "Avg loss (7/11): 0.113255\n",
            "Avg loss (7/12): 0.113537\n",
            "Avg loss (7/13): 0.114001\n",
            "Avg loss (7/14): 0.114559\n",
            "Avg loss (7/15): 0.115118\n",
            "Avg loss (7/16): 0.115586\n",
            "Avg loss (7/17): 0.115902\n",
            "Avg loss (7/18): 0.116020\n",
            "Avg loss (7/19): 0.115894\n",
            "Accuracy: 0.815600\n",
            "Avg loss (8/0): 0.116861\n",
            "Avg loss (8/1): 0.116452\n",
            "Avg loss (8/2): 0.115686\n",
            "Avg loss (8/3): 0.114647\n",
            "Avg loss (8/4): 0.113445\n",
            "Avg loss (8/5): 0.112191\n",
            "Avg loss (8/6): 0.110976\n",
            "Avg loss (8/7): 0.109870\n",
            "Avg loss (8/8): 0.108930\n",
            "Avg loss (8/9): 0.108206\n",
            "Avg loss (8/10): 0.107704\n",
            "Avg loss (8/11): 0.107429\n",
            "Avg loss (8/12): 0.107386\n",
            "Avg loss (8/13): 0.107542\n",
            "Avg loss (8/14): 0.107856\n",
            "Avg loss (8/15): 0.108262\n",
            "Avg loss (8/16): 0.108704\n",
            "Avg loss (8/17): 0.109086\n",
            "Avg loss (8/18): 0.109329\n",
            "Avg loss (8/19): 0.109355\n",
            "Accuracy: 0.832300\n",
            "Avg loss (9/0): 0.110598\n",
            "Avg loss (9/1): 0.110224\n",
            "Avg loss (9/2): 0.109516\n",
            "Avg loss (9/3): 0.108548\n",
            "Avg loss (9/4): 0.107414\n",
            "Avg loss (9/5): 0.106218\n",
            "Avg loss (9/6): 0.105058\n",
            "Avg loss (9/7): 0.104034\n",
            "Avg loss (9/8): 0.103208\n",
            "Avg loss (9/9): 0.102624\n",
            "Avg loss (9/10): 0.102296\n",
            "Avg loss (9/11): 0.102204\n",
            "Avg loss (9/12): 0.102314\n",
            "Avg loss (9/13): 0.102561\n",
            "Avg loss (9/14): 0.102894\n",
            "Avg loss (9/15): 0.103236\n",
            "Avg loss (9/16): 0.103519\n",
            "Avg loss (9/17): 0.103685\n",
            "Avg loss (9/18): 0.103688\n",
            "Avg loss (9/19): 0.103488\n",
            "Accuracy: 0.836900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DULgohtbqKGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}